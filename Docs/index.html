
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>mlfinlab 0.12.3 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <a class="reference external image-reference" href="https://hudsonthames.org/"><img alt="_images/ht_logo.png" class="align-center" src="_images/ht_logo.png" style="width: 157.2px; height: 210.0px;" /></a>
<div class="line-block">
<div class="line"><br /></div>
</div>
<section id="machine-learning-financial-laboratory-mlfinlab">
<h1>Machine Learning Financial Laboratory (mlfinlab)<a class="headerlink" href="#machine-learning-financial-laboratory-mlfinlab" title="Permalink to this heading">¶</a></h1>
<blockquote>
<div><p><a class="reference external" href="https://pypi.org/project/mlfinlab/"><img alt="PyPi" src="https://img.shields.io/pypi/v/mlfinlab.svg" /></a> <a class="reference external" href="https://pypi.org/project/mlfinlab/"><img alt="Python" src="https://img.shields.io/pypi/pyversions/mlfinlab.svg" /></a> <a class="reference external" href="https://travis-ci.com/hudson-and-thames/mlfinlab"><img alt="Build Status" src="https://travis-ci.com/hudson-and-thames/mlfinlab.svg?branch=master" /></a> <a class="reference external" href="https://codecov.io/gh/hudson-and-thames/mlfinlab"><img alt="codecov" src="https://codecov.io/gh/hudson-and-thames/mlfinlab/branch/master/graph/badge.svg" /></a> <img alt="pylint Score" src="https://mperlet.github.io/pybadge/badges/10.svg" /></p>
<p><a class="reference external" href="https://pypi.org/project/mlfinlab/"><img alt="Downloads" src="https://img.shields.io/pypi/dm/mlfinlab.svg" /></a></p>
</div></blockquote>
<p>MlFinlab is a python package which helps portfolio managers and traders who want to leverage the power of machine learning
by providing reproducible, interpretable, and easy to use tools. Adding MlFinLab to your companies pipeline is like adding
a department of PhD researchers to your team.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">mlfinlab</span>
</pre></div>
</div>
<p>We source all of our implementations from the most elite and peer-reviewed journals. Including publications from:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://jfds.pm-research.com/">The Journal of Financial Data Science</a></p></li>
<li><p><a class="reference external" href="https://jpm.pm-research.com/">The Journal of Portfolio Management</a></p></li>
<li><p><a class="reference external" href="http://www.algorithmicfinance.org/">The Journal of Algorithmic Finance</a></p></li>
<li><p><a class="reference external" href="https://www.cambridge.org/">Cambridge University Press</a></p></li>
</ol>
<p>We are making a big drive to include techniques from various authors, however the most dominant author would be Dr. Marcos
Lopez de Prado (<a class="reference external" href="http://www.quantresearch.org/">QuantResearch.org</a>). This package has its foundations in the two graduate
level textbooks:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://www.amazon.co.uk/Advances-Financial-Machine-Learning-Marcos/dp/1119482089">Advances in Financial Machine Learning</a></p></li>
<li><p><a class="reference external" href="https://www.cambridge.org/core/books/machine-learning-for-asset-managers/6D9211305EA2E425D33A9F38D0AE3545">Machine Learning for Asset Managers</a></p></li>
</ol>
<figure class="align-center">
<a class="reference external image-reference" href="https://www.pm-research.com/"><img alt="Academic Journals" src="_images/journals.png" style="width: 703.0px; height: 153.0px;" /></a>
</figure>
<section id="praise-for-mlfinlab">
<h2>Praise for MlFinLab<a class="headerlink" href="#praise-for-mlfinlab" title="Permalink to this heading">¶</a></h2>
<p>“Financial markets are complex systems like no other. Extracting signal from financial data requires specialized tools
that are distinct from those used in general machine learning. The MlFinLab package compiles important algorithms
that every quant should know and use.”</p>
<p>- <a class="reference external" href="https://www.linkedin.com/in/lopezdeprado/">Dr. Marcos Lopez de Prado</a>, Co-founder and CIO at True Positive Technologies; Professor of Practice at Cornell University</p>
<hr class="docutils" />
<p>“Those who doubt open-source libraries just need to look at the impact of Pandas, Scikit-learn, and the like. MIFinLab
is doing to financial machine learning what Tensorflow and PyTorch are doing to deep learning.”</p>
<p>- <a class="reference external" href="https://www.linkedin.com/in/epchan/">Dr. Ernest Chan</a>, Hedge Fund Manager at QTS &amp; Author</p>
<hr class="docutils" />
<p>“For many decades, finance has relied on overly simplistic statistical techniques to identify patterns in data.
Machine learning promises to change that by allowing researchers to use modern nonlinear and highly dimensional
techniques. Yet, applying those machine learning algorithms to model financial problems is easier said than done:
finance is not a plug-and-play subject as it relates to machine learning.</p>
<p>MlFinLab provides access to the latest cutting edges methods. MlFinLab is thus essential for quants who want to be
ahead of the technology rather than being replaced by it.”</p>
<p>- <a class="reference external" href="https://www.linkedin.com/in/thomas-raffinot-b75734b/">Dr. Thomas Raffinot</a>, Financial Data Scientist at ENGIE Global Markets</p>
</section>
<hr class="docutils" />
<section id="unlocking-the-commons">
<h2>Unlocking the Commons<a class="headerlink" href="#unlocking-the-commons" title="Permalink to this heading">¶</a></h2>
<p>We are currently running a sponsorship model of “Unlocking the Commmons”. Our code base, online documentation,
tutorial notebooks and presentations will remain open to everyone for so long as we can meet our minimum sponsorship
goals. We have set the <strong>deadline: December 2020</strong> - for a monthly total patronage of $4000 USD.</p>
<p><a class="reference external" href="https://github.com/nayafia/lemonade-stand">Nadia Eghbal</a> explains it well: “If you’d like to open source a project
but want to ensure that others will invest in its long-term maintenance, you could tell your community that you’ll
open-source the project once you’ve hit a certain amount of sponsorship. (Writer Tim Carmody refers to this as
“unlocking the commons.”)”</p>
<p><a class="reference external" href="https://www.patreon.com/HudsonThames">Become a Patron and keep MlFinLab Open!</a></p>
</section>
<section id="documentation-tutorials">
<h2>Documentation &amp; Tutorials<a class="headerlink" href="#documentation-tutorials" title="Permalink to this heading">¶</a></h2>
<p>We lower barriers to entry for all users by providing extensive <a class="reference external" href="https://mlfinlab.readthedocs.io/en/latest/">documentation</a>
and <a class="reference external" href="https://github.com/hudson-and-thames/research">tutorial notebooks</a>, with code examples.</p>
</section>
<section id="who-is-hudson-thames">
<h2>Who is Hudson &amp; Thames?<a class="headerlink" href="#who-is-hudson-thames" title="Permalink to this heading">¶</a></h2>
<p>We are a private research group focused on implementing research based financial machine learning. We all work in
virtual teams, spread across the world, primarily: New York, London, and Kyiv.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://hudsonthames.org/">Website</a></p></li>
<li><p><a class="reference external" href="https://github.com/hudson-and-thames">Github Group</a></p></li>
<li><p><a class="reference external" href="https://github.com/orgs/hudson-and-thames/projects">Project Boards</a></p></li>
<li><p><a class="reference external" href="https://mlfinlab.readthedocs.io/en/latest/">Documentation</a></p></li>
</ul>
</section>
<section id="sponsors-and-donating">
<h2>Sponsors and Donating<a class="headerlink" href="#sponsors-and-donating" title="Permalink to this heading">¶</a></h2>
<p>A special thank you to our sponsors! If you would like to become a sponsor and help support our research, please sign
up on <a class="reference external" href="https://www.patreon.com/HudsonThames">Patreon</a></p>
<p>Benefits include:</p>
<ol class="arabic simple">
<li><p>Uninterrupted access: Should the code base pivot to closed source - your company will have access to all
implementations and the source code.</p></li>
<li><p>A seat on the Hudson &amp; Thames advisory council and votes towards the direction of research and implementations.</p></li>
<li><p>Ongoing access to slide show presentations and Jupyter Notebooks. (files can be edited to suit your personal
needs such as classroom notes or client presentations.)</p></li>
<li><p>Company / Organisation profile on <a class="reference external" href="https://hudsonthames.org/sponsors/">www.hudsonthames.org</a></p></li>
<li><p>Use of Hudson &amp; Thames sponsor badge on your website.</p></li>
<li><p>Access to our communities Slack Channel.</p></li>
<li><p>Subscription to project release updates and news.</p></li>
</ol>
<section id="platinum-sponsor">
<h3>Platinum Sponsor:<a class="headerlink" href="#platinum-sponsor" title="Permalink to this heading">¶</a></h3>
<figure class="align-center">
<a class="reference external image-reference" href="https://hudsonthames.org/sponsors/"><img alt="Platinum Sponsors" src="_images/plat_sponsors.png" style="width: 366.0px; height: 179.0px;" /></a>
</figure>
</section>
<section id="gold-sponsors">
<h3>Gold Sponsors:<a class="headerlink" href="#gold-sponsors" title="Permalink to this heading">¶</a></h3>
<figure class="align-center">
<a class="reference external image-reference" href="https://hudsonthames.org/sponsors/"><img alt="Gold Sponsors" src="_images/gold_sponsors.png" style="width: 581.0px; height: 285.0px;" /></a>
</figure>
</section>
</section>
<section id="contact-us">
<h2>Contact us<a class="headerlink" href="#contact-us" title="Permalink to this heading">¶</a></h2>
<p>We host a booming community of like minded data scientists and quants, join the
<a class="reference external" href="https://www.patreon.com/HudsonThames">Slack Channel</a> now! Open to sponsors of our package.</p>
<p>The channel has the following benefits:</p>
<ul class="simple">
<li><p>Community of like minded individuals.</p></li>
<li><p>Ask questions about the package implementations and get community feedback.</p></li>
<li><p>Occasional presentations on topics within financial machine learning.</p></li>
<li><p>A papers channel where we share the papers which are freely available.</p></li>
<li><p>Access to members of our research group.</p></li>
</ul>
<p>Looking forward to hearing from you!</p>
</section>
<section id="early-adopters">
<h2>Early Adopters:<a class="headerlink" href="#early-adopters" title="Permalink to this heading">¶</a></h2>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="https://www.linkedin.com/in/john-keown-quantitative-finance-big-data-ml/">John B. Keown</a></p></td>
<td><p><a class="reference external" href="https://www.linkedin.com/in/roberto-spadim/">Roberto Spadim</a></p></td>
<td><p><a class="reference external" href="https://www.linkedin.com/in/zackgow/">Zack Gow</a></p></td>
<td><p>Alex Zivkovic</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://www.linkedin.com/in/jihao-yu/">Jack Yu</a></p></td>
<td><p>Егор Тарасенок</p></td>
<td><p><a class="reference external" href="https://www.linkedin.com/in/jmatthew/">Joseph Matthew</a></p></td>
<td><p><a class="reference external" href="https://www.linkedin.com/in/amandhaliwal44/">Aman Dhaliwal</a></p></td>
</tr>
<tr class="row-odd"><td><p>Justin Gerard</p></td>
<td><p>Jason Young</p></td>
<td><p><a class="reference external" href="https://www.linkedin.com/in/shaunmcdonogh/">Shaun McDonogh</a></p></td>
<td><p>Austin Hubbell</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://www.linkedin.com/in/christian-beckmann/">Christian Beckmann</a></p></td>
<td><p>Jeffrey Wang</p></td>
<td><p><a class="reference external" href="https://www.linkedin.com/in/etartakovsky/">Eugene Tartakovsky</a></p></td>
<td><p><a class="reference external" href="https://www.linkedin.com/in/haozhang777/">Ben Zhang</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://www.linkedin.com/in/ming-yue-wu/">Ming Wu</a></p></td>
<td><p><a class="reference external" href="https://www.linkedin.com/in/richardscheiwe/">Richard Scheiwe</a></p></td>
<td><p><a class="reference external" href="https://www.linkedin.com/in/tianfangwu/">Tianfang Wu</a></p></td>
<td><p><a class="reference external" href="https://www.linkedin.com/in/bcrblackarbs/">Brian Christopher</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://www.linkedin.com/in/ericdetterman/">Eric Detterman</a></p></td>
<td><p>Eric Huang</p></td>
<td><p>Gunther Schulz</p></td>
<td><p>Geoff Foster</p></td>
</tr>
<tr class="row-odd"><td><p>Golam Sakline</p></td>
<td><p><a class="reference external" href="https://www.linkedin.com/in/ilyapikulin/">Ilya Pikulin</a></p></td>
<td><p>Isabel Gonzalez</p></td>
<td><p>Jason Harris</p></td>
</tr>
<tr class="row-even"><td><p>Joshua Cortez</p></td>
<td><p><a class="reference external" href="https://www.linkedin.com/in/kofiglover/">Kofi Glover</a></p></td>
<td><p>Kristian Schmidt</p></td>
<td><p>Lucas Astorian</p></td>
</tr>
<tr class="row-odd"><td><p>Luque Li</p></td>
<td><p><a class="reference external" href="https://www.linkedin.com/in/mikhail-shishlenin/">Mikhail Shishlenin</a></p></td>
<td><p>Minsu Yeom</p></td>
<td><p><a class="reference external" href="https://www.linkedin.com/in/mislav-%C5%A1agovac-a72a6044">Mislav Sagovac</a></p></td>
</tr>
<tr class="row-even"><td><p>Paul Morgen</p></td>
<td><p><a class="reference external" href="https://www.linkedin.com/in/rino-hilman-854a89181/">Rino Hilman</a></p></td>
<td><p><a class="reference external" href="https://www.linkedin.com/in/ruifan-pei-b994b177/">Ruifan Pei</a></p></td>
<td><p><a class="reference external" href="https://www.linkedin.com/in/shawnunger1/">Shawn Unger</a></p></td>
</tr>
<tr class="row-odd"><td><p>Bramantyo Erlangga</p></td>
<td><p><a class="reference external" href="https://www.linkedin.com/in/sritanuchakraborty/">Sritanu Chakraborty</a></p></td>
<td><p>Stephen Caraher</p></td>
<td><p>Swaminathan Sethuraman</p></td>
</tr>
<tr class="row-even"><td><p>Thiago Matos</p></td>
<td><p>Thyme Sage</p></td>
<td><p>Tom Celig</p></td>
<td><p><a class="reference external" href="https://www.linkedin.com/in/thomas-parnell-7233a234/">Tom Parnell</a></p></td>
</tr>
<tr class="row-odd"><td><p>William Thompkins</p></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="license">
<h2>License<a class="headerlink" href="#license" title="Permalink to this heading">¶</a></h2>
<p>This project is licensed under an all rights reserved licence.</p>
<p><a class="reference external" href="https://github.com/hudson-and-thames/mlfinlab/blob/master/LICENSE.txt">LICENSE.txt</a> file for details.</p>
<div class="toctree-wrapper compound">
<span id="document-getting_started/installation"></span><section id="installation">
<h3>Installation<a class="headerlink" href="#installation" title="Permalink to this heading">¶</a></h3>
<section id="recommended-versions">
<h4>Recommended Versions<a class="headerlink" href="#recommended-versions" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Anaconda 3</p></li>
<li><p>Python 3.6</p></li>
</ul>
</section>
<section id="installation-for-users">
<h4>Installation for Users<a class="headerlink" href="#installation-for-users" title="Permalink to this heading">¶</a></h4>
<p>The package can be installed from the PyPi index via the console:
Launch the terminal and run.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">mlfinlab</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="installation-for-developers">
<h4>Installation for Developers<a class="headerlink" href="#installation-for-developers" title="Permalink to this heading">¶</a></h4>
<p>Clone the <a class="reference external" href="(https://github.com/hudson-and-thames/mlfinlab)">package repo</a> to your local machine then follow the steps below.</p>
<section id="mac-os-x-and-ubuntu-linux">
<h5>Mac OS X and Ubuntu Linux<a class="headerlink" href="#mac-os-x-and-ubuntu-linux" title="Permalink to this heading">¶</a></h5>
<ol class="arabic simple">
<li><p>Make sure you install the latest version of the Anaconda 3 distribution. To do this you can follow the install and update instructions found on this <a class="reference external" href="(https://www.anaconda.com/download/#mac)">link</a></p></li>
<li><p>Launch a terminal</p></li>
<li><p>Create a New Conda Environment. From terminal.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">create</span> <span class="o">-</span><span class="n">n</span> <span class="o">&lt;</span><span class="n">env</span> <span class="n">name</span><span class="o">&gt;</span> <span class="n">python</span><span class="o">=</span><span class="mf">3.6</span> <span class="n">anaconda</span>
</pre></div>
</div>
<p>Accept all the requests to install.</p>
<ol class="arabic simple" start="4">
<li><p>Now activate the environment with:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">source</span> <span class="n">activate</span> <span class="o">&lt;</span><span class="n">env</span> <span class="n">name</span><span class="o">&gt;</span>
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>From Terminal: go to the directory where you have saved the file, example:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="n">Desktop</span><span class="o">/</span><span class="n">mlfinlab</span>
</pre></div>
</div>
<ol class="arabic simple" start="6">
<li><p>Install Python requirements, by running the command:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">r</span> <span class="n">requirements</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</section>
<section id="windows">
<h5>Windows<a class="headerlink" href="#windows" title="Permalink to this heading">¶</a></h5>
<ol class="arabic simple">
<li><p>Download and install the latest version of <a class="reference external" href="(https://www.anaconda.com/distribution/#download-section)">Anaconda 3</a></p></li>
<li><p>Launch Anaconda Navigator</p></li>
<li><p>Click Environments, choose an environment name, select Python 3.6, and click Create</p></li>
<li><p>Click Home, browse to your new environment, and click Install under Jupyter Notebook</p></li>
<li><p>Launch Anaconda Prompt and activate the environment:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">activate</span> <span class="o">&lt;</span><span class="n">env</span> <span class="n">name</span><span class="o">&gt;</span>
</pre></div>
</div>
<ol class="arabic simple" start="6">
<li><p>From Anaconda Prompt: go to the directory where you have saved the file, example:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="n">Desktop</span><span class="o">/</span><span class="n">mlfinlab</span>
</pre></div>
</div>
<ol class="arabic simple" start="7">
<li><p>Install Python requirements, by running the command:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">r</span> <span class="n">requirements</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</section>
</section>
</section>
<span id="document-additional_information/contact"></span><section id="join-the-slack-channel">
<span id="additional-information-contact"></span><h3>Join the Slack Channel<a class="headerlink" href="#join-the-slack-channel" title="Permalink to this heading">¶</a></h3>
<p>We host a booming community of like minded data scientists and quants, join the Slack channel now! Open to
<a class="reference external" href="https://www.patreon.com/HudsonThames">sponsors</a> of our package.</p>
<p>The channel has the following benefits:</p>
<ul class="simple">
<li><p>Community of like minded individuals.</p></li>
<li><p>Ask questions about the package implementations and get community feedback.</p></li>
<li><p>Occasional presentations on topics within financial machine learning.</p></li>
<li><p>A papers channel where we share the papers which are freely available.</p></li>
<li><p>Access to members of our research group.</p></li>
</ul>
<p>Looking forward to hearing from you!</p>
<a class="reference internal image-reference" href="_images/slack.png"><img alt="_images/slack.png" class="align-center" src="_images/slack.png" style="width: 663.65px; height: 298.35px;" /></a>
</section>
<span id="document-getting_started/barriers_to_entry"></span><section id="barriers-to-entry">
<h3>Barriers to Entry<a class="headerlink" href="#barriers-to-entry" title="Permalink to this heading">¶</a></h3>
<p>As most of you know, getting through the first 3 chapters of the book is challenging as it relies on HFT data to
create the new financial data structures. Sourcing the HFT data is very difficult and thus we have resorted to purchasing
the full history of S&amp;P500 Emini futures tick data from <a class="reference external" href="https://www.tickdata.com/">TickData LLC</a>.</p>
<p>We are not affiliated with TickData in any way but would like to recommend others to make use of their service. The full
history cost us about $750 and is worth every penny. They have really done a great job at cleaning the data and providing
it in a user friendly manner.</p>
<section id="sample-data">
<h4>Sample Data<a class="headerlink" href="#sample-data" title="Permalink to this heading">¶</a></h4>
<p>TickData does offer about 20 days worth of raw tick data which can be sourced from their website <a class="reference external" href="https://s3-us-west-2.amazonaws.com/tick-data-s3/downloads/ES_Sample.zip">link</a>.</p>
<p>For those of you interested in working with a two years of sample tick, volume, and dollar bars, it is provided for in the <a class="reference external" href="https://github.com/hudson-and-thames/research/tree/master/Sample-Data">research repo</a>.</p>
<p>You should be able to work on a few implementations of the code with this set.</p>
</section>
<section id="additional-sources">
<h4>Additional Sources<a class="headerlink" href="#additional-sources" title="Permalink to this heading">¶</a></h4>
<p>Searching for free tick data can be a challenging task. The following three sources may help:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://www.dukascopy.com/swiss/english/marketwatch/historical/">Dukascopy</a>. Offers free historical tick data for some futures, though you do have to register.</p></li>
<li><p>Most crypto exchanges offer tick data but not historical (see <a class="reference external" href="https://github.com/binance-exchange/binance-official-api-docs/blob/master/rest-api.md">Binance API</a>). So you’d have to run a script for a few days.</p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/how-and-why-i-got-75gb-of-free-foreign-exchange-tick-data-9ca78f5fa26c">Blog Post</a>: How and why I got 75Gb of free foreign exchange “Tick” data.</p></li>
</ol>
</section>
</section>
<span id="document-getting_started/researcher"></span><section id="become-a-researcher">
<h3>Become a Researcher<a class="headerlink" href="#become-a-researcher" title="Permalink to this heading">¶</a></h3>
<iframe width="560" height="315" src="https://www.youtube.com/embed/1RWfvk8rvsY" frameborder="0" allow="accelerometer;
 autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><div class="line-block">
<div class="line"><br /></div>
</div>
<p>The Hudson &amp; Thames <a class="reference external" href="https://hudsonthames.org/mentorship/">Mentorship Program</a> caters to ambitious students looking to make an impact on open-source and develop
a portfolio of work based on financial machine learning. You will learn a wealth of skills, with a focus on writing
production-level code, based on the most cutting edge research, from elite peer-reviewed journals, such as:</p>
<ol class="arabic simple">
<li><p>Journal of Financial Data Science</p></li>
<li><p>Journal of Portfolio Management</p></li>
<li><p>Algorithmic Finance</p></li>
</ol>
<p>It is a powerful negotiating tool, when you can approach employers with an array of tools that can add immediate value to their company. To quote GitLab:</p>
<p class="centered">
<strong><strong>“Contributing to open source is the most effective job-seeking hack you can take advantage of right now!”</strong></strong></p><p>The program was established to help provide students with a guided path to gaining the Researcher achievement at
Hudson and Thames. Once the achievement has been unlocked you will gain lifetime access to our community, libraries,
and attribution for all of the work completed. As an optional service, we as a group share expenses for tools and
services such as high-quality tick data, journal subscriptions.</p>
<p>The goal of the program is to gain Researcher status. We will guide you through the following achievements:</p>
<ol class="arabic simple">
<li><p>Make a sizable contribution to the mlfinlab library.</p></li>
<li><p>Passed the following production-ready checks: code style and 100% test coverage.</p></li>
<li><p>Inline documentation, docstrings, and sphinx documentation.</p></li>
<li><p>Completed tutorial notebooks to show mastery of the concepts worked on and provide external users with guided documentation.</p></li>
<li><p>Long-form blog article (example) showcasing the newly implemented research. This gives researchers the opportunity to build
on their online reputation and show off some of the new skills attained.</p></li>
</ol>
<p class="centered">
<strong><a class="reference external" href="https://hudsonthames.org/mentorship/">Become a Researcher!</a></strong></p></section>
<span id="document-getting_started/datasets"></span><section id="datasets">
<h3>Datasets<a class="headerlink" href="#datasets" title="Permalink to this heading">¶</a></h3>
<p>Mlfinlab package contains various financial datasets which can be used by a researcher as sandbox data.</p>
<section id="tick-sample">
<h4>Tick sample<a class="headerlink" href="#tick-sample" title="Permalink to this heading">¶</a></h4>
<p>Mlfinlab provides a sample of tick data for E-Mini S&amp;P 500 futures which can be used to test bar compression algorithms,
microstructural features, etc. Tick data sample consists of Timestamp, Price and Volume.</p>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.datasets.load_datasets.load_tick_sample">
<span class="sig-name descname"><span class="pre">load_tick_sample</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">DataFrame</span></span></span><a class="headerlink" href="#mlfinlab.datasets.load_datasets.load_tick_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads E-Mini S&amp;P 500 futures tick data sample</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>(pd.DataFrame) with tick data sample</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="dollar-bars-sample">
<h4>Dollar bars sample<a class="headerlink" href="#dollar-bars-sample" title="Permalink to this heading">¶</a></h4>
<p>We also provide a sample of dollar bars for E-Mini S&amp;P 500 futures. Data set structure:</p>
<blockquote>
<div><ul class="simple">
<li><p>Open price (open)</p></li>
<li><p>High price (high)</p></li>
<li><p>Low price (low)</p></li>
<li><p>Close price (close)</p></li>
<li><p>Volume (cum_volume)</p></li>
<li><p>Dollar volume traded (cum_dollar)</p></li>
<li><p>Number of ticks inside of bar (cum_ticks)</p></li>
</ul>
</div></blockquote>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can find more information on dollar bars and other bar compression algorithms in
<a class="reference external" href="https://mlfinlab.readthedocs.io/en/latest/implementations/data_structures.html">Data Structures</a></p>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.datasets.load_datasets.load_dollar_bar_sample">
<span class="sig-name descname"><span class="pre">load_dollar_bar_sample</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">DataFrame</span></span></span><a class="headerlink" href="#mlfinlab.datasets.load_datasets.load_dollar_bar_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads E-Mini S&amp;P 500 futures dollar bars data sample.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>(pd.DataFrame) with dollar bar data sample</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="etf-prices">
<h4>ETF prices<a class="headerlink" href="#etf-prices" title="Permalink to this heading">¶</a></h4>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.datasets.load_datasets.load_stock_prices">
<span class="sig-name descname"><span class="pre">load_stock_prices</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">DataFrame</span></span></span><a class="headerlink" href="#mlfinlab.datasets.load_datasets.load_stock_prices" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads stock prices data sets consisting of
EEM, EWG, TIP, EWJ, EFA, IEF, EWQ, EWU, XLB, XLE, XLF, LQD, XLK, XLU, EPP, FXI, VGK, VPL, SPY, TLT, BND, CSJ,
DIA starting from 2008 till 2016.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>(pd.DataFrame) stock_prices data frame</p>
</dd>
</dl>
</dd></dl>

<p>The data set consists of close prices for: EEM, EWG, TIP, EWJ, EFA, IEF, EWQ, EWU, XLB, XLE, XLF, LQD, XLK, XLU, EPP,
FXI, VGK, VPL, SPY, TLT, BND, CSJ, DIA starting from 2008 till 2016. It can be used to test and validate portfolio
optimization techniques.</p>
</section>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.datasets</span> <span class="kn">import</span> <span class="p">(</span><span class="n">load_tick_sample</span><span class="p">,</span> <span class="n">load_stock_prices</span><span class="p">,</span> <span class="n">load_dollar_bar_sample</span><span class="p">)</span>

<span class="n">tick_df</span> <span class="o">=</span> <span class="n">load_tick_sample</span><span class="p">()</span>
<span class="n">dollar_bars_df</span> <span class="o">=</span> <span class="n">load_dollar_bar_sample</span><span class="p">()</span>
<span class="n">stock_prices_df</span> <span class="o">=</span> <span class="n">load_stock_prices</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<span id="document-getting_started/research_tools"></span><section id="research-tools">
<h3>Research Tools<a class="headerlink" href="#research-tools" title="Permalink to this heading">¶</a></h3>
<p>As researchers, we often neglect finding the right tools to streamline
the progress. Financial Machine Learning is no different in that a lot of the papers are scattered
across different journals and different fields. Ranging from journals on econometrics to machine
learning, researchers often struggle to find the best academic papers to begin their studies.</p>
<p>At Hudson &amp; Thames, we primarily use two resources: <a class="reference external" href="https://www.connectedpapers.com/">Connected Papers</a> and <a class="reference external" href="https://ethos.bl.uk/Home.do">EThOS</a>. These two
free sites have been invaluable and offer an advantage to search through the most cutting edge
resources available for our MlFinLab library.</p>
<section id="id1">
<h4>Connected Papers<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h4>
<p>Connected papers is unique in that it is not a citation tree. A citation from a paper does not
necessarily lead the reader to another paper. The two topics might be completely different and
an unimportant topic for the researcher.</p>
<p>It uniquely identifies the related papers by looking at the cocitation and bibliographic coupling.
More about the website is available at the connected papers founder’s <a class="reference external" href="https://medium.com/connectedpapers/announcing-connected-papers-a-visual-tool-for-researchers-to-find-and-explore-academic-papers-89146a54c7d4">medium</a> post.</p>
<p>To give a brief demonstration, we will examine a <a class="reference external" href="https://arxiv.org/abs/1212.2129">paper</a> by Li and Hoi that started our Online Portfolio Selection module.</p>
<p>If you type in the name of the paper, you will see a graph like the one below.</p>
<a class="reference internal image-reference" href="_images/graph.png"><img alt="_images/graph.png" class="align-center" src="_images/graph.png" style="width: 50%;" /></a>
<p>It immediately shows which are the most associated papers. The darker circles indicate that they are
more recent, so we can easily follow from the older papers to the newer ones. Connected papers also
has an amazing feature for prior works and derivative works.</p>
<p>Prior works is available for researchers to see what are the most famous and cited papers in this field
to recognize the importance and start with the baseline material. If we click the button for prior works,
for our current search, we see an image like this:</p>
<a class="reference internal image-reference" href="_images/prior.png"><img alt="_images/prior.png" class="align-center" src="_images/prior.png" style="width: 50%;" /></a>
<p>We can easily see which were the most cited papers. It is not surprising that the number one paper
associated with Online Portfolio Selection is Thomas Cover’s Universal Portfolio, the original paper
that began the studies in Portfolio Selection based on information theory.</p>
<p>Once the researcher gets more familiar with the topic by going through literature review with prior
works, they can move on to the derivative works, which cover the most recent papers associated with
the paper of interest.</p>
<a class="reference internal image-reference" href="_images/derivative.png"><img alt="_images/derivative.png" class="align-center" src="_images/derivative.png" style="width: 50%;" /></a>
</section>
<section id="id2">
<h4>EThOS<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h4>
<p><a class="reference external" href="https://ethos.bl.uk/Home.do">EThOS</a> is a online library sponsored by the United Kingdom to make publicly-funded research available
to all researchers.</p>
<p>The best feature for EThOS is the availability of all doctoral theses in the UK. If your topic of
interest does not have too many sources from journals, there is a high chance that you can find
good works in EThOS as it is not limited to published journals but rather all doctoral theses as well.</p>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-implementations/data_structures"></span><section id="data-structures">
<span id="implementations-data-structures"></span><h3>Data Structures<a class="headerlink" href="#data-structures" title="Permalink to this heading">¶</a></h3>
<p>When analyzing financial data, unstructured data sets, in this case tick data, are commonly transformed into a structured
format referred to as bars, where a bar represents a row in a table. mlfinlab implements tick, volume, and dollar bars
using traditional standard bar methods as well as the less common information driven bars.</p>
<section id="standard-bars">
<h4>Standard Bars<a class="headerlink" href="#standard-bars" title="Permalink to this heading">¶</a></h4>
<p>The four standard bar methods implemented share a similar underlying idea in that they take a sample of data after a
certain threshold is reached and they all result in a time series of Open, High, Low, and Close data.</p>
<ol class="arabic simple">
<li><p>Time bars, are sampled after a fixed interval of time has passed.</p></li>
<li><p>Tick bars, are sampled after a fixed number of ticks have taken place.</p></li>
<li><p>Volume bars, are sampled after a fixed number of contracts (volume) has been traded.</p></li>
<li><p>Dollar bars, are sampled after a fixed monetary amount has been traded.</p></li>
</ol>
<p>These bars are used throughout the text book (Advances in Financial Machine Learning, By Marcos Lopez de Prado, 2018,
pg 25) to build the more interesting features for predicting financial time series data.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>A fundamental paper that you need to read to have a better grasp on these concepts is:
<a class="reference external" href="https://jpm.pm-research.com/content/39/1/19.abstract">Easley, David, Marcos M. López de Prado, and Maureen O’Hara. “The volume clock: Insights into the high-frequency
paradigm.” The Journal of Portfolio Management 39.1 (2012): 19-29.</a></p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>A threshold can be either fixed (given as <code class="docutils literal notranslate"><span class="pre">float</span></code>) or dynamic (given as <code class="docutils literal notranslate"><span class="pre">pd.Series</span></code>). If a dynamic threshold is used
then there is no need to declare threshold for every observation. Values are needed only for the first observation
(or any time before it) and later at times when the threshold is changed to a new value.
Whenever sampling is made, the most recent threshold level is used.</p>
<p><strong>An example for volume bars</strong>
We have daily observations of prices and volumes:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Time</p></th>
<th class="head"><p>Price</p></th>
<th class="head"><p>Volume</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>20.04.2020</p></td>
<td><p>1000</p></td>
<td><p>10</p></td>
</tr>
<tr class="row-odd"><td><p>21.04.2020</p></td>
<td><p>990</p></td>
<td><p>10</p></td>
</tr>
<tr class="row-even"><td><p>22.04.2020</p></td>
<td><p>1000</p></td>
<td><p>20</p></td>
</tr>
<tr class="row-odd"><td><p>23.04.2020</p></td>
<td><p>1100</p></td>
<td><p>10</p></td>
</tr>
<tr class="row-even"><td><p>24.04.2020</p></td>
<td><p>1000</p></td>
<td><p>10</p></td>
</tr>
</tbody>
</table>
<p>And we set a dynamic threshold:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Time</p></th>
<th class="head"><p>Threshold</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>20.04.2020</p></td>
<td><p>20</p></td>
</tr>
<tr class="row-odd"><td><p>23.04.2020</p></td>
<td><p>10</p></td>
</tr>
</tbody>
</table>
<p>The data will be sampled as follows:</p>
<ul class="simple">
<li><p>20.04.2020 and 21.04.2020 into one bar, as their volume is 20.</p></li>
<li><p>22.04.2020 as a single bar, as its volume is 20.</p></li>
<li><p>23.04.2020 as a single bar, as it now fills the lower volume threshold of 10.</p></li>
<li><p>24.04.2020 as a single bar again.</p></li>
</ul>
</div>
<section id="time-bars">
<h5>Time Bars<a class="headerlink" href="#time-bars" title="Permalink to this heading">¶</a></h5>
<p>These are the traditional open, high, low, close bars that traders are used to seeing. The problem with using this sampling
technique is that information doesn’t arrive to market in a chronological clock, i.e. news event don’t occur on the hour - every hour.</p>
<p>It is for this reason that Time Bars have poor statistical properties in comparison to the other sampling techniques.</p>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.data_structures.time_data_structures.get_time_bars">
<span class="sig-name descname"><span class="pre">get_time_bars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path_or_df</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">resolution</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">'D'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_units</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_csv</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_path</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.data_structures.time_data_structures.get_time_bars" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates Time Bars: date_time, open, high, low, close, volume, cum_buy_volume, cum_ticks, cum_dollar_value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path_or_df</strong> – (str, iterable of str, or pd.DataFrame) Path to the csv file(s) or Pandas Data Frame containing raw tick data
in the format[date_time, price, volume]</p></li>
<li><p><strong>resolution</strong> – (str) Resolution type (‘D’, ‘H’, ‘MIN’, ‘S’)</p></li>
<li><p><strong>num_units</strong> – (int) Number of resolution units (3 days for example, 2 hours)</p></li>
<li><p><strong>batch_size</strong> – (int) The number of rows per batch. Less RAM = smaller batch size.</p></li>
<li><p><strong>verbose</strong> – (int) Print out batch numbers (True or False)</p></li>
<li><p><strong>to_csv</strong> – (bool) Save bars to csv after every batch run (True or False)</p></li>
<li><p><strong>output_path</strong> – (str) Path to csv file, if to_csv is True</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) Dataframe of time bars, if to_csv=True return None</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="tick-bars">
<h5>Tick Bars<a class="headerlink" href="#tick-bars" title="Permalink to this heading">¶</a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.data_structures.standard_data_structures.get_tick_bars">
<span class="sig-name descname"><span class="pre">get_tick_bars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path_or_df</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">float</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Series</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">70000000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_csv</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_path</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.data_structures.standard_data_structures.get_tick_bars" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the tick bars: date_time, open, high, low, close, volume, cum_buy_volume, cum_ticks, cum_dollar_value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path_or_df</strong> – (str, iterable of str, or pd.DataFrame) Path to the csv file(s) or Pandas Data Frame containing raw tick data
in the format[date_time, price, volume]</p></li>
<li><p><strong>threshold</strong> – (float, or pd.Series) A cumulative value above this threshold triggers a sample to be taken.
If a series is given, then at each sampling time the closest previous threshold is used.
(Values in the series can only be at times when the threshold is changed, not for every observation)</p></li>
<li><p><strong>batch_size</strong> – (int) The number of rows per batch. Less RAM = smaller batch size.</p></li>
<li><p><strong>verbose</strong> – (bool) Print out batch numbers (True or False)</p></li>
<li><p><strong>to_csv</strong> – (bool) Save bars to csv after every batch run (True or False)</p></li>
<li><p><strong>output_path</strong> – (str) Path to csv file, if to_csv is True</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) Dataframe of volume bars</p>
</dd>
</dl>
</dd></dl>

<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.data_structures</span> <span class="kn">import</span> <span class="n">standard_data_structures</span>

<span class="c1"># Tick Bars</span>
<span class="n">tick</span> <span class="o">=</span> <span class="n">standard_data_structures</span><span class="o">.</span><span class="n">get_tick_bars</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mi">5500</span><span class="p">,</span>
                                               <span class="n">batch_size</span><span class="o">=</span><span class="mi">1000000</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="volume-bars">
<h5>Volume Bars<a class="headerlink" href="#volume-bars" title="Permalink to this heading">¶</a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.data_structures.standard_data_structures.get_volume_bars">
<span class="sig-name descname"><span class="pre">get_volume_bars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path_or_df</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">float</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Series</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">70000000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_csv</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_path</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.data_structures.standard_data_structures.get_volume_bars" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the volume bars: date_time, open, high, low, close, volume, cum_buy_volume, cum_ticks, cum_dollar_value.</p>
<p>Following the paper “The Volume Clock: Insights into the high frequency paradigm” by Lopez de Prado, et al,
it is suggested that using 1/50 of the average daily volume, would result in more desirable statistical properties.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path_or_df</strong> – (str, iterable of str, or pd.DataFrame) Path to the csv file(s) or Pandas Data Frame containing raw tick data
in the format[date_time, price, volume]</p></li>
<li><p><strong>threshold</strong> – (float, or pd.Series) A cumulative value above this threshold triggers a sample to be taken.
If a series is given, then at each sampling time the closest previous threshold is used.
(Values in the series can only be at times when the threshold is changed, not for every observation)</p></li>
<li><p><strong>batch_size</strong> – (int) The number of rows per batch. Less RAM = smaller batch size.</p></li>
<li><p><strong>verbose</strong> – (bool) Print out batch numbers (True or False)</p></li>
<li><p><strong>to_csv</strong> – (bool) Save bars to csv after every batch run (True or False)</p></li>
<li><p><strong>output_path</strong> – (str) Path to csv file, if to_csv is True</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) Dataframe of volume bars</p>
</dd>
</dl>
</dd></dl>

<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.data_structures</span> <span class="kn">import</span> <span class="n">standard_data_structures</span>

<span class="c1"># Volume Bars</span>
<span class="n">volume</span> <span class="o">=</span> <span class="n">standard_data_structures</span><span class="o">.</span><span class="n">get_volume_bars</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mi">28000</span><span class="p">,</span>
                                              <span class="n">batch_size</span><span class="o">=</span><span class="mi">1000000</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="dollar-bars">
<h5>Dollar Bars<a class="headerlink" href="#dollar-bars" title="Permalink to this heading">¶</a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.data_structures.standard_data_structures.get_dollar_bars">
<span class="sig-name descname"><span class="pre">get_dollar_bars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path_or_df</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">float</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Series</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">70000000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_csv</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_path</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.data_structures.standard_data_structures.get_dollar_bars" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the dollar bars: date_time, open, high, low, close, volume, cum_buy_volume, cum_ticks, cum_dollar_value.</p>
<p>Following the paper “The Volume Clock: Insights into the high frequency paradigm” by Lopez de Prado, et al,
it is suggested that using 1/50 of the average daily dollar value, would result in more desirable statistical
properties.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path_or_df</strong> – (str, iterable of str, or pd.DataFrame) Path to the csv file(s) or Pandas Data Frame containing raw tick data
in the format[date_time, price, volume]</p></li>
<li><p><strong>threshold</strong> – (float, or pd.Series) A cumulative value above this threshold triggers a sample to be taken.
If a series is given, then at each sampling time the closest previous threshold is used.
(Values in the series can only be at times when the threshold is changed, not for every observation)</p></li>
<li><p><strong>batch_size</strong> – (int) The number of rows per batch. Less RAM = smaller batch size.</p></li>
<li><p><strong>verbose</strong> – (bool) Print out batch numbers (True or False)</p></li>
<li><p><strong>to_csv</strong> – (bool) Save bars to csv after every batch run (True or False)</p></li>
<li><p><strong>output_path</strong> – (str) Path to csv file, if to_csv is True</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) Dataframe of dollar bars</p>
</dd>
</dl>
</dd></dl>

<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.data_structures</span> <span class="kn">import</span> <span class="n">standard_data_structures</span>

<span class="c1"># Dollar Bars</span>
<span class="n">dollar</span> <span class="o">=</span> <span class="n">standard_data_structures</span><span class="o">.</span><span class="n">get_dollar_bars</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mi">70000000</span><span class="p">,</span>
                                                   <span class="n">batch_size</span><span class="o">=</span><span class="mi">1000000</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="statistical-properties">
<h5>Statistical Properties<a class="headerlink" href="#statistical-properties" title="Permalink to this heading">¶</a></h5>
<p>The chart below that tick, volume, and dollar bars all exhibit a distribution significantly closer to normal - versus
standard time bars:</p>
<a class="reference internal image-reference" href="_images/normality_graph.png"><img alt="_images/normality_graph.png" class="align-center" src="_images/normality_graph.png" style="width: 653.0999999999999px; height: 508.2px;" /></a>
<div class="line-block">
<div class="line"><br /></div>
</div>
<hr class="docutils" />
<div class="line-block">
<div class="line"><br /></div>
</div>
</section>
</section>
<section id="information-driven-bars">
<h4>Information-Driven Bars<a class="headerlink" href="#information-driven-bars" title="Permalink to this heading">¶</a></h4>
<p>Information-driven bars are based on the notion of sampling a bar when new information arrives to the market. The two
types of information-driven bars implemented are imbalance bars and run bars. For each type, tick, volume, and dollar bars
are included.</p>
<section id="imbalance-bars">
<h5>Imbalance Bars<a class="headerlink" href="#imbalance-bars" title="Permalink to this heading">¶</a></h5>
<p>2 types of imbalance bars are implemented in mlfinlab:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Expected number of ticks, defined as EMA (book implementation)</p></li>
<li><p>Constant number of expected number of ticks.</p></li>
</ol>
</div></blockquote>
<section id="imbalance-bars-generation-algorithm">
<h6>Imbalance Bars Generation Algorithm<a class="headerlink" href="#imbalance-bars-generation-algorithm" title="Permalink to this heading">¶</a></h6>
<p>Let’s discuss the generation of imbalance bars on an example of volume imbalance bars. As it is described in
Advances in Financial Machine Learning book:</p>
<p>First let’s define what is the tick rule:</p>
<div class="math notranslate nohighlight">
\[\begin{split}b_t = \begin{cases} b_{t-1},\;\;\;\;\;\;\;\;\;\; \Delta p_t \mbox{=0} \\ |\Delta p_t| / \Delta p_{t},\;\;\; \Delta p_t \neq\mbox{0} \end{cases}\end{split}\]</div>
<p>For any given <span class="math notranslate nohighlight">\(t\)</span>, where <span class="math notranslate nohighlight">\(p_t\)</span> is the price associated with <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(v_t\)</span> is volume, the tick rule <span class="math notranslate nohighlight">\(b_t\)</span> is defined as:</p>
<p>Tick rule is used as a proxy of trade direction, however, some data providers already provide customers with tick direction, in this case we don’t need to calculate tick rule, just use the provided tick direction instead.</p>
<p>Cumulative volume imbalance from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(T\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[\theta_t = \sum_{t=1}^T b_t*v_t`\]</div>
<p>Where <span class="math notranslate nohighlight">\(T\)</span> is the time when the bar is sampled.</p>
<p>Next we need to define <span class="math notranslate nohighlight">\(E_0[T]\)</span> as the expected number of ticks, the book suggests to use a exponentially weighted moving average (EWMA)
of the expected number of ticks from previously generated bars. Let’s introduce the first hyperparameter for imbalance bars generation:
<strong>num_prev_bars</strong> which corresponds to the window used for EWMA calculation.</p>
<p>Here we face the problem of the first bar’s generation, because we don’t know the expected number of ticks upfront.
To solve this we introduce the second hyperparameter: expected_num_ticks_init which corresponds to initial guess for
<strong>expected number of ticks</strong> before the first imbalance bar is generated.</p>
<p>Bar is sampled when:</p>
<div class="math notranslate nohighlight">
\[|\theta_t| \geq E_0[T]*[2v^+ - E_0[v_t]]\]</div>
<p>To estimate (expected imbalance) we simply calculate the EWMA of volume imbalance from previous bars, that is why we need
to store volume imbalances in an imbalance array, the window for estimation is either <strong>expected_num_ticks_init</strong> before
the first bar is sampled, or expected number of ticks(<span class="math notranslate nohighlight">\(E_0[T]\)</span>) * <strong>num_prev_bars</strong> when the first bar is generated.</p>
<p>Note that when we have at least one imbalance bar generated we update <span class="math notranslate nohighlight">\(2v^+ - E_0[v_t]\)</span> only when the next bar is
sampled and not on every trade observed</p>
</section>
<section id="algorithm-logic">
<h6>Algorithm Logic<a class="headerlink" href="#algorithm-logic" title="Permalink to this heading">¶</a></h6>
<p>Now that we have understood the logic of the imbalance bar generation, let’s understand the process in further detail.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">num_prev_bars</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">expected_num_ticks_init</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="n">expected_num_ticks</span> <span class="o">=</span> <span class="n">expected_num_ticks_init</span>
<span class="n">cum_theta</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">num_ticks</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">imbalance_array</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">imbalance_bars</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">bar_length_array</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">rows</span><span class="p">:</span>
    <span class="c1">#track high,low,close, volume info</span>
    <span class="n">num_ticks</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">tick_rule</span> <span class="o">=</span> <span class="n">get_tick_rule</span><span class="p">(</span><span class="n">price</span><span class="p">,</span> <span class="n">prev_price</span><span class="p">)</span>
    <span class="n">volume_imbalance</span> <span class="o">=</span> <span class="n">tick_rule</span> <span class="o">*</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;volume&#39;</span><span class="p">]</span>
    <span class="n">imbalance_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">volume_imbalance</span><span class="p">)</span>
    <span class="n">cum_theta</span> <span class="o">+=</span> <span class="n">volume_imbalance</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">imbalance_bars</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">imbalance_array</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">expected_num_ticks_init</span><span class="p">:</span>
        <span class="n">expected_imbalance</span> <span class="o">=</span> <span class="n">ewma</span><span class="p">(</span><span class="n">imbalance_array</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="n">expected_num_ticks_init</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">cum_theta</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">expected_num_ticks</span> <span class="o">*</span> <span class="nb">abs</span><span class="p">(</span><span class="n">expected_imbalance</span><span class="p">):</span>
        <span class="n">bar</span> <span class="o">=</span> <span class="n">form_bar</span><span class="p">(</span><span class="nb">open</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="n">low</span><span class="p">,</span> <span class="n">close</span><span class="p">,</span> <span class="n">volume</span><span class="p">)</span>
        <span class="n">imbalance_bars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bar</span><span class="p">)</span>
        <span class="n">bar_length_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">num_ticks</span><span class="p">)</span>
        <span class="n">cum_theta</span><span class="p">,</span> <span class="n">num_ticks</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="n">expected_num_ticks</span> <span class="o">=</span> <span class="n">ewma</span><span class="p">(</span><span class="n">bar_lenght_array</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="n">num_prev_bars</span><span class="p">)</span>
        <span class="n">expected_imbalance</span> <span class="o">=</span> <span class="n">ewma</span><span class="p">(</span><span class="n">imbalance_array</span><span class="p">,</span> <span class="n">window</span> <span class="o">=</span> <span class="n">num_prev_bars</span><span class="o">*</span><span class="n">expected_num_ticks</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that in algorithm pseudo-code we reset <span class="math notranslate nohighlight">\(\theta_t\)</span> when bar is formed, in our case the formula for <span class="math notranslate nohighlight">\(\theta_t\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\theta_t = \sum_{t=t^*}^T b_t*v_t\]</div>
<p>Let’s look at dynamics of <span class="math notranslate nohighlight">\(|\theta_t|\)</span> and <span class="math notranslate nohighlight">\(E_0[T] * |2v^+ - E_0[v_t]|\)</span> to understand why we decided to
reset <span class="math notranslate nohighlight">\(\theta_t\)</span> when a bar is formed. The following figure highlights the dynamics when theta value is reset:</p>
<a class="reference internal image-reference" href="_images/theta_reset.png"><img alt="_images/theta_reset.png" class="align-center" src="_images/theta_reset.png" style="width: 448.0px; height: 336.0px;" /></a>
<p>Note that on the first set of ticks, the threshold condition is not stable. Remember, before the first bar is generated,
the expected imbalance is calculated on every tick with window = expected_num_ticks_init, that is why it changes with every tick.
After the first bar was generated both expected number of ticks (<span class="math notranslate nohighlight">\(E_0[T]\)</span>) and expected volume imbalance
(<span class="math notranslate nohighlight">\(2v^+ - E_0[v_t]\)</span>) are updated only when the next bar is generated</p>
<p>When theta is not reset:</p>
<a class="reference internal image-reference" href="_images/theta_not_reset.png"><img alt="_images/theta_not_reset.png" class="align-center" src="_images/theta_not_reset.png" style="width: 448.0px; height: 336.0px;" /></a>
<p>The reason for that is due to the fact that theta is accumulated when several bars are generated theta value is not
reset <span class="math notranslate nohighlight">\(\Rightarrow\)</span> condition is met on small number of ticks <span class="math notranslate nohighlight">\(\Rightarrow\)</span> length of the next bar converges
to 1 <span class="math notranslate nohighlight">\(\Rightarrow\)</span> bar is sampled on the next consecutive tick.</p>
<p>The logic described above is implemented in the <strong>mlfinlab</strong> package under ImbalanceBars</p>
</section>
<section id="implementation">
<h6>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h6>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.data_structures.imbalance_data_structures.get_ema_dollar_imbalance_bars">
<span class="sig-name descname"><span class="pre">get_ema_dollar_imbalance_bars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path_or_df</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_prev_bars</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expected_imbalance_window</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_num_ticks_init</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_num_ticks_constraints</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000000.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">analyse_thresholds</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_csv</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_path</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.data_structures.imbalance_data_structures.get_ema_dollar_imbalance_bars" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the EMA dollar imbalance bars: date_time, open, high, low, close, volume, cum_buy_volume, cum_ticks, cum_dollar_value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path_or_df</strong> – (str, iterable of str, or pd.DataFrame) Path to the csv file(s) or Pandas Data Frame containing raw tick data
in the format[date_time, price, volume]</p></li>
<li><p><strong>num_prev_bars</strong> – (int) Window size for E[T]s (number of previous bars to use for expected number of ticks estimation)</p></li>
<li><p><strong>expected_imbalance_window</strong> – (int) EMA window used to estimate expected imbalance</p></li>
<li><p><strong>exp_num_ticks_init</strong> – (int) Initial expected number of ticks per bar</p></li>
<li><p><strong>exp_num_ticks_constraints</strong> – (list) Minimum and maximum possible number of expected ticks. Used to control bars sampling convergence</p></li>
<li><p><strong>batch_size</strong> – (int) The number of rows per batch. Less RAM = smaller batch size.</p></li>
<li><p><strong>verbose</strong> – (bool) Print out batch numbers (True or False)</p></li>
<li><p><strong>to_csv</strong> – (bool) Save bars to csv after every batch run (True or False)</p></li>
<li><p><strong>analyse_thresholds</strong> – (bool) Flag to save  and return thresholds used to sample imbalance bars</p></li>
<li><p><strong>output_path</strong> – (str) Path to csv file, if to_csv is True</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) DataFrame of dollar imbalance bars and DataFrame of thresholds, if to_csv=True returns None</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.data_structures.imbalance_data_structures.get_ema_volume_imbalance_bars">
<span class="sig-name descname"><span class="pre">get_ema_volume_imbalance_bars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path_or_df</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_prev_bars</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expected_imbalance_window</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_num_ticks_init</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_num_ticks_constraints</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000000.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">analyse_thresholds</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_csv</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_path</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.data_structures.imbalance_data_structures.get_ema_volume_imbalance_bars" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the EMA volume imbalance bars: date_time, open, high, low, close, volume, cum_buy_volume, cum_ticks, cum_dollar_value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path_or_df</strong> – (str, iterable of str, or pd.DataFrame) Path to the csv file(s) or Pandas Data Frame containing raw tick data
in the format[date_time, price, volume]</p></li>
<li><p><strong>num_prev_bars</strong> – (int) Window size for E[T]s (number of previous bars to use for expected number of ticks estimation)</p></li>
<li><p><strong>expected_imbalance_window</strong> – (int) EMA window used to estimate expected imbalance</p></li>
<li><p><strong>exp_num_ticks_init</strong> – (int) Initial expected number of ticks per bar</p></li>
<li><p><strong>exp_num_ticks_constraints</strong> – (list) Minimum and maximum possible number of expected ticks. Used to control bars sampling convergence</p></li>
<li><p><strong>batch_size</strong> – (int) The number of rows per batch. Less RAM = smaller batch size.</p></li>
<li><p><strong>verbose</strong> – (bool) Print out batch numbers (True or False)</p></li>
<li><p><strong>to_csv</strong> – (bool) Save bars to csv after every batch run (True or False)</p></li>
<li><p><strong>analyse_thresholds</strong> – (bool) Flag to save  and return thresholds used to sample imbalance bars</p></li>
<li><p><strong>output_path</strong> – (str) Path to csv file, if to_csv is True</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) DataFrame of volume imbalance bars and DataFrame of thresholds, if to_csv=True returns None</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.data_structures.imbalance_data_structures.get_ema_tick_imbalance_bars">
<span class="sig-name descname"><span class="pre">get_ema_tick_imbalance_bars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path_or_df</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_prev_bars</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expected_imbalance_window</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_num_ticks_init</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_num_ticks_constraints</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000000.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">analyse_thresholds</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_csv</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_path</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.data_structures.imbalance_data_structures.get_ema_tick_imbalance_bars" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the EMA tick imbalance bars: date_time, open, high, low, close, volume, cum_buy_volume, cum_ticks, cum_dollar_value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path_or_df</strong> – (str, iterable of str, or pd.DataFrame) Path to the csv file(s) or Pandas Data Frame containing raw tick data
in the format[date_time, price, volume]</p></li>
<li><p><strong>num_prev_bars</strong> – (int) Window size for E[T]s (number of previous bars to use for expected number of ticks estimation)</p></li>
<li><p><strong>expected_imbalance_window</strong> – (int) EMA window used to estimate expected imbalance</p></li>
<li><p><strong>exp_num_ticks_init</strong> – (int) Initial expected number of ticks per bar</p></li>
<li><p><strong>exp_num_ticks_constraints</strong> – (array) Minimum and maximum possible number of expected ticks. Used to control bars sampling convergence</p></li>
<li><p><strong>batch_size</strong> – (int) The number of rows per batch. Less RAM = smaller batch size.</p></li>
<li><p><strong>verbose</strong> – (bool) Print out batch numbers (True or False)</p></li>
<li><p><strong>to_csv</strong> – (bool) Save bars to csv after every batch run (True or False)</p></li>
<li><p><strong>analyse_thresholds</strong> – (bool) Flag to save  and return thresholds used to sample imbalance bars</p></li>
<li><p><strong>output_path</strong> – (str) Path to csv file, if to_csv is True</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) DataFrame of tick imbalance bars and DataFrame of thresholds, if to_csv=True returns None</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.data_structures.imbalance_data_structures.get_const_dollar_imbalance_bars">
<span class="sig-name descname"><span class="pre">get_const_dollar_imbalance_bars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path_or_df</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expected_imbalance_window</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_num_ticks_init</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000000.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">analyse_thresholds</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_csv</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_path</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.data_structures.imbalance_data_structures.get_const_dollar_imbalance_bars" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the Const dollar imbalance bars: date_time, open, high, low, close, volume, cum_buy_volume, cum_ticks, cum_dollar_value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path_or_df</strong> – (str, iterable of str, or pd.DataFrame) Path to the csv file(s) or Pandas Data Frame containing raw tick data
in the format[date_time, price, volume]</p></li>
<li><p><strong>expected_imbalance_window</strong> – (int) EMA window used to estimate expected imbalance</p></li>
<li><p><strong>exp_num_ticks_init</strong> – (int) Initial expected number of ticks per bar</p></li>
<li><p><strong>batch_size</strong> – (int) The number of rows per batch. Less RAM = smaller batch size.</p></li>
<li><p><strong>verbose</strong> – (bool) Print out batch numbers (True or False)</p></li>
<li><p><strong>to_csv</strong> – (bool) Save bars to csv after every batch run (True or False)</p></li>
<li><p><strong>analyse_thresholds</strong> – (bool) Flag to save  and return thresholds used to sample imbalance bars</p></li>
<li><p><strong>output_path</strong> – (str) Path to csv file, if to_csv is True</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) DataFrame of dollar imbalance bars and DataFrame of thresholds, if to_csv=True returns None</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.data_structures.imbalance_data_structures.get_const_volume_imbalance_bars">
<span class="sig-name descname"><span class="pre">get_const_volume_imbalance_bars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path_or_df</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expected_imbalance_window</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_num_ticks_init</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000000.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">analyse_thresholds</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_csv</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_path</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.data_structures.imbalance_data_structures.get_const_volume_imbalance_bars" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the Const volume imbalance bars: date_time, open, high, low, close, volume, cum_buy_volume, cum_ticks, cum_dollar_value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path_or_df</strong> – (str, iterable of str, or pd.DataFrame) Path to the csv file(s) or Pandas Data Frame containing raw tick data
in the format[date_time, price, volume]</p></li>
<li><p><strong>expected_imbalance_window</strong> – (int) EMA window used to estimate expected imbalance</p></li>
<li><p><strong>exp_num_ticks_init</strong> – (int) Initial expected number of ticks per bar</p></li>
<li><p><strong>batch_size</strong> – (int) The number of rows per batch. Less RAM = smaller batch size.</p></li>
<li><p><strong>verbose</strong> – (bool) Print out batch numbers (True or False)</p></li>
<li><p><strong>to_csv</strong> – (bool) Save bars to csv after every batch run (True or False)</p></li>
<li><p><strong>analyse_thresholds</strong> – (bool) Flag to save  and return thresholds used to sample imbalance bars</p></li>
<li><p><strong>output_path</strong> – (str) Path to csv file, if to_csv is True</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) DataFrame of volume imbalance bars and DataFrame of thresholds, if to_csv=True returns None</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.data_structures.imbalance_data_structures.get_const_tick_imbalance_bars">
<span class="sig-name descname"><span class="pre">get_const_tick_imbalance_bars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path_or_df</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expected_imbalance_window</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_num_ticks_init</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000000.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">analyse_thresholds</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_csv</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_path</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.data_structures.imbalance_data_structures.get_const_tick_imbalance_bars" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the Const tick imbalance bars: date_time, open, high, low, close, volume, cum_buy_volume, cum_ticks, cum_dollar_value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path_or_df</strong> – (str or pd.DataFrame) Path to the csv file or Pandas Data Frame containing raw tick data in the format[date_time, price, volume]</p></li>
<li><p><strong>expected_imbalance_window</strong> – (int) EMA window used to estimate expected imbalance</p></li>
<li><p><strong>exp_num_ticks_init</strong> – (int) Initial expected number of ticks per bar</p></li>
<li><p><strong>batch_size</strong> – (int) The number of rows per batch. Less RAM = smaller batch size.</p></li>
<li><p><strong>verbose</strong> – (bool) Print out batch numbers (True or False)</p></li>
<li><p><strong>to_csv</strong> – (bool) Save bars to csv after every batch run (True or False)</p></li>
<li><p><strong>analyse_thresholds</strong> – (bool) Flag to save  and return thresholds used to sample imbalance bars</p></li>
<li><p><strong>output_path</strong> – (str) Path to csv file, if to_csv is True</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) DataFrame of tick imbalance bars and DataFrame of thresholds, if to_csv=True returns None</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="example">
<h6>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h6>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.data_structures</span> <span class="kn">import</span> <span class="n">get_ema_dollar_imbalance_bars</span><span class="p">,</span> <span class="n">get_const_dollar_imbalance_bars</span>

<span class="c1"># EMA, Const Dollar Imbalance Bars</span>
<span class="n">dollar_imbalance_ema</span> <span class="o">=</span> <span class="n">get_ema_dollar_imbalance_bars</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">num_prev_bars</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">exp_num_ticks_init</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span>
                                                     <span class="n">exp_num_ticks_constraints</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">],</span> <span class="n">expected_imbalance_window</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>

<span class="n">dollar_imbalance_const</span> <span class="o">=</span> <span class="n">get_const_dollar_imbalance_bars</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">exp_num_ticks_init</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">expected_imbalance_window</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<hr class="docutils" />
<div class="line-block">
<div class="line"><br /></div>
</div>
</section>
</section>
<section id="run-bars">
<h5>Run Bars<a class="headerlink" href="#run-bars" title="Permalink to this heading">¶</a></h5>
<p>Run bars share the same mathematical structure as imbalance bars, however, instead of looking at each individual trade,
we are looking at sequences of trades in the same direction. The idea is that we are trying to detect order flow imbalance
caused by actions such as large traders sweeping the order book or iceberg orders.</p>
<p>2 types of run bars are implemented in mlfinlab:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Expected number of ticks, defined as EWMA (book implementation)</p></li>
<li><p>Constant number of expected number of ticks.</p></li>
</ol>
</div></blockquote>
<section id="id1">
<h6>Implementation<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h6>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.data_structures.run_data_structures.get_ema_dollar_run_bars">
<span class="sig-name descname"><span class="pre">get_ema_dollar_run_bars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path_or_df</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_prev_bars</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expected_imbalance_window</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_num_ticks_init</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_num_ticks_constraints</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000000.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">analyse_thresholds</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_csv</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_path</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.data_structures.run_data_structures.get_ema_dollar_run_bars" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the EMA dollar run bars: date_time, open, high, low, close, volume, cum_buy_volume, cum_ticks, cum_dollar_value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path_or_df</strong> – (str, iterable of str, or pd.DataFrame) Path to the csv file(s) or Pandas Data Frame containing raw tick data
in the format[date_time, price, volume]</p></li>
<li><p><strong>num_prev_bars</strong> – (int) Window size for E[T]s (number of previous bars to use for expected number of ticks estimation)</p></li>
<li><p><strong>expected_imbalance_window</strong> – (int) EMA window used to estimate expected run</p></li>
<li><p><strong>exp_num_ticks_init</strong> – (int) Initial expected number of ticks per bar</p></li>
<li><p><strong>exp_num_ticks_constraints</strong> – (list) Minimum and maximum possible number of expected ticks. Used to control bars sampling convergence</p></li>
<li><p><strong>batch_size</strong> – (int) The number of rows per batch. Less RAM = smaller batch size.</p></li>
<li><p><strong>verbose</strong> – (bool) Print out batch numbers (True or False)</p></li>
<li><p><strong>to_csv</strong> – (bool) Save bars to csv after every batch run (True or False)</p></li>
<li><p><strong>analyse_thresholds</strong> – (bool) Flag to save  and return thresholds used to sample run bars</p></li>
<li><p><strong>output_path</strong> – (str) Path to csv file, if to_csv is True</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) DataFrame of dollar bars and DataFrame of thresholds</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.data_structures.run_data_structures.get_ema_volume_run_bars">
<span class="sig-name descname"><span class="pre">get_ema_volume_run_bars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path_or_df</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_prev_bars</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expected_imbalance_window</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_num_ticks_init</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_num_ticks_constraints</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000000.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">analyse_thresholds</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_csv</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_path</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.data_structures.run_data_structures.get_ema_volume_run_bars" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the EMA volume run bars: date_time, open, high, low, close, volume, cum_buy_volume, cum_ticks, cum_dollar_value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_pats_or_df</strong> – (str, iterable of str, or pd.DataFrame) Path to the csv file(s) or Pandas Data Frame containing raw tick data
in the format[date_time, price, volume]</p></li>
<li><p><strong>num_prev_bars</strong> – (int) Window size for E[T]s (number of previous bars to use for expected number of ticks estimation)</p></li>
<li><p><strong>expected_imbalance_window</strong> – (int) EMA window used to estimate expected run</p></li>
<li><p><strong>exp_num_ticks_init</strong> – (int) Initial expected number of ticks per bar</p></li>
<li><p><strong>exp_num_ticks_constraints</strong> – (list) Minimum and maximum possible number of expected ticks. Used to control bars sampling convergence</p></li>
<li><p><strong>batch_size</strong> – (int) The number of rows per batch. Less RAM = smaller batch size.</p></li>
<li><p><strong>verbose</strong> – (bool) Print out batch numbers (True or False)</p></li>
<li><p><strong>to_csv</strong> – (bool) Save bars to csv after every batch run (True or False)</p></li>
<li><p><strong>analyse_thresholds</strong> – (bool) Flag to save  and return thresholds used to sample run bars</p></li>
<li><p><strong>output_path</strong> – (str) Path to csv file, if to_csv is True</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) DataFrame of volume bars and DataFrame of thresholds</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.data_structures.run_data_structures.get_ema_tick_run_bars">
<span class="sig-name descname"><span class="pre">get_ema_tick_run_bars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path_or_df</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_prev_bars</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expected_imbalance_window</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_num_ticks_init</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_num_ticks_constraints</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000000.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">analyse_thresholds</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_csv</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_path</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.data_structures.run_data_structures.get_ema_tick_run_bars" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the EMA tick run bars: date_time, open, high, low, close, volume, cum_buy_volume, cum_ticks, cum_dollar_value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path_or_df</strong> – (str, iterable of str, or pd.DataFrame) Path to the csv file(s) or Pandas Data Frame containing raw tick data
in the format[date_time, price, volume]</p></li>
<li><p><strong>num_prev_bars</strong> – (int) Window size for E[T]s (number of previous bars to use for expected number of ticks estimation)</p></li>
<li><p><strong>expected_imbalance_window</strong> – (int) EMA window used to estimate expected run</p></li>
<li><p><strong>exp_num_ticks_init</strong> – (int) Initial expected number of ticks per bar</p></li>
<li><p><strong>exp_num_ticks_constraints</strong> – (list) Minimum and maximum possible number of expected ticks. Used to control bars sampling convergence</p></li>
<li><p><strong>batch_size</strong> – (int) The number of rows per batch. Less RAM = smaller batch size.</p></li>
<li><p><strong>verbose</strong> – (bool) Print out batch numbers (True or False)</p></li>
<li><p><strong>to_csv</strong> – (bool) Save bars to csv after every batch run (True or False)</p></li>
<li><p><strong>analyse_thresholds</strong> – (bool) Flag to save  and return thresholds used to sample run bars</p></li>
<li><p><strong>output_path</strong> – (str) Path to csv file, if to_csv is True</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) DataFrame of tick bars and DataFrame of thresholds</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.data_structures.run_data_structures.get_const_dollar_run_bars">
<span class="sig-name descname"><span class="pre">get_const_dollar_run_bars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path_or_df</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_prev_bars</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expected_imbalance_window</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_num_ticks_init</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000000.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">analyse_thresholds</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_csv</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_path</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.data_structures.run_data_structures.get_const_dollar_run_bars" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the Const dollar run bars: date_time, open, high, low, close, volume, cum_buy_volume, cum_ticks, cum_dollar_value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path_or_df</strong> – (str, iterable of str, or pd.DataFrame) Path to the csv file(s) or Pandas Data Frame containing raw tick data
in the format[date_time, price, volume]</p></li>
<li><p><strong>num_prev_bars</strong> – (int) Window size for estimating buy ticks proportion (number of previous bars to use in EWMA)</p></li>
<li><p><strong>expected_imbalance_window</strong> – (int) EMA window used to estimate expected imbalance</p></li>
<li><p><strong>exp_num_ticks_init</strong> – (int) Initial expected number of ticks per bar</p></li>
<li><p><strong>batch_size</strong> – (int) The number of rows per batch. Less RAM = smaller batch size.</p></li>
<li><p><strong>verbose</strong> – (bool) Print out batch numbers (True or False)</p></li>
<li><p><strong>to_csv</strong> – (bool) Save bars to csv after every batch run (True or False)</p></li>
<li><p><strong>analyse_thresholds</strong> – (bool) Flag to save  and return thresholds used to sample run bars</p></li>
<li><p><strong>output_path</strong> – (str) Path to csv file, if to_csv is True</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) DataFrame of dollar bars and DataFrame of thresholds, if to_csv=True returns None</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.data_structures.run_data_structures.get_const_volume_run_bars">
<span class="sig-name descname"><span class="pre">get_const_volume_run_bars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path_or_df</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_prev_bars</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expected_imbalance_window</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_num_ticks_init</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000000.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">analyse_thresholds</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_csv</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_path</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.data_structures.run_data_structures.get_const_volume_run_bars" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the Const volume run bars: date_time, open, high, low, close, volume, cum_buy_volume, cum_ticks, cum_dollar_value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path_or_df</strong> – (str, iterable of str, or pd.DataFrame) Path to the csv file(s) or Pandas Data Frame containing raw tick data
in the format[date_time, price, volume]</p></li>
<li><p><strong>num_prev_bars</strong> – (int) Window size for estimating buy ticks proportion (number of previous bars to use in EWMA)</p></li>
<li><p><strong>expected_imbalance_window</strong> – (int) EMA window used to estimate expected imbalance</p></li>
<li><p><strong>exp_num_ticks_init</strong> – (int) Initial expected number of ticks per bar</p></li>
<li><p><strong>batch_size</strong> – (int) The number of rows per batch. Less RAM = smaller batch size.</p></li>
<li><p><strong>verbose</strong> – (bool) Print out batch numbers (True or False)</p></li>
<li><p><strong>to_csv</strong> – (bool) Save bars to csv after every batch run (True or False)</p></li>
<li><p><strong>analyse_thresholds</strong> – (bool) Flag to save  and return thresholds used to sample run bars</p></li>
<li><p><strong>output_path</strong> – (str) Path to csv file, if to_csv is True</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) DataFrame of volume bars and DataFrame of thresholds</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.data_structures.run_data_structures.get_const_tick_run_bars">
<span class="sig-name descname"><span class="pre">get_const_tick_run_bars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path_or_df</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_prev_bars</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expected_imbalance_window</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_num_ticks_init</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20000000.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">analyse_thresholds</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_csv</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_path</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.data_structures.run_data_structures.get_const_tick_run_bars" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the Const tick run bars: date_time, open, high, low, close, volume, cum_buy_volume, cum_ticks, cum_dollar_value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path_or_df</strong> – (str, iterable of str, or pd.DataFrame) Path to the csv file(s) or Pandas Data Frame containing raw tick data
in the format[date_time, price, volume]</p></li>
<li><p><strong>num_prev_bars</strong> – (int) Window size for estimating buy ticks proportion (number of previous bars to use in EWMA)</p></li>
<li><p><strong>expected_imbalance_window</strong> – (int) EMA window used to estimate expected imbalance</p></li>
<li><p><strong>exp_num_ticks_init</strong> – (int) Initial expected number of ticks per bar</p></li>
<li><p><strong>batch_size</strong> – (int) The number of rows per batch. Less RAM = smaller batch size.</p></li>
<li><p><strong>verbose</strong> – (bool) Print out batch numbers (True or False)</p></li>
<li><p><strong>to_csv</strong> – (bool) Save bars to csv after every batch run (True or False)</p></li>
<li><p><strong>analyse_thresholds</strong> – (bool) Flag to save  and return thresholds used to sample run bars</p></li>
<li><p><strong>output_path</strong> – (str) Path to csv file, if to_csv is True</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) DataFrame of tick bars and DataFrame of thresholds</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id2">
<h6>Example<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h6>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.data_structures</span> <span class="kn">import</span> <span class="n">get_ema_dollar_run_bars</span><span class="p">,</span> <span class="n">get_const_dollar_run_bars</span>

<span class="c1"># EMA, Const Dollar Imbalance Bars</span>
<span class="n">dollar_imbalance_ema</span> <span class="o">=</span> <span class="n">get_ema_dollar_run_bars</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">num_prev_bars</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">exp_num_ticks_init</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span>
                                                <span class="n">exp_num_ticks_constraints</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">],</span> <span class="n">expected_imbalance_window</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>

<span class="n">dollar_imbalance_const</span> <span class="o">=</span> <span class="n">get_const_dollar_run_bars</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">num_prev_bars</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">exp_num_ticks_init</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span>
                                                   <span class="n">expected_imbalance_window</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<hr class="docutils" />
<div class="line-block">
<div class="line"><br /></div>
</div>
</section>
</section>
</section>
<section id="research-notebooks">
<h4>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h4>
<p>The following research notebooks can be used to better understand the previously discussed data structures</p>
<section id="id3">
<h5>Standard Bars<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Financial%20Data%20Structures/Getting%20Started.ipynb">Getting Started</a></p></li>
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Financial%20Data%20Structures/Sample_Techniques.ipynb">Sample Techniques</a></p></li>
</ul>
</section>
<section id="id4">
<h5>Imbalance Bars<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Financial%20Data%20Structures/Dollar-Imbalance-Bars.ipynb">Imbalance Bars</a></p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<hr class="docutils" />
<div class="line-block">
<div class="line"><br /></div>
</div>
</section>
</section>
<section id="data-preparation-tutorial">
<h4>Data Preparation Tutorial<a class="headerlink" href="#data-preparation-tutorial" title="Permalink to this heading">¶</a></h4>
<p>First import your tick data.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Required Imports</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data.csv&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>In order to utilize the bar sampling methods presented below, our data must first be formatted properly.
Many data vendors will let you choose the format of your raw tick data files. We want to only focus on the following
3 columns: date_time, price, volume. The reason for this is to minimise the size of the csv files and the amount of time
when reading in the files.</p>
<p>Our data is sourced from TickData LLC which provides software called TickWrite, to aid in the formatting of saved files.
This allows us to save csv files in the format date_time, price, volume. (If you don’t use TickWrite then make sure to pre-format your files)</p>
<p>For this tutorial we will assume that you need to first do some pre-processing and then save your data to a csv file.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Don&#39;t convert to datetime here, it will take forever to convert</span>
<span class="c1"># on account of the sheer size of tick data files.</span>
<span class="n">date_time</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Date&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Time&#39;</span><span class="p">]</span>
<span class="n">new_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">date_time</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Price&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Volume&#39;</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">new_data</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;price&#39;</span><span class="p">,</span> <span class="s1">&#39;volume&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>Initially, your instinct may be to pass an in-memory DataFrame object but the truth is when you’re running the function
in production, your raw tick data csv files will be way too large to hold in memory. We used the subset 2011 to 2019 and
it was more than 25 gigs. It is for this reason that the mlfinlab package suggests using a file path to read the raw data
files from disk.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Save to csv</span>
<span class="n">new_data</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<span id="document-implementations/filters"></span><section id="filters">
<span id="implementations-filters"></span><h3>Filters<a class="headerlink" href="#filters" title="Permalink to this heading">¶</a></h3>
<p>Filters are used to filter events based on some kind of trigger. For example a structural break filter can be
used to filter events where a structural break occurs. In Triple-Barrier labeling, this event is then used to measure
the return from the event to some event horizon, say a day.</p>
<p>The core idea is that labeling every trading day is a fools errand, researchers should instead focus on forecasting specific
market anomalies or how the market moves after an event.</p>
<section id="cusum-filter">
<h4>CUSUM Filter<a class="headerlink" href="#cusum-filter" title="Permalink to this heading">¶</a></h4>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.filters.filters.cusum_filter">
<span class="sig-name descname"><span class="pre">cusum_filter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">raw_time_series</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_stamps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.filters.filters.cusum_filter" title="Permalink to this definition">¶</a></dt>
<dd><p>Advances in Financial Machine Learning, Snippet 2.4, page 39.</p>
<p>The Symmetric Dynamic/Fixed CUSUM Filter.</p>
<p>The CUSUM filter is a quality-control method, designed to detect a shift in the mean value of a measured quantity
away from a target value. The filter is set up to identify a sequence of upside or downside divergences from any
reset level zero. We sample a bar t if and only if S_t &gt;= threshold, at which point S_t is reset to 0.</p>
<p>One practical aspect that makes CUSUM filters appealing is that multiple events are not triggered by raw_time_series
hovering around a threshold level, which is a flaw suffered by popular market signals such as Bollinger Bands.
It will require a full run of length threshold for raw_time_series to trigger an event.</p>
<p>Once we have obtained this subset of event-driven bars, we will let the ML algorithm determine whether the occurrence
of such events constitutes actionable intelligence. Below is an implementation of the Symmetric CUSUM filter.</p>
<p>Note: As per the book this filter is applied to closing prices but we extended it to also work on other
time series such as volatility.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>raw_time_series</strong> – (pd.Series) Close prices (or other time series, e.g. volatility).</p></li>
<li><p><strong>threshold</strong> – (float or pd.Series) When the abs(change) is larger than the threshold, the function captures
it as an event, can be dynamic if threshold is pd.Series</p></li>
<li><p><strong>time_stamps</strong> – (bool) Default is to return a DateTimeIndex, change to false to have it return a list.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(datetime index vector) Vector of datetimes when the events occurred. This is used later to sample.</p>
</dd>
</dl>
</dd></dl>

<p>An example showing how the CUSUM filter can be used to downsample a time series of close prices can be seen below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.filters</span> <span class="kn">import</span> <span class="n">cusum_filter</span>

<span class="n">cusum_events</span> <span class="o">=</span> <span class="n">cusum_filter</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;close&#39;</span><span class="p">],</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="z-score-filter">
<h4>Z-Score Filter<a class="headerlink" href="#z-score-filter" title="Permalink to this heading">¶</a></h4>
<p>The <a class="reference external" href="https://stackoverflow.com/questions/22583391/peak-signal-detection-in-realtime-timeseries-data">Z-Score</a> filter is
used to define explosive/peak points in time series.</p>
<p>It uses rolling simple moving average, rolling simple moving standard deviation, and z_score(threshold). When the current
time series value exceeds (rolling average + z_score * rolling std) an event is triggered.</p>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.filters.filters.z_score_filter">
<span class="sig-name descname"><span class="pre">z_score_filter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">raw_time_series</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean_window</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std_window</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">z_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_stamps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.filters.filters.z_score_filter" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter which implements z_score filter
(<a class="reference external" href="https://stackoverflow.com/questions/22583391/peak-signal-detection-in-realtime-timeseries-data">https://stackoverflow.com/questions/22583391/peak-signal-detection-in-realtime-timeseries-data</a>)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>raw_time_series</strong> – (pd.Series) Close prices (or other time series, e.g. volatility).</p></li>
<li><p><strong>mean_window</strong> – (int): Rolling mean window</p></li>
<li><p><strong>std_window</strong> – (int): Rolling std window</p></li>
<li><p><strong>z_score</strong> – (float): Number of standard deviations to trigger the event</p></li>
<li><p><strong>time_stamps</strong> – (bool) Default is to return a DateTimeIndex, change to false to have it return a list.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(datetime index vector) Vector of datetimes when the events occurred. This is used later to sample.</p>
</dd>
</dl>
</dd></dl>

<p>An example of how the Z-score filter can be used to downsample a time series:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlb.filters</span> <span class="kn">import</span> <span class="n">z_score_filter</span>

<span class="n">z_score_events</span> <span class="o">=</span> <span class="n">z_score_filter</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;close&#39;</span><span class="p">],</span> <span class="n">mean_window</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">std_window</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">z_score</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<span id="document-implementations/frac_diff"></span><section id="fractionally-differentiated-features">
<span id="implementations-frac-diff"></span><h3>Fractionally Differentiated Features<a class="headerlink" href="#fractionally-differentiated-features" title="Permalink to this heading">¶</a></h3>
<p>One of the challenges of quantitative analysis in finance is that time series of prices have trends or a non-constant mean.
This makes the time series is non-stationary. A non-stationary time series are hard to work with when we want to do inferential
analysis based on the variance of returns, or probability of loss.</p>
<p>Many supervised learning algorithms have the underlying assumption that the data is stationary. Specifically, in supervised
learning, one needs to map hitherto unseen observations to a set of labeled examples and determine the label of the new observation.</p>
<p>According to Marcos Lopez de Prado: “If the features are not stationary we cannot map the new observation
to a large number of known examples”. Making time series stationary often requires stationary data transformations,
such as integer differentiation. These transformations remove memory from the series. The method proposed by Marcos Lopez de Prado aims
to make data stationary while preserving as much memory as possible, as it’s the memory part that has predictive power.</p>
<p>Fractionally differentiated features approach allows differentiating a time series to the point where the series is
stationary, but not over differencing such that we lose all predictive power.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><strong>Underlying Literature</strong></p>
<p>The following sources elaborate extensively on the topic:</p>
<ul class="simple">
<li><p><strong>Advances in Financial Machine Learning, Chapter 5</strong> <em>by</em> Marcos Lopez de Prado. <em>Describes the motivation behind the Fractionally Differentiated Features and algorithms in more detail</em></p></li>
</ul>
</div>
<section id="fixed-width-window-fracdiff">
<h4>Fixed-width Window Fracdiff<a class="headerlink" href="#fixed-width-window-fracdiff" title="Permalink to this heading">¶</a></h4>
<p>The following description is based on <strong>Chapter 5 of Advances in Financial Machine Learning</strong>:</p>
<p>Using a positive coefficient <span class="math notranslate nohighlight">\(d\)</span> the memory can be preserved:</p>
<div class="math notranslate nohighlight">
\[\widetilde{X}_{t} = \sum_{k=0}^{\infty}\omega_{k}X_{t-k}\]</div>
<p>where <span class="math notranslate nohighlight">\(X\)</span> is the original series, the <span class="math notranslate nohighlight">\(\widetilde{X}\)</span> is the fractionally differentiated one, and
the weights <span class="math notranslate nohighlight">\(\omega\)</span> are defined as follows:</p>
<div class="math notranslate nohighlight">
\[\omega = \{1, -d, \frac{d(d-1)}{2!}, -\frac{d(d-1)(d-2)}{3!}, ..., (-1)^{k}\prod_{i=0}^{k-1}\frac{d-i}{k!}, ...\}\]</div>
<p>“When <span class="math notranslate nohighlight">\(d\)</span> is a positive integer number, <span class="math notranslate nohighlight">\(\prod_{i=0}^{k-1}\frac{d-i}{k!} = 0, \forall k &gt; d\)</span>, and memory
beyond that point is cancelled.”</p>
<p>Given a series of <span class="math notranslate nohighlight">\(T\)</span> observations, for each window length <span class="math notranslate nohighlight">\(l\)</span>, the relative weight-loss can be calculated as:</p>
<div class="math notranslate nohighlight">
\[\lambda_{l} = \frac{\sum_{j=T-l}^{T} | \omega_{j} | }{\sum_{i=0}^{T-l} | \omega_{i} |}\]</div>
<p>The weight-loss calculation is attributed to a fact that “the initial points have a different amount of memory”
( <span class="math notranslate nohighlight">\(\widetilde{X}_{T-l}\)</span> uses <span class="math notranslate nohighlight">\(\{ \omega \}, k=0, .., T-l-1\)</span> ) compared to the final points
( <span class="math notranslate nohighlight">\(\widetilde{X}_{T}\)</span> uses <span class="math notranslate nohighlight">\(\{ \omega \}, k=0, .., T-1\)</span> ).</p>
<p>With a defined tolerance level <span class="math notranslate nohighlight">\(\tau \in [0, 1]\)</span> a <span class="math notranslate nohighlight">\(l^{*}\)</span> can be calculated so that <span class="math notranslate nohighlight">\(\lambda_{l^{*}} \le \tau\)</span>
and <span class="math notranslate nohighlight">\(\lambda_{l^{*}+1} &gt; \tau\)</span>, which determines the first <span class="math notranslate nohighlight">\(\{ \widetilde{X}_{t} \}_{t=1,...,l^{*}}\)</span> where “the
weight-loss is beyond the acceptable threshold <span class="math notranslate nohighlight">\(\lambda_{t} &gt; \tau\)</span> .”</p>
<p>Without the control of weight-loss the <span class="math notranslate nohighlight">\(\widetilde{X}\)</span> series will pose a severe negative drift. This problem
is corrected by using a fixed-width window and not an expanding one.</p>
<p>With a fixed-width window, the weights <span class="math notranslate nohighlight">\(\omega\)</span> are adjusted to <span class="math notranslate nohighlight">\(\widetilde{\omega}\)</span> :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\widetilde{\omega}_{k} =
\begin{cases}
\omega_{k}, &amp; \text{if } k \le l^{*} \\
0, &amp; \text{if } k &gt; l^{*}
\end{cases}\end{split}\]</div>
<p>Therefore, the fractionally differentiated series is calculated as:</p>
<div class="math notranslate nohighlight">
\[\widetilde{X}_{t} = \sum_{k=0}^{l^{*}}\widetilde{\omega_{k}}X_{t-k}\]</div>
<p>for <span class="math notranslate nohighlight">\(t = T - l^{*} + 1, ..., T\)</span></p>
<p>The following graph shows a fractionally differenced series plotted over the original closing price series:</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="_images/frac_diff_graph.png"><img alt="Fractionally Differentiated Series" src="_images/frac_diff_graph.png" style="width: 504.8px; height: 272.8px;" /></a>
<figcaption>
<p><span class="caption-text">Fractionally differentiated series with a fixed-width window <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3447398">(Lopez de Prado 2018)</a></span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>A deeper analysis of the problem and the tests of the method on various futures is available in the
<strong>Chapter 5 of Advances in Financial Machine Learning</strong>.</p>
</div>
<section id="implementation">
<h5>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h5>
<p>The following function implemented in mlfinlab can be used to derive fractionally differentiated features.</p>
</section>
</section>
<section id="stationarity-with-maximum-memory-representation">
<h4>Stationarity With Maximum Memory Representation<a class="headerlink" href="#stationarity-with-maximum-memory-representation" title="Permalink to this heading">¶</a></h4>
<p>The following description is based on <strong>Chapter 5 of Advances in Financial Machine Learning</strong>:</p>
<p>Applying the fixed-width window fracdiff (FFD) method on series, the minimum coefficient <span class="math notranslate nohighlight">\(d^{*}\)</span> can be computed.
With this <span class="math notranslate nohighlight">\(d^{*}\)</span> the resulting fractionally differentiated series is stationary. This coefficient
<span class="math notranslate nohighlight">\(d^{*}\)</span> quantifies the amount of memory that needs to be removed to achieve stationarity.</p>
<p>If the input series:</p>
<ul class="simple">
<li><p>is already stationary, then <span class="math notranslate nohighlight">\(d^{*}=0\)</span>.</p></li>
<li><p>contains a unit root, then <span class="math notranslate nohighlight">\(d^{*} &lt; 1\)</span>.</p></li>
<li><p>exhibits explosive behavior (like in a bubble), then <span class="math notranslate nohighlight">\(d^{*} &gt; 1\)</span>.</p></li>
</ul>
<p>A case of particular interest is <span class="math notranslate nohighlight">\(0 &lt; d^{*} \ll 1\)</span>, when the original series is “mildly non-stationary.”
In this case, although differentiation is needed, a full integer differentiation removes
excessive memory (and predictive power).</p>
<p>The following grap shows how the output of a <code class="docutils literal notranslate"><span class="pre">plot_min_ffd</span></code> function looks.</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="_images/plot_min_ffd_graph.png"><img alt="Minimum D value that passes the ADF test" src="_images/plot_min_ffd_graph.png" style="width: 498.40000000000003px; height: 375.20000000000005px;" /></a>
<figcaption>
<p><span class="caption-text">ADF statistic as a function of d</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The right y-axis on the plot is the ADF statistic computed on the input series downsampled
to a daily frequency.</p>
<p>The x-axis displays the d value used to generate the series on which the ADF statistic is computed.</p>
<p>The left y-axis plots the correlation between the original series ( <span class="math notranslate nohighlight">\(d = 0\)</span> ) and the differentiated
series at various <span class="math notranslate nohighlight">\(d\)</span> values.</p>
<p>The horizontal dotted line is the ADF test critical value at a 95% confidence level. Based on
where the ADF statistic crosses this threshold, the minimum <span class="math notranslate nohighlight">\(d\)</span> value can be defined.</p>
<p>The correlation coefficient at a given <span class="math notranslate nohighlight">\(d\)</span> value can be used to determine the amount of memory
that was given up to achieve stationarity. (The higher the correlation - the less memory was given up)</p>
<p>According to Lopez de Prado:</p>
<p>“Virtually all finance papers attempt to recover stationarity by applying an integer
differentiation <span class="math notranslate nohighlight">\(d = 1\)</span>, which means that most studies have over-differentiated
the series, that is, they have removed much more memory than was necessary to
satisfy standard econometric assumptions.”</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>An example on how the resulting figure can be analyzed is available in
<strong>Chapter 5 of Advances in Financial Machine Learning</strong>.</p>
</div>
<section id="id1">
<h5>Implementation<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h5>
<p>The following function implemented in mlfinlab can be used to achieve stationarity with maximum memory representation.</p>
</section>
</section>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h4>
<p>Given that we know the amount we want to difference our price series, fractionally differentiated features, and the
minimum d value that passes the ADF test can be derived as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">from</span> <span class="nn">mlfinlab.features.fracdiff</span> <span class="kn">import</span> <span class="n">frac_diff_ffd</span><span class="p">,</span> <span class="n">plot_min_ffd</span>

<span class="c1"># Import price data</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">)</span>

<span class="c1"># Deriving the fractionally differentiated features</span>
<span class="n">frac_diff_series</span> <span class="o">=</span> <span class="n">frac_diff_ffd</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;close&#39;</span><span class="p">],</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Plotting the graph to find the minimum d</span>
<span class="c1"># Make sure the input dataframe has a &#39;close&#39; column</span>
<span class="n">plot_min_ffd</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="research-notebook">
<h4>Research Notebook<a class="headerlink" href="#research-notebook" title="Permalink to this heading">¶</a></h4>
<p>The following research notebook can be used to better understand fractionally differentiated features.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Fractionally%20Differentiated%20Features/Chapter5_Exercises.ipynb">Fractionally Differentiated Features</a></p></li>
</ul>
</section>
</section>
<span id="document-implementations/structural_breaks"></span><section id="structural-breaks">
<span id="implementations-structural-breaks"></span><h3>Structural Breaks<a class="headerlink" href="#structural-breaks" title="Permalink to this heading">¶</a></h3>
<p>This implementation is based on Chapter 17 of the book Advances in Financial Machine Learning. Structural breaks, like
the transition from one market regime to another, represent the shift in the behaviour of market participants.</p>
<p>The first market participant to notice the changes in the market can adapt to them before others and, consequently,
gain an advantage over market participants who have not yet noticed market regime changes.</p>
<p>To quote Marcos Lopez de Prado, “Structural breaks offer some of the best risk/rewards”.</p>
<p>We can classify the structural break in two general categories:</p>
<ol class="arabic simple">
<li><p><strong>CUSUM tests</strong>: These test whether the cumulative forecasting errors significantly deviate from white noise.</p></li>
<li><p><strong>Explosiveness tests</strong>: Beyond deviation from white noise, these test whether the process exhibits exponential
growth or collapse, as this is inconsistent with a random walk or stationary process, and it is unsustainable in the long run.</p></li>
</ol>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><strong>Underlying Literature</strong></p>
<p>The following sources elaborate extensively on the topic:</p>
<ul class="simple">
<li><p><strong>Advances in Financial Machine Learning</strong> <em>by</em> Marcos Lopez de Prado <em>Chapter 17 describes structural breaks in more detail.</em></p></li>
<li><p><strong>Testing for Speculative Bubbles in Stock Markets: A Comparison of Alternative Methods</strong> <em>by</em> Ulrich Homm <em>and</em> Jorg Breitung <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.511.6559&amp;rep=rep1&amp;type=pdf">available here</a>. <em>Explains the Chu-Stinchcombe-White CUSUM Test in more detail.</em></p></li>
<li><p><strong>Tests of Equality Between Sets of Coefficients in Two Linear Regressions</strong> <em>by</em> Gregory C. Chow <a class="reference external" href="http://web.sonoma.edu/users/c/cuellar/econ411/chow">available here</a>. <em>A work that inspired a family of explosiveness tests.</em></p></li>
</ul>
</div>
<section id="cusum-tests">
<h4>CUSUM tests<a class="headerlink" href="#cusum-tests" title="Permalink to this heading">¶</a></h4>
<section id="chu-stinchcombe-white-cusum-test-on-levels">
<h5>Chu-Stinchcombe-White CUSUM Test on Levels<a class="headerlink" href="#chu-stinchcombe-white-cusum-test-on-levels" title="Permalink to this heading">¶</a></h5>
<p>We are given a set of observations <span class="math notranslate nohighlight">\(t = 1, ... , T\)</span> and we assume an array of features <span class="math notranslate nohighlight">\(x_{i}\)</span> to be
predictive of a value <span class="math notranslate nohighlight">\(y_{t}\)</span> .</p>
<div class="math notranslate nohighlight">
\[y_{t} = \beta_{t}x_{t} + \epsilon_{t}\]</div>
<p>Authors of the <strong>Testing for Speculative Bubbles in Stock Markets: A Comparison of Alternative Methods</strong> paper suggest
assuming <span class="math notranslate nohighlight">\(H_{0} : \beta_{t} = 0\)</span> and therefore forecast <span class="math notranslate nohighlight">\(E_{t-1}[\Delta y_{t}] = 0\)</span>. This allows working directly
with <span class="math notranslate nohighlight">\(y_{t}\)</span> instead of computing recursive least squares (RLS) estimates of <span class="math notranslate nohighlight">\(\beta\)</span> .</p>
<p>As <span class="math notranslate nohighlight">\(y_{t}\)</span> we take the log-price and calculate the standardized departure of <span class="math notranslate nohighlight">\(y_{t}\)</span> relative to <span class="math notranslate nohighlight">\(y_{n}\)</span>
(CUSUM statistic) with <span class="math notranslate nohighlight">\(t &gt; n\)</span> as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
    S_{n,t} &amp; = (y_{t}-y_{n})(\hat\sigma_{t}\sqrt{t-n})^{-1}, \ \ t&gt;n \\
    \hat\sigma_{t}^{2} &amp; = (t-1)^{-1} \sum_{i=2}^{t}({\Delta y_{t_{i}}})^2 \\
\end{split}
\end{equation}\end{split}\]</div>
<p>With the <span class="math notranslate nohighlight">\(H_{0} : \beta_{t} = 0\)</span> hypothesis, <span class="math notranslate nohighlight">\(S_{n, t} \sim N[0, 1]\)</span> .</p>
<p>We can test the null hypothesis comparing CUSUM statistic <span class="math notranslate nohighlight">\(S_{n, t}\)</span> with critical value <span class="math notranslate nohighlight">\(c_{\alpha}[n, t]\)</span>,
which is calculated using a one-sided test as:</p>
<div class="math notranslate nohighlight">
\[c_{\alpha}[n, t] = \sqrt{b_{\alpha} + \log{[t-n]}}\]</div>
<p>The authors in the above paper have derived using Monte Carlo method that <span class="math notranslate nohighlight">\(b_{0.05} = 4.6\)</span> .</p>
<p>The disadvantage is that <span class="math notranslate nohighlight">\(y_{n}\)</span> is chosen arbitrarily, and results may be inconsistent due to that. This can be
fixed by estimating <span class="math notranslate nohighlight">\(S_{n, t}\)</span> on backward-shifting windows <span class="math notranslate nohighlight">\(n \in [1, t]\)</span> and picking:</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
S_{t}= \sup_{n \in [1, t]} \{ S_{n, t}\}
\end{equation}\]</div>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.structural_breaks.cusum.get_chu_stinchcombe_white_statistics">
<span class="sig-name descname"><span class="pre">get_chu_stinchcombe_white_statistics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">series</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Series</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_type</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">'one_sided'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_threads</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Series</span></span></span><a class="headerlink" href="#mlfinlab.structural_breaks.cusum.get_chu_stinchcombe_white_statistics" title="Permalink to this definition">¶</a></dt>
<dd><p>Multithread Chu-Stinchcombe-White test implementation, p.251</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>series</strong> – (pd.Series) Series to get statistics for</p></li>
<li><p><strong>test_type</strong> – (str): Two-sided or one-sided test</p></li>
<li><p><strong>num_threads</strong> – (int) Number of cores</p></li>
<li><p><strong>verbose</strong> – (bool) Flag to report progress on asynch jobs</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.Series) Statistics</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<hr class="docutils" />
<section id="explosiveness-tests">
<h4>Explosiveness tests<a class="headerlink" href="#explosiveness-tests" title="Permalink to this heading">¶</a></h4>
<section id="chow-type-dickey-fuller-test">
<h5>Chow-Type Dickey-Fuller Test<a class="headerlink" href="#chow-type-dickey-fuller-test" title="Permalink to this heading">¶</a></h5>
<p>The Chow-Type Dickey-Fuller test is based on an <span class="math notranslate nohighlight">\(AR(1)\)</span> process:</p>
<div class="math notranslate nohighlight">
\[y_{t} = \rho y_{t} + \varepsilon_{t}\]</div>
<p>where <span class="math notranslate nohighlight">\(\varepsilon_{t}\)</span> is white noise.</p>
<p>This test is used for detecting whether the process changes from the random walk (<span class="math notranslate nohighlight">\(\rho = 1\)</span>) into an explosive
process at some time <span class="math notranslate nohighlight">\(\tau^{*}T\)</span>, <span class="math notranslate nohighlight">\(\tau^{*} \in (0,1)\)</span>, where <span class="math notranslate nohighlight">\(T\)</span> is the number of observations.</p>
<p>So, the hypothesis <span class="math notranslate nohighlight">\(H_{0}\)</span> is tested against <span class="math notranslate nohighlight">\(H_{1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
  H_{0} &amp; : y_{t} = y_{t-1} + \varepsilon_{t} \\
  H_{1} &amp; :
  y_{t}=\begin{cases}
  y_{t-1} + \varepsilon_{t} \ \text{for} \ \ t= 1, ..., \tau^*T  \\
  \rho y_{t-1} + \varepsilon_{t} \ \text{for} \ \ t= \tau^*T+1, ..., T, \text{ with } \rho &gt; 1
  \end{cases}
\end{split}
\end{equation}\end{split}\]</div>
<p>To test the hypothesis, the following specification is being fit:</p>
<div class="math notranslate nohighlight">
\[\Delta y_{t} = \delta y_{t-1} D_{t}[\tau^*] + \varepsilon_{t}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
  D_{t}[\tau^*] &amp; = \begin{cases}
  0 \ \text{if} \ \ t &lt; \tau^*T  \\
  1 \ \text{if} \ \ t \geq \tau^*T
  \end{cases}
\end{split}
\end{equation}\end{split}\]</div>
<p>So, the hypothesis tested are now transformed to:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
  H_{0} &amp; : \delta = 0 \\
  H_{1} &amp; : \delta &gt; 1 \\
\end{split}
\end{equation}\end{split}\]</div>
<p>And the Dickey-Fuller-Chow(DFC) test-statistics for <span class="math notranslate nohighlight">\(\tau^*\)</span> is:</p>
<div class="math notranslate nohighlight">
\[DFC_{\tau^*} = \frac{\hat\delta}{\hat\sigma_{\delta}}\]</div>
<p>As described in the <strong>Advances in Financial Machine Learning</strong>:</p>
<p>The first drawback of this method is that <span class="math notranslate nohighlight">\(\tau^{*}\)</span> is unknown, and the second one is that Chow’s approach
assumes that there is only one break and that the bubble runs up to the end of the sample.</p>
<p>To address the first issue, in the work <strong>Tests for Parameter Instability and Structural Change With Unknown ChangePoint</strong>
<a class="reference external" href="https://pdfs.semanticscholar.org/77c9/86937d205592a007df3661778a5ed4fc4e38.pdf">available here</a>, Andrews proposed to
try all possible <span class="math notranslate nohighlight">\(\tau^{*}\)</span> in an interval <span class="math notranslate nohighlight">\(\tau^{*} \in [\tau_{0}, 1 - \tau_{0}]\)</span></p>
<p>For the unknown <span class="math notranslate nohighlight">\(\tau^{*}\)</span> the test statistic is the Supremum Dickey-Fuller Chow which is the maximum of all
<span class="math notranslate nohighlight">\(T(1 - 2\tau_{0})\)</span> values of <span class="math notranslate nohighlight">\(DFC_{\tau^{*}}\)</span> :</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
SDFC = \sup_{\tau^* \in [\tau_0,1-\tau_0]} \{ DFC_{\tau^*}\}
\end{equation}\]</div>
<p>To address the second issue, the Supremum Augmented Dickey-Fuller test was introduced.</p>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.structural_breaks.chow.get_chow_type_stat">
<span class="sig-name descname"><span class="pre">get_chow_type_stat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">series</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Series</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_length</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_threads</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Series</span></span></span><a class="headerlink" href="#mlfinlab.structural_breaks.chow.get_chow_type_stat" title="Permalink to this definition">¶</a></dt>
<dd><p>Multithread implementation of Chow-Type Dickey-Fuller Test, p.251-252</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>series</strong> – (pd.Series) Series to test</p></li>
<li><p><strong>min_length</strong> – (int) Minimum sample length used to estimate statistics</p></li>
<li><p><strong>num_threads</strong> – (int): Number of cores to use</p></li>
<li><p><strong>verbose</strong> – (bool) Flag to report progress on asynch jobs</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.Series) Chow-Type Dickey-Fuller Test statistics</p>
</dd>
</dl>
</dd></dl>

</section>
<hr class="docutils" />
<section id="supremum-augmented-dickey-fuller">
<h5>Supremum Augmented Dickey-Fuller<a class="headerlink" href="#supremum-augmented-dickey-fuller" title="Permalink to this heading">¶</a></h5>
<p>This test was proposed by Phillips, Shi, and Yu in the work <strong>Testing for Multiple Bubbles: Historical Episodes of Exuberance and Collapse in the S&amp;P 500</strong>
<a class="reference external" href="http://korora.econ.yale.edu/phillips/pubs/art/p1498.pdf">available here</a>. The advantage
of this test is that it allows testing for multiple regimes switches (random walk to bubble and back).</p>
<p>The test is based on the following regression:</p>
<div class="math notranslate nohighlight">
\[\Delta y_{t} = \alpha + \beta y_{t-1} + \sum_{l=1}^{L}{\gamma_{l} \Delta y_{t-l}} + \varepsilon_{t}\]</div>
<p>And, the hypothesis <span class="math notranslate nohighlight">\(H_{0}\)</span> is tested against <span class="math notranslate nohighlight">\(H_{1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
  H_{0} &amp; : \beta \le 0 \\
  H_{1} &amp; : \beta &gt; 0 \\
\end{split}
\end{equation}\end{split}\]</div>
<p>The Supremum Augmented Dickey-Fuller fits the above regression for each end point <span class="math notranslate nohighlight">\(t\)</span> with backward expanding
start points and calculates the test-statistic as:</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
 SADF_{t} = \sup_{t_0 \in [1, t-\tau]}\{ADF_{t_0, t}\} = \sup_{t_0 \in [1, t-\tau]} \Bigg\{\frac{\hat\beta_{t_0,t}}{\hat\sigma_{\beta_{t_0, t}}}\Bigg\}
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat\beta_{t_0,t}\)</span> is estimated on the sample from <span class="math notranslate nohighlight">\(t_{0}\)</span> to <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(\tau\)</span> is the minimum
sample length in the analysis, <span class="math notranslate nohighlight">\(t_{0}\)</span> is the left bound of the backwards expanding window, <span class="math notranslate nohighlight">\(t\)</span> iterates
through <span class="math notranslate nohighlight">\([\tau, ..., T]\)</span> .</p>
<p>In comparison to SDFC, which is computed only at time <span class="math notranslate nohighlight">\(T\)</span>, the SADF is computed at each <span class="math notranslate nohighlight">\(t \in [\tau, T]\)</span>,
recursively expanding the sample <span class="math notranslate nohighlight">\(t_{0} \in [1, t - \tau]\)</span> . By doing so, the SADF does not assume a known number of
regime switches.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="_images/sadf.png"><img alt="structural breaks" src="_images/sadf.png" style="width: 629.3px; height: 413.0px;" /></a>
<figcaption>
<p><span class="caption-text">Image showing SADF test statistic with 5 lags and linear model. The
SADF line spikes when prices exhibit a bubble-like behavior, and returns to low levels
when the bubble bursts.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The <cite>model</cite> and the <cite>add_const</cite> parameters of the <strong>get_sadf</strong> function allow for different specifications of the
regression’s time trend component.</p>
<p>Linear model (<cite>model=’linear’</cite>) uses a linear time trend:</p>
<div class="math notranslate nohighlight">
\[\Delta y_{t} = \beta y_{t-1} + \sum_{l=1}^{L}{\gamma_{l} \Delta y_{t-l}} + \varepsilon_{t}\]</div>
<p>Quadratic model (<cite>model=’quadratic’</cite>) uses a second-degree polynomial time trend:</p>
<div class="math notranslate nohighlight">
\[\Delta y_{t} = \beta y_{t-1} + \sum_{l=1}^{L}{\gamma_{l} \Delta y_{t-l}} + \sum_{l=1}^{L}{\delta_{l}^2 \Delta y_{t-l}} + \varepsilon_{t}\]</div>
<p>Adding a constant (<cite>add_const=True</cite>) to those specifications results in:</p>
<div class="math notranslate nohighlight">
\[\Delta y_{t} = \alpha + \beta y_{t-1} + \sum_{l=1}^{L}{\gamma_{l} \Delta y_{t-l}} + \varepsilon_{t}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\Delta y_{t} = \alpha + \beta y_{t-1} + \sum_{l=1}^{L}{\gamma_{l} \Delta y_{t-l}} + \sum_{l=1}^{L}{\delta_{l}^2 \Delta y_{t-l}} + \varepsilon_{t}\]</div>
<p>respectively.</p>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.structural_breaks.sadf.get_sadf">
<span class="sig-name descname"><span class="pre">get_sadf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">series</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Series</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lags</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_length</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_const</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">phi</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">float</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_threads</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">bool</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Series</span></span></span><a class="headerlink" href="#mlfinlab.structural_breaks.sadf.get_sadf" title="Permalink to this definition">¶</a></dt>
<dd><p>Advances in Financial Machine Learning, p. 258-259.</p>
<p>Multithread implementation of SADF</p>
<p>SADF fits the ADF regression at each end point t with backwards expanding start points. For the estimation
of SADF(t), the right side of the window is fixed at t. SADF recursively expands the beginning of the sample
up to t - min_length, and returns the sup of this set.</p>
<p>When doing with sub- or super-martingale test, the variance of beta of a weak long-run bubble may be smaller than
one of a strong short-run bubble, hence biasing the method towards long-run bubbles. To correct for this bias,
ADF statistic in samples with large lengths can be penalized with the coefficient phi in [0, 1] such that:</p>
<p>ADF_penalized = ADF / (sample_length ^ phi)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>series</strong> – (pd.Series) Series for which SADF statistics are generated</p></li>
<li><p><strong>model</strong> – (str) Either ‘linear’, ‘quadratic’, ‘sm_poly_1’, ‘sm_poly_2’, ‘sm_exp’, ‘sm_power’</p></li>
<li><p><strong>lags</strong> – (int or list) Either number of lags to use or array of specified lags</p></li>
<li><p><strong>min_length</strong> – (int) Minimum number of observations needed for estimation</p></li>
<li><p><strong>add_const</strong> – (bool) Flag to add constant</p></li>
<li><p><strong>phi</strong> – (float) Coefficient to penalize large sample lengths when computing SMT, in [0, 1]</p></li>
<li><p><strong>num_threads</strong> – (int) Number of cores to use</p></li>
<li><p><strong>verbose</strong> – (bool) Flag to report progress on asynch jobs</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.Series) SADF statistics</p>
</dd>
</dl>
</dd></dl>

<p>The function used in the SADF Test to estimate the <span class="math notranslate nohighlight">\(\hat\beta_{t_0,t}\)</span> is:</p>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.structural_breaks.sadf.get_betas">
<span class="sig-name descname"><span class="pre">get_betas</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">DataFrame</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">array</span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">array</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#mlfinlab.structural_breaks.sadf.get_betas" title="Permalink to this definition">¶</a></dt>
<dd><p>Advances in Financial Machine Learning, Snippet 17.4, page 259.</p>
<p>Fitting The ADF Specification (get beta estimate and estimate variance)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> – (pd.DataFrame) Features(factors)</p></li>
<li><p><strong>y</strong> – (pd.DataFrame) Outcomes</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(np.array, np.array) Betas and variances of estimates</p>
</dd>
</dl>
</dd></dl>

<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><strong>Advances in Financial Machine Learning</strong> book additionally describes why log prices data is more appropriate to use
in the above tests, their computational complexity, and other details.</p>
</div>
<p>The SADF also allows for explosiveness testing that doesn’t rely on the standard ADF specification. If the process is either
sub- or super martingale, the hypotheses <span class="math notranslate nohighlight">\(H_{0}: \beta = 0, H_{1}: \beta \ne 0\)</span> can be tested under these specifications:</p>
<p>Polynomial trend (<cite>model=’sm_poly_1’</cite>):</p>
<div class="math notranslate nohighlight">
\[y_{t} = \alpha + \gamma t + \beta t^{2} + \varepsilon_{t}\]</div>
<p>Polynomial trend (<cite>model=’sm_poly_2’</cite>):</p>
<div class="math notranslate nohighlight">
\[log[y_{t}] = \alpha + \gamma t + \beta t^{2} + \varepsilon_{t}\]</div>
<p>Exponential trend (<cite>model=’sm_exp’</cite>):</p>
<div class="math notranslate nohighlight">
\[y_{t} = \alpha e^{\beta t} + \varepsilon_{t} \Rightarrow log[y_{t}] = log[\alpha] + \beta t^{2} + \xi_{t}\]</div>
<p>Power trend (<cite>model=’sm_power’</cite>):</p>
<div class="math notranslate nohighlight">
\[y_{t} = \alpha t^{\beta} + \varepsilon_{t} \Rightarrow log[y_{t}] = log[\alpha] + \beta log[t] + \xi_{t}\]</div>
<p>Again, the SADF fits the above regressions for each end point <span class="math notranslate nohighlight">\(t\)</span> with backward expanding start points,
but the test statistic is taken as an absolute value, as we’re testing both the explosive growth and collapse.
This is described in more detail in the <strong>Advances in Financial Machine Learning</strong> book p. 260.</p>
<p>The test statistic calculated (SMT for Sub/Super Martingale Tests) is:</p>
<div class="math notranslate nohighlight">
\[SMT_{t} = \sup_{t_0 \in [1, t-\tau]} \Bigg\{\frac{ | \hat\beta_{t_0,t} | }{\hat\sigma_{\beta_{t_0, t}}}\Bigg\}\]</div>
<p>From the book:</p>
<p>Parameter <cite>phi</cite> in range (0, 1) can be used (<cite>phi=0.5</cite>) to penalize large sample lengths ( “this corrects for the bias that the <span class="math notranslate nohighlight">\(\hat\sigma_{\beta_{t_0, t}}\)</span>
of a weak long-run bubble  may be smaller than the <span class="math notranslate nohighlight">\(\hat\sigma_{\beta_{t_0, t}}\)</span> of a strong short-run bubble,
hence biasing method towards long-run bubbles” ):</p>
<div class="math notranslate nohighlight">
\[SMT_{t} = \sup_{t_0 \in [1, t-\tau]} \Bigg\{\frac{ | \hat\beta_{t_0,t} | }{\hat\sigma_{\beta_{t_0, t}}(t-t_{0})^{\phi}}\Bigg\}\]</div>
</section>
</section>
<hr class="docutils" />
<section id="examples">
<h4>Examples<a class="headerlink" href="#examples" title="Permalink to this heading">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mlfinlab.structural_breaks</span> <span class="kn">import</span> <span class="p">(</span><span class="n">get_chu_stinchcombe_white_statistics</span><span class="p">,</span>
                                        <span class="n">get_chow_type_stat</span><span class="p">,</span> <span class="n">get_sadf</span><span class="p">)</span>

<span class="c1"># Importing price data</span>
<span class="n">bars</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;BARS_PATH&#39;</span><span class="p">,</span> <span class="n">index_col</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># Changing to log prices data</span>
<span class="n">log_prices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">bars</span><span class="o">.</span><span class="n">close</span><span class="p">)</span> <span class="c1"># see p.253, 17.4.2.1 Raw vs Log Prices</span>

<span class="c1"># Chu-Stinchcombe test (one-sided and two-sided)</span>
<span class="n">one_sided_test</span> <span class="o">=</span> <span class="n">get_chu_stinchcombe_white_statistics</span><span class="p">(</span><span class="n">log_prices</span><span class="p">,</span> <span class="n">test_type</span><span class="o">=</span><span class="s1">&#39;one_sided&#39;</span><span class="p">)</span>
<span class="n">two_sided_test</span> <span class="o">=</span> <span class="n">get_chu_stinchcombe_white_statistics</span><span class="p">(</span><span class="n">log_prices</span><span class="p">,</span> <span class="n">test_type</span><span class="o">=</span><span class="s1">&#39;two_sided&#39;</span><span class="p">)</span>

<span class="c1"># Chow-type test</span>
<span class="n">chow_stats</span> <span class="o">=</span> <span class="n">get_chow_type_stat</span><span class="p">(</span><span class="n">log_prices</span><span class="p">,</span> <span class="n">min_length</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># SADF test with linear model and a constant, lag of 5 and minimum sample length of 20</span>
<span class="n">linear_sadf</span> <span class="o">=</span> <span class="n">get_sadf</span><span class="p">(</span><span class="n">log_prices</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">add_const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">min_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">lags</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Polynomial trend SMT</span>
<span class="n">sm_poly_1_sadf</span> <span class="o">=</span> <span class="n">get_sadf</span><span class="p">(</span><span class="n">log_prices</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;sm_poly_1&#39;</span><span class="p">,</span> <span class="n">add_const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">min_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">lags</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<span id="document-implementations/microstructural_features"></span><section id="microstructural-features">
<span id="implementations-microstructural-features"></span><h3>Microstructural Features<a class="headerlink" href="#microstructural-features" title="Permalink to this heading">¶</a></h3>
<p>This module implements features from Advances in Financial Machine Learning, Chapter 18: Entropy features and
Chapter 19: Microstructural features</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="_images/kyle_lambda.png"><img alt="Kyle's Lambda" src="_images/kyle_lambda.png" style="width: 750.0px; height: 500.0px;" /></a>
<figcaption>
<p><span class="caption-text">Closing prices in blue, and Kyle’s Lambda in red</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<section id="entropy-features">
<h4>Entropy Features<a class="headerlink" href="#entropy-features" title="Permalink to this heading">¶</a></h4>
<p>Entropy is used to measure the average amount of information produced by a source of data. In financial machine learning,
sources of data to get entropy from can be tick sizes, tick rule series, and percent changes between ticks.
Estimating entropy requires the encoding of a message. The researcher can apply either a binary (usually applied to tick rule),
quantile or sigma encoding.</p>
<section id="message-encoding">
<h5>Message Encoding<a class="headerlink" href="#message-encoding" title="Permalink to this heading">¶</a></h5>
</section>
<section id="estimate-entropy">
<h5>Estimate Entropy<a class="headerlink" href="#estimate-entropy" title="Permalink to this heading">¶</a></h5>
<p>The various ways to estimate entropy are:</p>
<ol class="arabic simple">
<li><p>Shannon</p></li>
<li><p>Lempel-Ziv</p></li>
<li><p>Plug-In</p></li>
<li><p>Kontoyiannis</p></li>
</ol>
<section id="example">
<h6>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h6>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.entropy</span> <span class="kn">import</span> <span class="n">get_shannon_entropy</span><span class="p">,</span> <span class="n">get_lempel_ziv_entropy</span><span class="p">,</span> <span class="n">get_plug_in_entropy</span>

<span class="n">message</span> <span class="o">=</span> <span class="s1">&#39;abbnaacdeaas&#39;</span>
<span class="n">shannon</span> <span class="o">=</span> <span class="n">get_shannon_entropy</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
<span class="n">lempel_ziv</span> <span class="o">=</span> <span class="n">get_lempel_ziv_entropy</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
<span class="n">plug_in</span> <span class="o">=</span> <span class="n">get_plug_in_entropy</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">word_length</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="bar-based-inter-bar-features">
<h4>Bar Based (Inter-Bar) Features<a class="headerlink" href="#bar-based-inter-bar-features" title="Permalink to this heading">¶</a></h4>
<p>When bars are generated (time, volume, imbalance, run) researcher can get inter-bar microstructural features:
Roll Measure, Roll Impact, Corwin-Schultz spread estimator, Bekker-Parkinson volatility, Kyle/Amihud/Hasbrouck lambdas,
and VPIN.</p>
</section>
<section id="trade-based-intra-bar-features">
<h4>Trade Based (Intra-Bar) Features<a class="headerlink" href="#trade-based-intra-bar-features" title="Permalink to this heading">¶</a></h4>
<p>Some microstructural features need to be calculated from trades (tick rule/volume/percent change entropies, average
tick size, vwap, tick rule sum, trade based lambdas). Mlfinlab has a special function which calculates features for
generated bars using trade data and bar date_time index.</p>
<section id="id1">
<h5>Example<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.microstructural_features</span> <span class="kn">import</span> <span class="n">quantile_mapping</span><span class="p">,</span> <span class="n">MicrostructuralFeaturesGenerator</span>

<span class="n">df_trades</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;TRADES_PATH&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">df_trades</span> <span class="o">=</span> <span class="n">df_trades</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">10000</span><span class="p">]</span> <span class="c1"># Take subsample to avoid look-ahead bias</span>
<span class="n">df_trades</span><span class="p">[</span><span class="s1">&#39;log_ret&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_trades</span><span class="o">.</span><span class="n">Price</span> <span class="o">/</span> <span class="n">df_trades</span><span class="o">.</span><span class="n">Price</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="n">non_null_log_ret</span> <span class="o">=</span> <span class="n">df_trades</span><span class="p">[</span><span class="n">df_trades</span><span class="o">.</span><span class="n">log_ret</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">log_ret</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Take unique volumes only</span>
<span class="n">volume_mapping</span> <span class="o">=</span> <span class="n">quantile_mapping</span><span class="p">(</span><span class="n">df_trades</span><span class="o">.</span><span class="n">Volume</span><span class="o">.</span><span class="n">drop_duplicates</span><span class="p">(),</span> <span class="n">num_letters</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">returns_mapping</span> <span class="o">=</span> <span class="n">quantile_mapping</span><span class="p">(</span><span class="n">non_null_log_ret</span><span class="p">,</span> <span class="n">num_letters</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Compress bars from ticks</span>
<span class="n">compressed_bars</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;BARS_PATH&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">tick_number</span> <span class="o">=</span> <span class="n">compressed_bars</span><span class="o">.</span><span class="n">tick_num</span> <span class="c1"># tick number where bar was formed</span>

<span class="n">gen</span> <span class="o">=</span> <span class="n">MicrostructuralFeaturesGenerator</span><span class="p">(</span><span class="s1">&#39;TRADES_PATH&#39;</span><span class="p">,</span> <span class="n">tick_number</span><span class="p">,</span> <span class="n">volume_encoding</span><span class="o">=</span><span class="n">volume_mapping</span><span class="p">,</span>
                                       <span class="n">pct_encoding</span><span class="o">=</span><span class="n">returns_mapping</span><span class="p">)</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">gen</span><span class="o">.</span><span class="n">get_features</span><span class="p">(</span><span class="n">to_csv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-codependence/introduction"></span><section id="introduction">
<span id="codependence-introduction"></span><h3>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h3>
<p>This module includes implementations of codependence metrics. According to Lopez de Prado:</p>
<p>“Two random variables are codependent when knowing the value of one helps us determine the value of the other.
This should not be confounded with the notion of causality.”</p>
<p>Pearson correlation coefficient is the most famous and widely used measure of codependence, however, it has some drawbacks.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Pearson correlation suffers from 3 major drawbacks:</p>
<ol class="arabic simple">
<li><p>It captures linear effects, but if two variables have strong non-linear dependency (squared or abs for example) Pearson correlation won’t find any pattern between them.</p></li>
<li><p>Correlation is not a distance metric: it does not satisfy non-negativity and subadditivity conditions.</p></li>
<li><p>Financial markets have non-linear patterns, which Pearson correlation fails to capture.</p></li>
</ol>
</div>
<p>Pearson correlation is not the only way of measuring codependence. There are alternative and more modern measures of codependence,
which are described in the parts of this module.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For some methods in this module, it’s discussed whether they are true metrics.
According to Arkhangel’skii, A. V. and Pontryagin, L. S. (1990), <strong>General Topology I</strong>:
A metric on a set <span class="math notranslate nohighlight">\(X\)</span> is a function (called a distance):</p>
<div class="math notranslate nohighlight">
\[d: X \times X \rightarrow [0,+ \infty) ;   x, y, z \in X\]</div>
<p>for which the following three axioms are satisfied:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(d(x, y) = 0 \iff x = y\)</span> — identity of indiscernibles;</p></li>
<li><p><span class="math notranslate nohighlight">\(d(x,y) = d(y,x)\)</span> — symmetry;</p></li>
<li><p><span class="math notranslate nohighlight">\(d(x,y) \le d(x,z) + d(z,y)\)</span> — triangle inequality;</p></li>
</ol>
<p>and these imply <span class="math notranslate nohighlight">\(d(x,y) \ge 0\)</span> — non-negativity.</p>
</div>
</section>
<span id="document-codependence/correlation_based_metrics"></span><div class="admonition note" id="codependence-correlation-based-metrics">
<p class="admonition-title">Note</p>
<p>The following implementations and documentation, closely follows the lecture notes from Cornell University, by Marcos Lopez de Prado:
<a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3512994">Codependence (Presentation Slides)</a>.</p>
</div>
<section id="correlation-based-metrics">
<h3>Correlation-Based Metrics<a class="headerlink" href="#correlation-based-metrics" title="Permalink to this heading">¶</a></h3>
<section id="distance-correlation">
<h4>Distance Correlation<a class="headerlink" href="#distance-correlation" title="Permalink to this heading">¶</a></h4>
<p><strong>Distance correlation</strong> can capture not only linear association but also non-linear variable dependencies which Pearson correlation can not.
It was introduced in 2005 by Gábor J. Szekely and is described in the work
<a class="reference external" href="https://projecteuclid.org/download/pdfview_1/euclid.aos/1201012979">“Measuring and testing independence by correlation of distances”.</a>
It is calculated as:</p>
<div class="math notranslate nohighlight">
\[\rho_{dist}[X, Y] = \frac{dCov[X, Y]}{\sqrt{dCov[X, X]dCov[Y,Y}}\]</div>
<p>Where <span class="math notranslate nohighlight">\(dCov[X, Y]\)</span> can be interpreted as the average Hadamard product of the doubly-centered Euclidean distance matrices of
<span class="math notranslate nohighlight">\(X, Y\)</span>. (<a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3512994">Cornell lecture slides, p.7</a>)</p>
<p>Values of distance correlation fall in the range:</p>
<div class="math notranslate nohighlight">
\[0 \leq \rho_{dist}[X, Y] \leq 1\]</div>
<p>Distance correlation is equal to zero if and only if the two variables are independent (in contrast to Pearson correlation
that can be zero even if the variables are dependant).</p>
<div class="math notranslate nohighlight">
\[\rho_{dist}[X, Y] = 0 \Leftrightarrow X \perp Y\]</div>
<p>As shown in the figure below, distance correlation captures the nonlinear relationship.</p>
<a class="reference internal image-reference" href="_images/distance_correlation.png"><img alt="_images/distance_correlation.png" class="align-center" src="_images/distance_correlation.png" style="width: 578.1999999999999px; height: 164.5px;" /></a>
<p>The numbers in the first line are Pearson correlation values and the values in the second line are Distance correlation values.
This figure is from <a class="reference external" href="https://www.researchgate.net/publication/238879872_Introducing_the_discussion_paper_by_Szekely_and_Rizzo">“Introducing the discussion paper by Székely and Rizzo”</a>
by Michale A. Newton. It provides a great overview for readers.</p>
<section id="implementation">
<h5>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h5>
</section>
</section>
<section id="standard-angular-distance">
<h4>Standard Angular Distance<a class="headerlink" href="#standard-angular-distance" title="Permalink to this heading">¶</a></h4>
<p><strong>Angular distance</strong> is a slight modification of the Pearson correlation coefficient which satisfies all distance metric conditions.
This measure is known as the angular distance because when we use <em>covariance</em> as an <em>inner product</em>, we can interpret correlation as <span class="math notranslate nohighlight">\(cos\theta\)</span>.</p>
<p>A proof that angular distance is a true metric can be found in the work by Lopez de Prado
<a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2708678">Building Diversified Portfolios that Outperform Out-of-Sample:</a></p>
<p>“Angular distance is a linear multiple of the Euclidean distance between the vectors <span class="math notranslate nohighlight">\(\{X, Y\}\)</span> after z-standardization,
hence it inherits the true-metric properties of the Euclidean distance.”</p>
<p>According to Lopez de Prado:</p>
<p>“The [standard angular distance] metric deems more distant two random variables with negative correlation than two random
variables with positive correlation”.</p>
<p>“This property makes sense in many applications. For example, we may wish to build a <strong>long-only portfolio</strong>, where holdings
in negative-correlated securities can only offset risk, and therefore should be treated as different for diversification purposes”.</p>
<p>Formula used to calculate standard angular distance:</p>
<div class="math notranslate nohighlight">
\[d_\rho[X, Y] = \sqrt{\frac{1}{2}(1-\rho[X,Y])}\]</div>
<p>where <span class="math notranslate nohighlight">\(\rho[X,Y]\)</span> is Pearson correlation between the vectors <span class="math notranslate nohighlight">\(\{X, Y\}\)</span> .</p>
<p>Values of standard angular distance fall in the range:</p>
<div class="math notranslate nohighlight">
\[d_\rho[X, Y] \in [0, 1]\]</div>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="_images/angular_distance.png"><img alt="Angular Distance" src="_images/angular_distance.png" style="width: 403.2px; height: 352.79999999999995px;" /></a>
<figcaption>
<p><span class="caption-text">The angular distance satisfies all the conditions of a true metric, (Lopez de Prado, 2020.)</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<section id="id1">
<h5>Implementation<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h5>
</section>
</section>
<section id="absolute-angular-distance">
<h4>Absolute Angular Distance<a class="headerlink" href="#absolute-angular-distance" title="Permalink to this heading">¶</a></h4>
<p>This modification of angular distance uses an absolute value of Pearson correlation in the formula.</p>
<p>This property assigns small distance to elements that have a high negative correlation. According to Lopez de Prado, this
is useful because “in <strong>long-short portfolios</strong>, we often prefer to consider highly negatively-correlated securities as similar,
because the position sign can override the sign of the correlation”.</p>
<p>Formula used to calculate absolute angular distance:</p>
<div class="math notranslate nohighlight">
\[d_{|\rho|}[X, Y] = \sqrt{1-|\rho[X,Y]|}\]</div>
<p>where <span class="math notranslate nohighlight">\(\rho[X,Y]\)</span> is Pearson correlation between the vectors <span class="math notranslate nohighlight">\(\{X, Y\}\)</span> .</p>
<p>Values of absolute angular distance fall in the range:</p>
<div class="math notranslate nohighlight">
\[d_{|\rho|}[X, Y] \in [0, 1]\]</div>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="_images/modified_angular_distance.png"><img alt="Modified Angular Distance" src="_images/modified_angular_distance.png" style="width: 403.2px; height: 352.79999999999995px;" /></a>
<figcaption>
<p><span class="caption-text">In some financial applications, it makes more sense to apply a modified definition of angular distance, such that the
sign of the correlation is ignored, (Lopez de Prado, 2020)</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<section id="id2">
<h5>Implementation<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h5>
</section>
</section>
<section id="squared-angular-distance">
<h4>Squared Angular Distance<a class="headerlink" href="#squared-angular-distance" title="Permalink to this heading">¶</a></h4>
<p>Squared angular distance uses the squared value of Pearson correlation in the formula and has similar properties to absolute
angular distance. The only difference is that a higher distance is assigned to the elements that have a small absolute correlation.</p>
<p>Formula used to calculate squared angular distance:</p>
<div class="math notranslate nohighlight">
\[d_{\rho^2}[X, Y] = \sqrt{1-{\rho[X,Y]}^2}\]</div>
<p>where <span class="math notranslate nohighlight">\(\rho[X,Y]\)</span> is Pearson correlation between the vectors <span class="math notranslate nohighlight">\(\{X, Y\}\)</span> .</p>
<p>Values of squared angular distance fall in the range:</p>
<div class="math notranslate nohighlight">
\[d_{\rho^2}[X, Y] \in [0, 1]\]</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/modified_angular_distance.png"><img alt="Modified Angular Distance" src="_images/modified_angular_distance.png" style="width: 403.2px; height: 352.79999999999995px;" /></a>
</figure>
<section id="id3">
<h5>Implementation<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h5>
</section>
</section>
<section id="examples">
<h4>Examples<a class="headerlink" href="#examples" title="Permalink to this heading">¶</a></h4>
<p>The following examples show how the described above correlation-based metrics can be used on real data:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.codependence</span> <span class="kn">import</span> <span class="n">distance_correlation</span><span class="p">,</span> <span class="n">angular_distance</span><span class="p">,</span>
                                  <span class="n">absolute_angular_distance</span><span class="p">,</span> <span class="n">squared_angular_distance</span>

<span class="c1"># Import dataframe of returns for assets in a portfolio</span>
<span class="n">asset_returns</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">DATA_PATH</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">asset1</span> <span class="o">=</span> <span class="s1">&#39;SPY&#39;</span>
<span class="n">asset2</span> <span class="o">=</span> <span class="s1">&#39;TLT&#39;</span>

<span class="c1"># Calculate distance correlation between chosen assets</span>
<span class="n">distance_corr</span> <span class="o">=</span> <span class="n">distance_correlation</span><span class="p">(</span><span class="n">asset_returns</span><span class="p">[</span><span class="n">asset1</span><span class="p">],</span> <span class="n">asset_returns</span><span class="p">[</span><span class="n">assets2</span><span class="p">])</span>

<span class="c1"># Calculate angular distance between chosen assets</span>
<span class="n">angular_dist</span> <span class="o">=</span> <span class="n">angular_distance</span><span class="p">(</span><span class="n">asset_returns</span><span class="p">[</span><span class="n">asset1</span><span class="p">],</span> <span class="n">asset_returns</span><span class="p">[</span><span class="n">assets2</span><span class="p">])</span>

<span class="c1"># Calculate absolute angular distance between chosen assets</span>
<span class="n">angular_dist</span> <span class="o">=</span> <span class="n">absolute_angular_distance</span><span class="p">(</span><span class="n">asset_returns</span><span class="p">[</span><span class="n">asset1</span><span class="p">],</span> <span class="n">asset_returns</span><span class="p">[</span><span class="n">assets2</span><span class="p">])</span>

<span class="c1"># Calculate squared angular distance between chosen assets</span>
<span class="n">angular_dist</span> <span class="o">=</span> <span class="n">squared_angular_distance</span><span class="p">(</span><span class="n">asset_returns</span><span class="p">[</span><span class="n">asset1</span><span class="p">],</span> <span class="n">asset_returns</span><span class="p">[</span><span class="n">assets2</span><span class="p">])</span>
</pre></div>
</div>
</section>
</section>
<span id="document-codependence/information_theory_metrics"></span><div class="admonition note" id="codependence-information-theory-metrics">
<p class="admonition-title">Note</p>
<p>The following implementations and documentation, closely follows the lecture notes from Cornell University, by Marcos Lopez de Prado:
<a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3512994">Codependence (Presentation Slides)</a>.</p>
</div>
<section id="information-theory-metrics">
<h3>Information Theory Metrics<a class="headerlink" href="#information-theory-metrics" title="Permalink to this heading">¶</a></h3>
<p>We can gauge the codependence from the information theory perspective. In information theory, (Shannon’s) entropy is a
measure of information (uncertainty). As described in the <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3512994">Cornell lecture slides, p.13</a>
, entropy is calculated as:</p>
<div class="math notranslate nohighlight">
\[H[X] = -\sum\limits_{x \in S_{X}}p[x]log[p[x]]\]</div>
<p>Where <span class="math notranslate nohighlight">\(X\)</span> is a discrete random variable that takes a value <span class="math notranslate nohighlight">\(x\)</span> from the set <span class="math notranslate nohighlight">\(S_{X}\)</span> with probability
<span class="math notranslate nohighlight">\(p[x]\)</span> .</p>
<p>In short, we can say that entropy is the expectation of the amount of information when we sample from a particular probability
distribution or the number of bits to transmit to the target. So, if there is a correspondence between random variables,
the correspondence will be reflected in entropy. For example, if two random variables are associated, the amount of
information in the joint probability distribution of the two random variables will be less than the sum of the information
in each random variable. This is because knowing a correspondence means knowing one random variable can reduce uncertainty
about the other random variable.</p>
<div class="math notranslate nohighlight">
\[H[X+Y] = H[X] + H[Y],  X \bot Y\]</div>
<p>This module presents two ways of measuring correspondence:</p>
<ol class="arabic simple">
<li><p>Mutual Information</p></li>
<li><p>Variation of Information</p></li>
</ol>
<p>The following figure highlights how we can view the relationships of various information measures associated with correlated variables
<span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> through the below figure. (<a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3512994">Cornell lecture slides, p.24</a>)</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="_images/entropy_relation_diagram.png"><img alt="Entropy Relational Diagram" src="_images/entropy_relation_diagram.png" style="width: 446.59999999999997px; height: 301.0px;" /></a>
<figcaption>
<p><span class="caption-text">The correspondence between joint entropy, marginal entropies, conditional entropies, mutual information and variation of information (Lopez de Prado, 2020)</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<section id="mutual-information">
<h4>Mutual Information<a class="headerlink" href="#mutual-information" title="Permalink to this heading">¶</a></h4>
<p>According to Lopez de Prado: “<strong>Mutual Information</strong> is defined as the decrease in uncertainty (or informational gain)
in <span class="math notranslate nohighlight">\(X\)</span> that results from knowing the value of <span class="math notranslate nohighlight">\(Y\)</span>. Mutual information is not a metric as it doesn’t satisfy
the triangle inequality”. The properties of non-negativity and symmetry are satisfied. Mutual information is calculated as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
I[X, Y]=&amp; H[X] - H[X|Y]\\
       =&amp; H[X] + H[Y] - H[X,Y]\\
       =&amp; \sum\limits_{x \in S_{X}} \sum\limits_{y \in S_{Y}}p[x,y]log[\frac{p[x,y]}{p[x]p[y]}]\\
\end{align*}\end{split}\]</div>
<p>Mutual information has a grouping property:</p>
<div class="math notranslate nohighlight">
\[I[X, Y, Z] = I[X, Y] + I[(X, Y), Z]\]</div>
<p>where <span class="math notranslate nohighlight">\((X, Y)\)</span> is a joint distribution of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> .</p>
<p>It can also be normalized using a known upper boundary:</p>
<div class="math notranslate nohighlight">
\[I[X, Y] \le min\{H[X] + H[Y]\}\]</div>
<section id="implementation">
<h5>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h5>
</section>
</section>
<section id="variation-of-information">
<h4>Variation of Information<a class="headerlink" href="#variation-of-information" title="Permalink to this heading">¶</a></h4>
<p>According to Lopez de Prado: “<strong>Variation of Information</strong> can be interpreted as the uncertainty we expect in one variable
if we are told the value of another”. The variation of information is a true metric and satisfies the axioms from the introduction.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
  VI[X,Y]=&amp; H[X|Y] + H[Y|X]\\
        =&amp; H[X] + H[Y]-2I[X,Y]\\
        =&amp; 2H[X,Y]-H[X]-H[Y]\\
\end{align*}\end{split}\]</div>
<p>The upper bound of Variation of information is not firm as it depends on the sizes of the population which is problematic
when comparing variations of information across different population sizes, as described in
<a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3512994">Cornell lecture slides, p.21</a></p>
<section id="id1">
<h5>Implementation<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h5>
</section>
</section>
<section id="discretization">
<h4>Discretization<a class="headerlink" href="#discretization" title="Permalink to this heading">¶</a></h4>
<p>Both mutual information and variation of information are using random variables that are discrete. To use these tools for
continuous random variables the discretization approach can be used.</p>
<p>For the continuous case, we can quantize the values to estimate <span class="math notranslate nohighlight">\(H[X]\)</span>. Following the <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3512994">Cornell lecture slides, p.26</a> :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
H[X] =&amp; \int_{\infty}^{\infty}f_{X}[x]log[f_{X}[x]]dx\\
\:    \approx&amp; -\sum\limits_{i=1}^{B_{X}}f_{X}[x_{i}]log[f_{X}[x_{i}]]\Delta_{x}\\
\end{align*}\end{split}\]</div>
<p>where the observed values <span class="math notranslate nohighlight">\(\{x\}\)</span> are divided into <span class="math notranslate nohighlight">\(B_{X}\)</span> bins of equal size <span class="math notranslate nohighlight">\(\Delta_{X}\)</span>,
<span class="math notranslate nohighlight">\(\Delta_{X} = \frac{max\{x\} - min\{x\}}{B_{X}}\)</span> , and <span class="math notranslate nohighlight">\(f_{X}[x_{i}]\)</span> is the frequency of observations
within the i-th bin.</p>
<p>So, the discretized estimator of entropy is:</p>
<div class="math notranslate nohighlight">
\[\hat{H}[X]=-\sum\limits_{i=1}^{B_{X}}\frac{N_{i}}{N}log[\frac{N_{i}}{N}]log[\Delta_{X}]\]</div>
<p>where <span class="math notranslate nohighlight">\(N_{i}\)</span> is the number of observations within the i-th bin, <span class="math notranslate nohighlight">\(N = \sum_{i=1}^{B_{X}}N_{i}\)</span> .</p>
<p>From the above equations, the size of the bins should be chosen. The results of the entropy estimation will depend on the
binning. The works by <a class="reference external" href="https://www.researchgate.net/publication/257014935">Hacine-Gharbi et al. (2012)</a>  and
<a class="reference external" href="https://www.researchgate.net/publication/320887281">Hacine-Gharbi and Ravier (2018)</a>  present optimal binning
for marginal and joint entropy.</p>
<p>This optimal binning method is used in the mutual information and variation of information functions.</p>
<section id="id2">
<h5>Implementation<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h5>
</section>
</section>
<section id="examples">
<h4>Examples<a class="headerlink" href="#examples" title="Permalink to this heading">¶</a></h4>
<p>The following example highlights how the various metrics behave under various variable dependencies:</p>
<ol class="arabic simple">
<li><p>Linear</p></li>
<li><p>Squared</p></li>
<li><p><span class="math notranslate nohighlight">\(Y = abs(X)\)</span></p></li>
<li><p>Independent variables</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">mlfinlab.codependence</span> <span class="kn">import</span> <span class="n">distance_correlation</span><span class="p">,</span> <span class="n">get_mutual_info</span><span class="p">,</span> <span class="n">variation_of_information_score</span>
<span class="kn">from</span> <span class="nn">ace</span> <span class="kn">import</span> <span class="n">model</span> <span class="c1"># ace package is used for max correlation estimation</span>

<span class="k">def</span> <span class="nf">max_correlation</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get max correlation using ace package.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">x_input</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="n">y_input</span> <span class="o">=</span> <span class="n">y</span>
    <span class="n">ace_model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span>
    <span class="n">ace_model</span><span class="o">.</span><span class="n">build_model_from_xy</span><span class="p">(</span><span class="n">x_input</span><span class="p">,</span> <span class="n">y_input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">ace_model</span><span class="o">.</span><span class="n">ace</span><span class="o">.</span><span class="n">x_transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ace_model</span><span class="o">.</span><span class="n">ace</span><span class="o">.</span><span class="n">y_transform</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

<span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">y_1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">state</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span> <span class="o">/</span> <span class="mi">5</span> <span class="c1"># linear</span>
<span class="n">y_2</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">state</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span> <span class="o">/</span> <span class="mi">5</span> <span class="c1"># squared</span>
<span class="n">y_3</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">state</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span> <span class="o">/</span> <span class="mi">5</span> <span class="c1"># Abs</span>
<span class="c1"># independent</span>
<span class="n">y_4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="k">for</span> <span class="n">y</span><span class="p">,</span> <span class="n">dependency</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">y_1</span><span class="p">,</span> <span class="n">y_2</span><span class="p">,</span> <span class="n">y_3</span><span class="p">,</span> <span class="n">y_4</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;squared&#39;</span><span class="p">,</span> <span class="s1">&#39;y=|x|&#39;</span><span class="p">,</span> <span class="s1">&#39;independent&#39;</span><span class="p">]):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Pearson corr: </span><span class="si">{:0.2f}</span><span class="s2"> &quot;</span> <span class="o">+</span> \
           <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Norm.mutual info: </span><span class="si">{:0.2f}</span><span class="s2"> &quot;</span> <span class="o">+</span> \
           <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Distance correlation: </span><span class="si">{:0.2f}</span><span class="s2"> &quot;</span> <span class="o">+</span> \
           <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Information variation: </span><span class="si">{:0.2f}</span><span class="s2"> &quot;</span> <span class="o">+</span> \
           <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Max correlation: </span><span class="si">{:0.2f}</span><span class="s2">&quot;</span>

    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                       <span class="n">get_mutual_info</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                       <span class="n">distance_correlation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span>
                       <span class="n">variation_of_information_score</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                       <span class="n">max_correlation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>

    <span class="c1"># Plot relationships</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
    <span class="n">props</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s1">&#39;round&#39;</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;wheat&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">verticalalignment</span><span class="o">=</span><span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="n">props</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">dependency</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">.png&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dependency</span><span class="p">))</span>
</pre></div>
</div>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="_images/linear.png"><img alt="Linear Codependence" src="_images/linear.png" style="width: 403.2px; height: 352.79999999999995px;" /></a>
<figcaption>
<p><span class="caption-text">Linear</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="_images/squared.png"><img alt="Squared Codependence" src="_images/squared.png" style="width: 403.2px; height: 352.79999999999995px;" /></a>
<figcaption>
<p><span class="caption-text">Squared</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="_images/abs.png"><img alt="Absolute Codependence" src="_images/abs.png" style="width: 403.2px; height: 352.79999999999995px;" /></a>
<figcaption>
<p><span class="caption-text">Absolute</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="_images/independent.png"><img alt="No Relationship" src="_images/independent.png" style="width: 403.2px; height: 352.79999999999995px;" /></a>
<figcaption>
<p><span class="caption-text">Indepedent</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
</section>
<span id="document-codependence/codependence_marti"></span><div class="admonition note" id="implementations-codependence-marti">
<p class="admonition-title">Note</p>
<p>The following implementations and documentation closely follow the work of Gautier Marti:
<a class="reference external" href="https://www.researchgate.net/publication/322714557">Some contributions to the clustering of financial time series and applications to credit default swaps</a>.</p>
</div>
<section id="codependence-by-marti">
<h3>Codependence by Marti<a class="headerlink" href="#codependence-by-marti" title="Permalink to this heading">¶</a></h3>
<p>The work mentioned above introduces a new approach of representing the random variables that splits apart dependency and
distribution without losing any information. It also contains a distance metric between two financial time series based
on a novel approach.</p>
<p>According to the author’s classification:</p>
<p>“Many statistical distances exist to measure the dissimilarity of two random variables, and therefore two i.i.d. random
processes. Such distances can be roughly classified in two families:</p>
<blockquote>
<div><p>1. distributional distances, […] which focus on dissimilarity between probability distributions and quantify divergences
in marginal behaviours,</p>
<p>2. dependence distances, such as the distance correlation or copula-based kernel dependency measures […],
which focus on the joint behaviours of random variables, generally ignoring their distribution properties.</p>
</div></blockquote>
<p>However, we may want to be able to discriminate random variables both on distribution and dependence. This can be
motivated, for instance, from the study of financial assets returns: are two perfectly correlated random variables
(assets returns), but one being normally distributed and the other one following a heavy-tailed distribution, similar?
From risk perspective, the answer is no […], hence the propounded distance of this article”.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Read the original work to understand the motivation behind creating the novel technique deeper and check the reference
papers that prove the above statements.</p>
</div>
<section id="spearmans-rho">
<h4>Spearman’s Rho<a class="headerlink" href="#spearmans-rho" title="Permalink to this heading">¶</a></h4>
<p>Following the work of Marti:</p>
<p>“[The Pearson correlation coefficient] suffers from several drawbacks:
- it only measures linear relationship between two variables;
- it is not robust to noise
- it may be undefined if the distribution of one of these variables have infinite second moment.</p>
<p>More robust correlation coefficients are copula-based dependence measures such as Spearman’s rho:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\rho_{S}(X, Y) &amp;= 12 E[F_{X}(X), F_{Y}(Y)] - 3 \\
&amp;= \rho(F_{X}(X), F_{Y}(Y))\end{split}\]</div>
<p>and its statistical estimate:</p>
<div class="math notranslate nohighlight">
\[\hat{\rho}_{S}(X, Y) = 1 - \frac{6}{T(T^2-1)}\sum_{t=1}^{T}(X^{(t)}- Y^{(t)})^2\]</div>
<p>where <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are univariate random variables, <span class="math notranslate nohighlight">\(F_{X}(X)\)</span> is the cumulative distribution
function of <span class="math notranslate nohighlight">\(X\)</span> , <span class="math notranslate nohighlight">\(X^{(t)}\)</span> is the <span class="math notranslate nohighlight">\(t\)</span> -th sorted observation of <span class="math notranslate nohighlight">\(X\)</span> , and <span class="math notranslate nohighlight">\(T\)</span> is the
total number of observations”.</p>
<p>Our method is a wrapper for the scipy spearmanr function. For more details about the function and its parameters,
please visit <a class="reference external" href="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.spearmanr.html">scipy documentation</a>.</p>
<section id="implementation">
<h5>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h5>
</section>
</section>
<section id="generic-parametric-representation-gpr-distance">
<h4>Generic Parametric Representation (GPR) distance<a class="headerlink" href="#generic-parametric-representation-gpr-distance" title="Permalink to this heading">¶</a></h4>
<p>Theoretically, Marty defines the distance <span class="math notranslate nohighlight">\(d_{\Theta}\)</span> between two random variables as:</p>
<p>“ Let <span class="math notranslate nohighlight">\(\theta \in [0, 1]\)</span> . Let <span class="math notranslate nohighlight">\((X, Y) \in \nu^{2}\)</span> , where <span class="math notranslate nohighlight">\(\nu\)</span> is the space of all continuous
real-valued random variables. Let <span class="math notranslate nohighlight">\(G = (G_{X}, G_{Y})\)</span> , where <span class="math notranslate nohighlight">\(G_{X}\)</span> and <span class="math notranslate nohighlight">\(G_{Y}\)</span> are respectively
<span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> marginal cdfs. We define the following distance</p>
<div class="math notranslate nohighlight">
\[d_{\Theta}^{2}(X, Y) = \Theta d_{1}^{2}(G_{X}(X), G_{Y}(Y)) + (1 - \Theta) d_{0}^{2}(G_{X}, G_{Y})\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[d_{1}^{2}(G_{X}(X), G_{Y}(Y)) = 3 \mathbb{E}[|G_{X}(X) - G_{Y}(Y)|^{2}]\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[d_{0}^{2}(G_{X}, G_{Y}) = \frac{1}{2} \int_{R} (\sqrt{\frac{d G_{X}}{d \lambda}} -
\sqrt{\frac{d G_{Y}}{d \lambda}})^{2} d \lambda  &quot;\]</div>
<p>For two Gaussian random variables, the distance <span class="math notranslate nohighlight">\(d_{\Theta}\)</span> is therefore defined by Marti as:</p>
<p>“ Let <span class="math notranslate nohighlight">\((X, Y)\)</span> be a bivariate Gaussian vector, with <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(\mu_{X}, \sigma_{X}^{2})\)</span> ,
<span class="math notranslate nohighlight">\(Y \sim \mathcal{N}(\mu_{Y}, \sigma_{Y}^{2})\)</span> and <span class="math notranslate nohighlight">\(\rho (X,Y)\)</span> . We obtain,</p>
<div class="math notranslate nohighlight">
\[d_{\Theta}^{2}(X, Y) = \Theta \frac{1 - \rho_{S}}{2} + (1 - \Theta) (1 -
\sqrt{\frac{2 \sigma_{X} \sigma_{Y}}{\sigma_{X}^{2} + \sigma_{Y}^{2}}} e^{ -
\frac{1}{4} \frac{(\mu_{X} - \mu_{Y})^{2}}{\sigma_{X}^{2} + \sigma_{Y}^{2}}})  &quot;\]</div>
<p>The use of this distance is referenced as the generic parametric representation (GPR) approach.</p>
<p>From the paper:</p>
<p>“GPR distance is a fast and good proxy for distance <span class="math notranslate nohighlight">\(d_{\Theta}\)</span> when the first two moments <span class="math notranslate nohighlight">\(\mu\)</span>
and <span class="math notranslate nohighlight">\({\sigma}\)</span> predominate. Nonetheless, for datasets which contain heavy-tailed distributions,
GPR fails to capture this information”.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The process of deriving this definition as well as a proof that <span class="math notranslate nohighlight">\(d_{\Theta}\)</span> is a metric is present in the work:
<a class="reference external" href="https://www.researchgate.net/publication/322714557">Some contributions to the clustering of financial time series and applications to credit default swaps</a>.</p>
</div>
<section id="id2">
<h5>Implementation<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h5>
</section>
</section>
<section id="generic-non-parametric-representation-gnpr-distance">
<h4>Generic Non-Parametric Representation (GNPR) distance<a class="headerlink" href="#generic-non-parametric-representation-gnpr-distance" title="Permalink to this heading">¶</a></h4>
<p>The statistical estimate of the distance <span class="math notranslate nohighlight">\(\tilde{d}_{\Theta}\)</span> working on realizations of the i.i.d. random variables
is defined by the author as:</p>
<p>“ Let <span class="math notranslate nohighlight">\((X^{t})_{t=1}^{T}\)</span> and <span class="math notranslate nohighlight">\((Y^{t})_{t=1}^{T}\)</span> be <span class="math notranslate nohighlight">\(T\)</span> realizations of real-valued random variables
<span class="math notranslate nohighlight">\(X, Y \in \nu\)</span> respectively. An empirical distance between realizations of random variables can be defined by</p>
<div class="math notranslate nohighlight">
\[\tilde{d}_{\Theta}^{2}((X^{t})_{t=1}^{T}, (Y^{t})_{t=1}^{T}) \stackrel{\text{a.s.}}{=}
\Theta \tilde{d}_{1}^{2} + (1 - \Theta) \tilde{d}_{0}^{2}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\tilde{d}_{1}^{2} = \frac{3}{T(T^{2} - 1)} \sum_{t = 1}^{T} (X^{(t)} - Y^{(t)}) ^ {2}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\tilde{d}_{0}^{2} = \frac{1}{2} \sum_{k = - \infty}^{+ \infty} (\sqrt{g_{X}^{h}(hk)} - \sqrt{g_{Y}^{h}(hk)})^{2}\]</div>
<p><span class="math notranslate nohighlight">\(h\)</span> being here a suitable bandwidth, and
<span class="math notranslate nohighlight">\(g_{X}^{h}(x) = \frac{1}{T} \sum_{t = 1}^{T} \mathbf{1}(\lfloor \frac{x}{h} \rfloor h \le X^{t} &lt;
(\lfloor \frac{x}{h} \rfloor + 1)h)\)</span> being a density histogram estimating dpf <span class="math notranslate nohighlight">\(g_{X}\)</span> from
<span class="math notranslate nohighlight">\((X^{t})_{t=1}^{T}\)</span> , <span class="math notranslate nohighlight">\(T\)</span> realization of a random variable <span class="math notranslate nohighlight">\(X \in \nu\)</span> “.</p>
<p>The use of this distance is referenced as the generic non-parametric representation (GNPR) approach.</p>
<p>As written in the paper:</p>
<p>“ To use effectively <span class="math notranslate nohighlight">\(d_{\Theta}\)</span> and its statistical estimate, it boils down to select a particular value for
<span class="math notranslate nohighlight">\(\Theta\)</span> . We suggest here an exploratory approach where one can test</p>
<blockquote>
<div><ol class="lowerroman simple">
<li><p>distribution information (θ = 0),</p></li>
<li><p>dependence information (θ = 1), and</p></li>
<li><p>a mix of both information (θ = 0,5).</p></li>
</ol>
</div></blockquote>
<p>Ideally, <span class="math notranslate nohighlight">\(\Theta\)</span> should reflect the balance of dependence and distribution information in the data.
In a supervised setting, one could select an estimate <span class="math notranslate nohighlight">\(\hat{\Theta}\)</span> of the right balance <span class="math notranslate nohighlight">\(\Theta^{*}\)</span>
optimizing some loss function by techniques such as cross-validation. Yet, the lack of a clear loss function makes
the estimation of <span class="math notranslate nohighlight">\(\Theta^{*}\)</span> difficult in an unsupervised setting”.</p>
<section id="id3">
<h5>Implementation<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h5>
</section>
</section>
<section id="examples">
<h4>Examples<a class="headerlink" href="#examples" title="Permalink to this heading">¶</a></h4>
<p>The following example shows how the above functions can be used:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.codependence</span> <span class="kn">import</span> <span class="n">spearmans_rho</span><span class="p">,</span> <span class="n">gpr_distance</span><span class="p">,</span> <span class="n">gnpr_distance</span>

<span class="c1"># Getting the dataframe with time series of returns</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;X_FILE_PATH.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">element_x</span> <span class="o">=</span> <span class="s1">&#39;SPY&#39;</span>
<span class="n">element_y</span> <span class="o">=</span> <span class="s1">&#39;TLT&#39;</span>

<span class="c1"># Calculating the Spearman&#39;s rho coefficient between two time series</span>
<span class="n">rho</span> <span class="o">=</span> <span class="n">spearmans_rho</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">element_x</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">element_y</span><span class="p">])</span>

<span class="c1"># Calculating the GPR distance between two time series with both</span>
<span class="c1"># distribution and dependence information</span>
<span class="n">gpr_dist</span> <span class="o">=</span> <span class="n">gpr_distance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">element_x</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">element_y</span><span class="p">],</span> <span class="n">theta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Calculating the GNPR distance between two time series with dependence information only</span>
<span class="n">gnpr_dist</span> <span class="o">=</span> <span class="n">gnpr_distance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">element_x</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">element_y</span><span class="p">],</span> <span class="n">theta</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<section id="research-notebooks">
<h5>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h5>
<p>The following research notebook can be used to better understand the codependence metrics described above.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Codependence/Codependence%20by%20Marti/codependence_by_marti.ipynb">Codependence by Marti</a></p></li>
</ul>
</section>
</section>
</section>
<span id="document-codependence/codependence_matrix"></span><section id="codependence-matrix">
<span id="codependence-codependence-matrix"></span><h3>Codependence Matrix<a class="headerlink" href="#codependence-matrix" title="Permalink to this heading">¶</a></h3>
<p>The functions in this part of the module are used to generate dependence and distance matrices using the codependency and
distance metrics described previously.</p>
<ol class="arabic simple">
<li><p><strong>Dependence Matrix</strong> function is used to compute codependences between elements in a given dataframe of elements
using various codependence metrics like Mutual Information, Variation of Information, Distance Correlation,
Spearman’s Rho, GPR distance, and GNPR distance.</p></li>
<li><p><strong>Distance Matrix</strong> function can be used to compute a distance matrix from a given codependency matrix using
distance metrics like angular, squared angular and absolute angular.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>MlFinLab makes use of these functions in the clustered feature importance and portfolio optimization modules.</p>
</div>
<section id="implementation">
<h4>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h4>
</section>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.codependence</span> <span class="kn">import</span> <span class="n">get_dependence_matrix</span><span class="p">,</span> <span class="n">get_distance_matrix</span>

 <span class="c1"># Import dataframe of returns for assets in a portfolio</span>
 <span class="n">asset_returns</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">DATA_PATH</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

 <span class="c1"># Calculate distance correlation matrix</span>
 <span class="n">distance_corr</span> <span class="o">=</span> <span class="n">get_dependence_matrix</span><span class="p">(</span><span class="n">asset_returns</span><span class="p">,</span> <span class="n">dependence_method</span><span class="o">=</span><span class="s1">&#39;distance_correlation&#39;</span><span class="p">)</span>

 <span class="c1"># Calculate Pearson correlation matrix</span>
 <span class="n">pearson_corr</span> <span class="o">=</span> <span class="n">asset_returns</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>

 <span class="c1"># Calculate absolute angular distance from a Pearson correlation matrix</span>
 <span class="n">abs_angular_dist</span> <span class="o">=</span> <span class="n">absolute_angular_distance</span><span class="p">(</span><span class="n">pearson_corr</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-labeling/tb_meta_labeling"></span><section id="triple-barrier-and-meta-labelling">
<h3>Triple-Barrier and Meta-Labelling<a class="headerlink" href="#triple-barrier-and-meta-labelling" title="Permalink to this heading">¶</a></h3>
<p>The primary labeling method used in financial academia is the fixed-time horizon method. While ubiquitous, this method
has many faults which are remedied by the triple-barrier method discussed below. The triple-barrier method can be
extended to incorporate meta-labeling which will also be demonstrated and discussed below.</p>
<section id="triple-barrier-method">
<h4>Triple-Barrier Method<a class="headerlink" href="#triple-barrier-method" title="Permalink to this heading">¶</a></h4>
<p>The idea behind the triple-barrier method is that we have three barriers: an upper barrier, a lower barrier, and a
vertical barrier. The upper barrier represents the threshold an observation’s return needs to reach in order to be
considered a buying opportunity (a label of 1), the lower barrier represents the threshold an observation’s return needs
to reach in order to be considered a selling opportunity (a label of -1), and the vertical barrier represents the amount
of time an observation has to reach its given return in either direction before it is given a label of 0. This concept
can be better understood visually and is shown in the figure below taken from Advances in Financial Machine
Learning (<a class="reference external" href="https://www.wiley.com/en-us/Advances+in+Financial+Machine+Learning-p-9781119482086">reference</a>):</p>
<a class="reference internal image-reference" href="_images/triple_barrier.png"><img alt="_images/triple_barrier.png" class="align-center" src="_images/triple_barrier.png" style="width: 468.0px; height: 346.0px;" /></a>
<p>One of the major faults with the fixed-time horizon method is that observations are given a label with respect to a certain
threshold after a fixed interval regardless of their respective volatilities. In other words, the expected returns of every
observation are treated equally regardless of the associated risk. The triple-barrier method tackles this issue by dynamically
setting the upper and lower barriers for each observation based on their given volatilities.</p>
</section>
<section id="meta-labeling">
<h4>Meta-Labeling<a class="headerlink" href="#meta-labeling" title="Permalink to this heading">¶</a></h4>
<p>Advances in Financial Machine Learning, Chapter 3, page 50. Reads:</p>
<p>“Suppose that you have a model for setting the side of the bet (long or short). You just need to learn the size of that
bet, which includes the possibility of no bet at all (zero size). This is a situation that practitioners face regularly.
We often know whether we want to buy or sell a product, and the only remaining question is how much money we should risk
in such a bet. We do not want the ML algorithm to learn the side, just to tell us what is the appropriate size. At this
point, it probably does not surprise you to hear that no book or paper has so far discussed this common problem. Thankfully,
that misery ends here.””</p>
<p>I call this problem meta-labeling because we want to build a secondary ML model that learns how to use a primary exogenous model.</p>
<p>The ML algorithm will be trained to decide whether to take the bet or pass, a purely binary prediction. When the predicted
label is 1, we can use the probability of this secondary prediction to derive the size of the bet, where the side (sign) of
the position has been set by the primary model.</p>
<section id="how-to-use-meta-labeling">
<h5>How to use Meta-Labeling<a class="headerlink" href="#how-to-use-meta-labeling" title="Permalink to this heading">¶</a></h5>
<p>Binary classification problems present a trade-off between type-I errors (false positives) and type-II errors (false negatives).
In general, increasing the true positive rate of a binary classifier will tend to increase its false positive rate. The receiver
operating characteristic (ROC) curve of a binary classifier measures the cost of increasing the true positive rate, in terms of
accepting higher false positive rates.</p>
<a class="reference internal image-reference" href="_images/confusion_matrix.png"><img alt="_images/confusion_matrix.png" class="align-center" src="_images/confusion_matrix.png" style="width: 264.0px; height: 480.0px;" /></a>
<p>The image illustrates the so-called “confusion matrix.” On a set of observations, there are items that exhibit a condition
(positives, left rectangle), and items that do not exhibit a condition (negative, right rectangle). A binary classifier predicts
that some items exhibit the condition (ellipse), where the TP area contains the true positives and the TN area contains the true negatives.
This leads to two kinds of errors: false positives (FP) and false negatives (FN). “Precision” is the ratio between the TP area and
the area in the ellipse. “Recall” is the ratio between the TP area and the area in the left rectangle. This notion of recall
(aka true positive rate) is in the context of classification problems, the analogous to “power” in the context of hypothesis testing.
“Accuracy” is the sum of the TP and TN areas divided by the overall set of items (square). In general, decreasing the FP area comes at
a cost of increasing the FN area, because higher precision typically means fewer calls, hence lower recall. Still, there is some
combination of precision and recall that maximizes the overall efficiency of the classifier. The F1-score measures the efficiency of a
classifier as the harmonic average between precision and recall.</p>
<p><strong>Meta-labeling is particularly helpful when you want to achieve higher F1-scores</strong>. First, we build a model that achieves high recall,
even if the precision is not particularly high. Second, we correct for the low precision by applying meta-labeling to the positives predicted
by the primary model.</p>
<p>Meta-labeling will increase your F1-score by filtering out the false positives, where the majority of positives have already been identified
by the primary model. Stated differently, the role of the secondary ML algorithm is to determine whether a positive from the primary (exogenous)
model is true or false. It is not its purpose to come up with a betting opportunity. Its purpose is to determine whether we should act or pass
on the opportunity that has been presented.</p>
<p>Meta-labeling is a very powerful tool to have in your arsenal, for four additional reasons. <strong>First</strong>, ML algorithms are often criticized as
black boxes. Meta-labeling allows you to build an ML system on top of a white box (like a fundamental model founded on economic theory). This
ability to transform a fundamental model into an ML model should make meta-labeling particularly useful to “quantamental” firms. <strong>Second</strong>,
the effects of overfitting are limited when you apply metalabeling, because ML will not decide the side of your bet, only the size. <strong>Third</strong>,
by decoupling the side prediction from the size prediction, meta-labeling enables sophisticated strategy structures. For instance, consider that
the features driving a rally may differ from the features driving a sell-off. In that case, you may want to develop an ML strategy exclusively
for long positions, based on the buy recommendations of a primary model, and an ML strategy exclusively for short positions, based on the sell
recommendations of an entirely different primary model. <strong>Fourth</strong>, achieving high accuracy on small bets and low accuracy on large bets will ruin you.
As important as identifying good opportunities is to size them properly, so it makes sense to develop an ML algorithm solely focused on getting that
critical decision (sizing) right. We will retake this fourth point in Chapter 10. In my experience, meta-labeling ML models can deliver more robust
and reliable outcomes than standard labeling models.</p>
</section>
<section id="model-architecture">
<h5>Model Architecture<a class="headerlink" href="#model-architecture" title="Permalink to this heading">¶</a></h5>
<p>The following image explains the model architecture. The <strong>first</strong> step is to train a primary model (binary classification).
<strong>Second</strong> a threshold level is determined at which the primary model has a high recall, in the coded example you will find that
0.30 is a good threshold, ROC curves could be used to help determine a good level. <strong>Third</strong> the features from the first model
are concatenated with the predictions from the first model, into a new feature set for the secondary model. Meta Labels are used
as the target variable in the second model. Now fit the second model. <strong>Fourth</strong> the prediction from the secondary model is combined
with the prediction from the primary model and only where both are true, is your final prediction true. I.e. if your primary model
predicts a 3 and your secondary model says you have a high probability of the primary model being correct, is your final prediction
a 3, else not 3.</p>
<a class="reference internal image-reference" href="_images/meta_labeling_architecture.png"><img alt="_images/meta_labeling_architecture.png" class="align-center" src="_images/meta_labeling_architecture.png" style="width: 527.1px; height: 475.99999999999994px;" /></a>
</section>
</section>
<section id="implementation">
<h4>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h4>
<p>The following functions are used for the triple-barrier method which works in tandem with meta-labeling.</p>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.labeling.labeling.add_vertical_barrier">
<span class="sig-name descname"><span class="pre">add_vertical_barrier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t_events</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">close</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_days</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_hours</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_minutes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_seconds</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.labeling.labeling.add_vertical_barrier" title="Permalink to this definition">¶</a></dt>
<dd><p>Advances in Financial Machine Learning, Snippet 3.4 page 49.</p>
<p>Adding a Vertical Barrier</p>
<p>For each index in t_events, it finds the timestamp of the next price bar at or immediately after
a number of days num_days. This vertical barrier can be passed as an optional argument t1 in get_events.</p>
<p>This function creates a series that has all the timestamps of when the vertical barrier would be reached.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t_events</strong> – (pd.Series) Series of events (symmetric CUSUM filter)</p></li>
<li><p><strong>close</strong> – (pd.Series) Close prices</p></li>
<li><p><strong>num_days</strong> – (int) Number of days to add for vertical barrier</p></li>
<li><p><strong>num_hours</strong> – (int) Number of hours to add for vertical barrier</p></li>
<li><p><strong>num_minutes</strong> – (int) Number of minutes to add for vertical barrier</p></li>
<li><p><strong>num_seconds</strong> – (int) Number of seconds to add for vertical barrier</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.Series) Timestamps of vertical barriers</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.labeling.labeling.get_events">
<span class="sig-name descname"><span class="pre">get_events</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">close</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">t_events</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pt_sl</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_ret</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_threads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vertical_barrier_times</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">side_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.labeling.labeling.get_events" title="Permalink to this definition">¶</a></dt>
<dd><p>Advances in Financial Machine Learning, Snippet 3.6 page 50.</p>
<p>Getting the Time of the First Touch, with Meta Labels</p>
<p>This function is orchestrator to meta-label the data, in conjunction with the Triple Barrier Method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>close</strong> – (pd.Series) Close prices</p></li>
<li><p><strong>t_events</strong> – (pd.Series) of t_events. These are timestamps that will seed every triple barrier.
These are the timestamps selected by the sampling procedures discussed in Chapter 2, Section 2.5.
Eg: CUSUM Filter</p></li>
<li><p><strong>pt_sl</strong> – (2 element array) Element 0, indicates the profit taking level; Element 1 is stop loss level.
A non-negative float that sets the width of the two barriers. A 0 value means that the respective
horizontal barrier (profit taking and/or stop loss) will be disabled.</p></li>
<li><p><strong>target</strong> – (pd.Series) of values that are used (in conjunction with pt_sl) to determine the width
of the barrier. In this program this is daily volatility series.</p></li>
<li><p><strong>min_ret</strong> – (float) The minimum target return required for running a triple barrier search.</p></li>
<li><p><strong>num_threads</strong> – (int) The number of threads concurrently used by the function.</p></li>
<li><p><strong>vertical_barrier_times</strong> – (pd.Series) A pandas series with the timestamps of the vertical barriers.
We pass a False when we want to disable vertical barriers.</p></li>
<li><p><strong>side_prediction</strong> – (pd.Series) Side of the bet (long/short) as decided by the primary model</p></li>
<li><p><strong>verbose</strong> – (bool) Flag to report progress on asynch jobs</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) Events
-events.index is event’s starttime
-events[‘t1’] is event’s endtime
-events[‘trgt’] is event’s target
-events[‘side’] (optional) implies the algo’s position side
-events[‘pt’] is profit taking multiple
-events[‘sl’]  is stop loss multiple</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.labeling.labeling.get_bins">
<span class="sig-name descname"><span class="pre">get_bins</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">triple_barrier_events</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">close</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.labeling.labeling.get_bins" title="Permalink to this definition">¶</a></dt>
<dd><p>Advances in Financial Machine Learning, Snippet 3.7, page 51.</p>
<p>Labeling for Side &amp; Size with Meta Labels</p>
<p>Compute event’s outcome (including side information, if provided).
events is a DataFrame where:</p>
<p>Now the possible values for labels in out[‘bin’] are {0,1}, as opposed to whether to take the bet or pass,
a purely binary prediction. When the predicted label the previous feasible values {−1,0,1}.
The ML algorithm will be trained to decide is 1, we can use the probability of this secondary prediction
to derive the size of the bet, where the side (sign) of the position has been set by the primary model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>triple_barrier_events</strong> – (pd.DataFrame)
-events.index is event’s starttime
-events[‘t1’] is event’s endtime
-events[‘trgt’] is event’s target
-events[‘side’] (optional) implies the algo’s position side
Case 1: (‘side’ not in events): bin in (-1,1) &lt;-label by price action
Case 2: (‘side’ in events): bin in (0,1) &lt;-label by pnl (meta-labeling)</p></li>
<li><p><strong>close</strong> – (pd.Series) Close prices</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) Meta-labeled events</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.labeling.labeling.drop_labels">
<span class="sig-name descname"><span class="pre">drop_labels</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">events</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_pct</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.05</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.labeling.labeling.drop_labels" title="Permalink to this definition">¶</a></dt>
<dd><p>Advances in Financial Machine Learning, Snippet 3.8 page 54.</p>
<p>This function recursively eliminates rare observations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>events</strong> – (dp.DataFrame) Events.</p></li>
<li><p><strong>min_pct</strong> – (float) A fraction used to decide if the observation occurs less than that fraction.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) Events.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h4>
<p>Suppose we use a mean-reverting strategy as our primary model, giving each observation a label of -1 or 1.
We can then use meta-labeling to act as a filter for the bets of our primary model.</p>
<p>Assuming we have a pandas series with the timestamps of our observations and their respective labels given by the primary
model, the process to generate meta-labels goes as follows.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">mlfinlab</span> <span class="k">as</span> <span class="nn">ml</span>

<span class="c1"># Read in data</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">)</span>

<span class="c1"># Compute daily volatility</span>
<span class="n">daily_vol</span> <span class="o">=</span> <span class="n">ml</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">get_daily_vol</span><span class="p">(</span><span class="n">close</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;close&#39;</span><span class="p">],</span> <span class="n">lookback</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="c1"># Apply Symmetric CUSUM Filter and get timestamps for events</span>
<span class="c1"># Note: Only the CUSUM filter needs a point estimate for volatility</span>
<span class="n">cusum_events</span> <span class="o">=</span> <span class="n">ml</span><span class="o">.</span><span class="n">filters</span><span class="o">.</span><span class="n">cusum_filter</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;close&#39;</span><span class="p">],</span>
                                       <span class="n">threshold</span><span class="o">=</span><span class="n">daily_vol</span><span class="p">[</span><span class="s1">&#39;2011-09-01&#39;</span><span class="p">:</span><span class="s1">&#39;2018-01-01&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="c1"># Compute vertical barrier</span>
<span class="n">vertical_barriers</span> <span class="o">=</span> <span class="n">ml</span><span class="o">.</span><span class="n">labeling</span><span class="o">.</span><span class="n">add_vertical_barrier</span><span class="p">(</span><span class="n">t_events</span><span class="o">=</span><span class="n">cusum_events</span><span class="p">,</span>
                                                     <span class="n">close</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;close&#39;</span><span class="p">],</span>
                                                     <span class="n">num_days</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Once we have computed the daily volatility along with our vertical time barriers and have downsampled our series using
the CUSUM filter, we can use the triple-barrier method to compute our meta-labels by passing in the side predicted by
the primary model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pt_sl</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">min_ret</span> <span class="o">=</span> <span class="mf">0.005</span>
<span class="n">triple_barrier_events</span> <span class="o">=</span> <span class="n">ml</span><span class="o">.</span><span class="n">labeling</span><span class="o">.</span><span class="n">get_events</span><span class="p">(</span><span class="n">close</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;close&#39;</span><span class="p">],</span>
                                               <span class="n">t_events</span><span class="o">=</span><span class="n">cusum_events</span><span class="p">,</span>
                                               <span class="n">pt_sl</span><span class="o">=</span><span class="n">pt_sl</span><span class="p">,</span>
                                               <span class="n">target</span><span class="o">=</span><span class="n">daily_vol</span><span class="p">,</span>
                                               <span class="n">min_ret</span><span class="o">=</span><span class="n">min_ret</span><span class="p">,</span>
                                               <span class="n">num_threads</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                               <span class="n">vertical_barrier_times</span><span class="o">=</span><span class="n">vertical_barriers</span><span class="p">,</span>
                                               <span class="n">side_prediction</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;side&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>As can be seen above, we have scaled our lower barrier and set our minimum return to 0.005.</p>
<p>Meta-labels can then be computed using the time that each observation touched its respective barrier.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">meta_labels</span> <span class="o">=</span> <span class="n">ml</span><span class="o">.</span><span class="n">labeling</span><span class="o">.</span><span class="n">get_bins</span><span class="p">(</span><span class="n">triple_barrier_events</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;close&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>This example ends with creating the meta-labels. To see a further explanation of using these labels in a secondary model
to help filter out false positives, see the research notebooks below.</p>
</section>
<section id="blog-posts">
<h4>Blog Posts<a class="headerlink" href="#blog-posts" title="Permalink to this heading">¶</a></h4>
<section id="does-meta-labeling-add-to-signal-efficacy">
<h5>Does Meta Labeling Add to Signal Efficacy?<a class="headerlink" href="#does-meta-labeling-add-to-signal-efficacy" title="Permalink to this heading">¶</a></h5>
<p>Successful and long-lasting quantitative research programs require a solid foundation that includes procurement and
curation of data, creation of building blocks for feature engineering, state of the art methodologies, and backtesting.
In this project we explore an example of applying meta labeling to high quality S&amp;P500 EMini Futures data and create an
open-source python package (mlfinlab) that is based on the work of Dr. Marcos Lopez de Prado in his book
‘Advances in Financial Machine Learning’. Dr. de Prado’s book provides a guideline for creating a successful platform.
We also implement a Trend Following and Mean-reverting Bollinger band based trading strategies. Our results confirm the
fact that a combination of event-based sampling, triple-barrier method and meta labeling improves the performance of the
strategies.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://hudsonthames.org/does-meta-labeling-add-to-signal-efficacy-triple-barrier-method">WorldQuant University Final Project</a></p></li>
</ul>
</section>
<section id="meta-labeling-a-toy-example">
<h5>Meta Labeling (A Toy Example)<a class="headerlink" href="#meta-labeling-a-toy-example" title="Permalink to this heading">¶</a></h5>
<p>This blog post investigates the idea of Meta Labeling and tries to help build an intuition for what is taking place.
The idea of meta-labeling is first mentioned in the textbook Advances in Financial Machine Learning by Marcos Lopez de
Prado and promises to improve model and strategy performance metrics by helping to filter-out false positives.</p>
<p>We make use of a computer vision problem known as the MNIST handwritten digit classification. By using of a non-financial
timeseries data set we can illustrate the components that make up meta labeling more clearly. Lets begin!</p>
<ul class="simple">
<li><p><a class="reference external" href="https://hudsonthames.org/meta-labeling-a-toy-example">A Toy Example</a></p></li>
</ul>
</section>
</section>
<section id="research-notebooks">
<h4>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h4>
<p>The following research notebooks can be used to better understand the triple-barrier method and meta-labeling</p>
<section id="id1">
<h5>Triple-Barrier Method<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Labelling/Trend-Follow-Question.ipynb">Trend Follow Question</a></p></li>
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Labelling/BBand-Question.ipynb">Bollinger band Question</a></p></li>
</ul>
</section>
<section id="meta-labeling-toy-example">
<h5>Meta-Labeling Toy Example<a class="headerlink" href="#meta-labeling-toy-example" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Labelling/Meta-Labels-MNIST.ipynb">Meta Labeling MNIST</a></p></li>
</ul>
</section>
</section>
</section>
<span id="document-labeling/labeling_trend_scanning"></span><section id="trend-scanning">
<span id="implementations-labeling-trend-scanning"></span><h3>Trend Scanning<a class="headerlink" href="#trend-scanning" title="Permalink to this heading">¶</a></h3>
<a class="reference internal image-reference" href="_images/trend_scanning_plot.png"><img alt="_images/trend_scanning_plot.png" class="align-center" src="_images/trend_scanning_plot.png" style="width: 560.0px; height: 354.0px;" /></a>
<p>Trend Scanning is both a classification and regression labeling technique introduced by Marcos Lopez de Prado in the
following lecture slides: <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3257419">Advances in Financial Machine Learning, Lecture 3/10</a>, and again in his text book <a class="reference external" href="https://www.cambridge.org/core/books/machine-learning-for-asset-managers/6D9211305EA2E425D33A9F38D0AE3545">Machine Learning for Asset Managers</a>.</p>
<p>For some trading algorithms, the researcher may not want to explicitly set a fixed profit / stop loss level, but rather detect overall
trend direction and sit in a position until the trend changes. For example, market timing strategy which holds ETFs except during volatile
periods. Trend scanning labels are designed to solve this type of problems.</p>
<p>This algorithm is also useful for defining market regimes between downtrend, no-trend, and uptrend.</p>
<p>The idea of trend-scanning labels are to fit multiple regressions from time t to t + L (L is a maximum look-forward window)
and select the one which yields maximum t-value for the slope coefficient, for a specific observation.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ol class="arabic simple">
<li><p>Classification: By taking the sign of t-value for a given observation we can set {-1, 1} labels to define the trends as either downward or upward.</p></li>
<li><p>Classification: By adding a minimum t-value threshold you can generate {-1, 0, 1} labels for downward, no-trend, upward.</p></li>
<li><p>The t-values can be used as sample weights in classification problems.</p></li>
<li><p>Regression: The t-values can be used in a regression setting to determine the magnitude of the trend.</p></li>
</ol>
</div>
<p>The output of this algorithm is a DataFrame with t1 (time stamp for the farthest observation), t-value, returns for the trend, and bin.</p>
<section id="module-mlfinlab.labeling.trend_scanning">
<span id="implementation"></span><h4>Implementation<a class="headerlink" href="#module-mlfinlab.labeling.trend_scanning" title="Permalink to this heading">¶</a></h4>
<p>Implementation of Trend-Scanning labels described in <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2708678">Advances in Financial Machine Learning: Lecture 3/10</a></p>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.labeling.trend_scanning.trend_scanning_labels">
<span class="sig-name descname"><span class="pre">trend_scanning_labels</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">price_series</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Series</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">t_events</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">list</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">look_forward_window</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_sample_length</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">DataFrame</span></span></span><a class="headerlink" href="#mlfinlab.labeling.trend_scanning.trend_scanning_labels" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3257419">Trend scanning</a> is both a classification and
regression labeling technique.</p>
<p>That can be used in the following ways:</p>
<ol class="arabic simple">
<li><p>Classification: By taking the sign of t-value for a given observation we can set {-1, 1} labels to define the
trends as either downward or upward.</p></li>
<li><p>Classification: By adding a minimum t-value threshold you can generate {-1, 0, 1} labels for downward, no-trend,
upward.</p></li>
<li><p>The t-values can be used as sample weights in classification problems.</p></li>
<li><p>Regression: The t-values can be used in a regression setting to determine the magnitude of the trend.</p></li>
</ol>
<p>The output of this algorithm is a DataFrame with t1 (time stamp for the farthest observation), t-value, returns for
the trend, and bin.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>price_series</strong> – (pd.Series) Close prices used to label the data set</p></li>
<li><p><strong>t_events</strong> – (list) Filtered events, array of pd.Timestamps</p></li>
<li><p><strong>look_forward_window</strong> – (int) Maximum look forward window used to get the trend value</p></li>
<li><p><strong>min_sample_length</strong> – (int) Minimum sample length used to fit regression</p></li>
<li><p><strong>step</strong> – (int) Optimal t-value index is searched every ‘step’ indices</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) Consists of t1, t-value, ret, bin (label information). t1 - label endtime, tvalue,
ret - price change %, bin - label value based on price change sign</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">from</span> <span class="nn">mlfinlab.labeling</span> <span class="kn">import</span> <span class="n">trend_scanning_labels</span>

<span class="bp">self</span><span class="o">.</span><span class="n">eem_close</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;./test_data/stock_prices.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1"># In 2008, EEM had some clear trends</span>
<span class="bp">self</span><span class="o">.</span><span class="n">eem_close</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eem_close</span><span class="p">[</span><span class="s1">&#39;EEM&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">Timestamp</span><span class="p">(</span><span class="mi">2008</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span><span class="n">pd</span><span class="o">.</span><span class="n">Timestamp</span><span class="p">(</span><span class="mi">2008</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>


<span class="n">t_events</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eem_close</span><span class="o">.</span><span class="n">index</span> <span class="c1"># Get indexes that we want to label</span>
<span class="c1"># We look at a maximum of the next 20 days to define the trend, however we fit regression on samples with length &gt;= 10</span>
<span class="n">tr_scan_labels</span> <span class="o">=</span> <span class="n">trend_scanning_labels</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eem_close</span><span class="p">,</span> <span class="n">t_events</span><span class="p">,</span> <span class="n">look_forward_window</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">min_sample_length</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<span id="document-labeling/labeling_tail_sets"></span><section id="tail-sets">
<span id="implementations-labeling-tail-sets"></span><h3>Tail Sets<a class="headerlink" href="#tail-sets" title="Permalink to this heading">¶</a></h3>
<p>Tail set labels are a classification labeling technique introduced in the following paper: <a class="reference external" href="https://content.iospress.com/download/algorithmic-finance/af016?id=algorithmic-finance%2Faf016">Huerta, R., Corbacho, F. and
Elkan, C., 2013. Nonlinear support vector machines can systematically identify stocks with high and low future returns.
Algorithmic Finance, 2(1), pp.45-58.</a></p>
<p>A tail set is defined to be a group of assets whose return is in the highest or lowest quantile, for example the highest or lowest 5%,
for a given timestamp. The returns may be volatility-adjusted.</p>
<p>A classification model is then fit using these labels to determine which stocks to buy and sell, for a long / short
portfolio.</p>
<p>We label the y variable using the tail set labeling technique, which makes up the positive and negative (1, -1) classes
of the training data. The original paper investigates the performance of 3 types of metrics on which the tail sets are
built:</p>
<ol class="arabic simple">
<li><p>Real returns</p></li>
<li><p>Residual alpha after regression on the sector index</p></li>
<li><p>Volatility-adjusted returns</p></li>
</ol>
<a class="reference internal image-reference" href="_images/performance_tail_sets.png"><img alt="_images/performance_tail_sets.png" class="align-center" src="_images/performance_tail_sets.png" style="width: 379.0px; height: 245.0px;" /></a>
<p>For our particular implementation, we have focused on the volatility-adjusted returns.</p>
<section id="metric-volatility-adjusted-returns">
<h4>Metric: Volatility-Adjusted Returns<a class="headerlink" href="#metric-volatility-adjusted-returns" title="Permalink to this heading">¶</a></h4>
<p>The formula for the volatility-adjusted returns are as follows:</p>
<div class="math notranslate nohighlight">
\[r(t - t', t) = \frac{R(t-t',t)}{vol(t)}\]</div>
<p>Where <span class="math notranslate nohighlight">\(R(t-t',t)\)</span> is the return for the asset, in our case we make use of daily (single period) returns, and
<span class="math notranslate nohighlight">\(vol(t-1)\)</span> is a measure for volatility on daily returns. We provide two implementations for estimations of
volatility, first the exponential moving average of the mean absolute returns, and second the traditional standard
deviation. (The paper suggests a 180 day window period.)</p>
<p>To quote the paper: “Huffman and Moll (2011) show that risk measured as the mean absolute deviation has more explanatory
power for future expected returns than standard deviation.”</p>
</section>
<section id="creating-tail-sets">
<h4>Creating Tail Sets<a class="headerlink" href="#creating-tail-sets" title="Permalink to this heading">¶</a></h4>
<p>Once the volatility adjusted returns have been applied to the DataFrame of prices we then loop over each timestamp
and group the assets into quantiles. The user inputs the number of quantiles desired using the n_bins input, and the highest and lowest
quantiles compose positive and negative tail sets, respectively. For example, if the highest and lowest octile are desired, then
n_bins would be 8.</p>
<p>Its important to note that we drop the 0 labels (for a given timestamp) and only train the model assets that made it into
the tail sets.</p>
<p>The following figure from the paper shows the distribution of the 91-day volatility-adjusted returns for the
industrials sector.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="_images/var_distribution.png"><img alt="tail sets" src="_images/var_distribution.png" style="width: 366.0px; height: 269.0px;" /></a>
<figcaption>
<p><span class="caption-text">The positive tail sets are the 10% most positive volatility-adjusted returns, and the negative tail sets are the
10% most negative. The vertical dotted lines represent the decile cut. The + and − regions are the ones used for
model training.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="how-to-use-these-labels-in-practice">
<h4>How to use these labels in practice?<a class="headerlink" href="#how-to-use-these-labels-in-practice" title="Permalink to this heading">¶</a></h4>
<p>The tail set labels from the code above returns the names of the assets which should be labeled with a positive or
negative label. Its important to note that the model you would develop is a many to one model, in that it has many
x variables and only one y variable. The model is a binary classifier.</p>
<p>The model is trained on the training data and then used to score every security in the test data (on a given day).
Example: On December 1st 2019, the strategy needs to rebalance its positions, we score all 100 securities in our tradable
universe and then rank the outputs in a top down fashion. We form a long / short portfolio by going long the top 10
stocks and short the bottom 10 (equally weighted). We then hold the position to the next rebalance date.</p>
<p>The paper provides the following investment performance:</p>
<a class="reference internal image-reference" href="_images/tail_sets_perf.png"><img alt="_images/tail_sets_perf.png" class="align-center" src="_images/tail_sets_perf.png" style="width: 391.0px; height: 236.0px;" /></a>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The Tail Set labels are for the current day! In order to use them as a labeling technique you need to lag them so that
they can be forward looking. We recommend using the pandas DataFrames <code class="docutils literal notranslate"><span class="pre">df.lag(1)</span></code> method.</p>
</div>
</section>
<section id="module-mlfinlab.labeling.tail_sets">
<span id="implementation"></span><h4>Implementation<a class="headerlink" href="#module-mlfinlab.labeling.tail_sets" title="Permalink to this heading">¶</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="mlfinlab.labeling.tail_sets.TailSetLabels">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">TailSetLabels</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bins</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vol_adj</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.labeling.tail_sets.TailSetLabels" title="Permalink to this definition">¶</a></dt>
<dd><p>Tail set labels are a classification labeling technique introduced in the following paper: Nonlinear support vector
machines can systematically identify stocks with high and low future returns. Algorithmic Finance, 2(1), pp.45-58.</p>
<p>A tail set is defined to be a group of stocks whose volatility-adjusted return is in the highest or lowest
quantile, for example the highest or lowest 5%.</p>
<p>A classification model is then fit using these labels to determine which stocks to buy and sell in a long / short
portfolio.</p>
<dl class="py method">
<dt class="sig sig-object py" id="mlfinlab.labeling.tail_sets.TailSetLabels.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bins</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vol_adj</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.labeling.tail_sets.TailSetLabels.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prices</strong> – (pd.DataFrame) Asset prices.</p></li>
<li><p><strong>n_bins</strong> – (int) Number of bins to determine the quantiles for defining the tail sets. The top and
bottom quantiles are considered to be the positive and negative tail sets, respectively.</p></li>
<li><p><strong>vol_adj</strong> – (str) Whether to take volatility adjusted returns. Allowable inputs are <code class="docutils literal notranslate"><span class="pre">None</span></code>,
<code class="docutils literal notranslate"><span class="pre">mean_abs_dev</span></code>, and <code class="docutils literal notranslate"><span class="pre">stdev</span></code>.</p></li>
<li><p><strong>window</strong> – (int) Window period used in the calculation of the volatility adjusted returns, if vol_adj is not
None. Has no impact if vol_adj is None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlfinlab.labeling.tail_sets.TailSetLabels.get_tail_sets">
<span class="sig-name descname"><span class="pre">get_tail_sets</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.labeling.tail_sets.TailSetLabels.get_tail_sets" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the tail sets (positive and negative) and then returns a tuple with 3 elements, positive set, negative
set, full matrix set.</p>
<p>The positive and negative sets are each a series of lists with the names of the securities that fall within each
set at a specific timestamp.</p>
<p>For the full matrix a value of 1 indicates the volatility adjusted returns were in the top quantile, a value of
-1 for the bottom quantile.
:return: (tuple) positive set, negative set, full matrix set.</p>
</dd></dl>

</dd></dl>

</section>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h4>
<p>Below is an example on how to create the positive, negative, and full matrix Tail Sets.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.labeling</span> <span class="kn">import</span> <span class="n">TailSetLabels</span>

<span class="c1"># Import price data</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../Sample-Data/stock_prices.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create tail set labels</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">TailSetLabels</span><span class="p">(</span><span class="n">prices</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">vol_adj</span><span class="o">=</span><span class="s1">&#39;mean_abs_dev&#39;</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">180</span><span class="p">)</span>
<span class="n">pos_set</span><span class="p">,</span> <span class="n">neg_set</span><span class="p">,</span> <span class="n">matrix_set</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">get_tail_sets</span><span class="p">()</span>

<span class="c1"># Lag the labels to make them forward looking</span>
<span class="n">pos_set</span> <span class="o">=</span> <span class="n">pos_set</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">neg_set</span> <span class="o">=</span> <span class="n">neg_set</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">matrix_set</span> <span class="o">=</span> <span class="n">matrix_set</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="research-notebooks">
<h4>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h4>
<p>The following research notebooks can be used to better understand the Tail Set labeling technique.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Labelling/Labels%20Tail%20Sets/tail_set_labels_example.ipynb">Tail Set Labels Example</a></p></li>
</ul>
</section>
</section>
<span id="document-labeling/labeling_fixed_time_horizon"></span><section id="fixed-horizon-method">
<span id="implementations-labeling-fixed-time-horizon"></span><h3>Fixed Horizon Method<a class="headerlink" href="#fixed-horizon-method" title="Permalink to this heading">¶</a></h3>
<p>Fixed horizon labels is a classification labeling technique used in the following paper: <a class="reference external" href="https://arxiv.org/abs/1603.08604">Dixon, M., Klabjan, D. and
Bang, J., 2016. Classification-based Financial Markets Prediction using Deep Neural Networks.</a></p>
<p>Fixed time horizon is a common method used in labeling financial data, usually applied on time bars. The rate of return relative
to <span class="math notranslate nohighlight">\(t_0\)</span> over time horizon <span class="math notranslate nohighlight">\(h\)</span>, assuming that returns are lagged, is calculated as follows (M.L. de Prado, Advances in Financial Machine Learning, 2018):</p>
<div class="math notranslate nohighlight">
\[r_{t0,t1} = \frac{p_{t1}}{p_{t0}} - 1\]</div>
<p>Where <span class="math notranslate nohighlight">\(t_1\)</span> is the time bar index after a fixed horizon has passed, and <span class="math notranslate nohighlight">\(p_{t0}, p_{t1}\)</span>
are prices at times <span class="math notranslate nohighlight">\(t_0, t_1\)</span>. This method assigns a label based on comparison of rate of return to a threshold <span class="math notranslate nohighlight">\(\tau\)</span></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
  L_{t0, t1} = \begin{cases}
  -1 &amp;\ \text{if} \ \ r_{t0, t1} &lt; -\tau\\
  0 &amp;\ \text{if} \ \ -\tau \leq r_{t0, t1} \leq \tau\\
  1 &amp;\ \text{if} \ \ r_{t0, t1} &gt; \tau
  \end{cases}
\end{split}
\end{equation}\end{split}\]</div>
</div></blockquote>
<p>To avoid overlapping return windows, rather than specifying <span class="math notranslate nohighlight">\(h\)</span>, the user is given the option of resampling the returns to
get the desired return period. Possible inputs for the resample period can be found <a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects">here.</a>.
Optionally, returns can be standardized by scaling by the mean and standard deviation of a rolling window. If threshold is a pd.Series,
<strong>threshold.index and prices.index must match</strong>; otherwise labels will fail to be returned. If resampling
is used, the threshold must match the index of prices after resampling. This is to avoid the user being forced to manually fill
in thresholds.</p>
<p>The following shows the distribution of labels for standardized returns on closing prices of SPY in the time period from Jan 2008 to July 2016
using a 20-day rolling window for the standard deviation.</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="_images/fixed_horizon_labels_example.png"><img alt="fixed horizon example" src="_images/fixed_horizon_labels_example.png" style="width: 432.0px; height: 288.0px;" /></a>
<figcaption>
<p><span class="caption-text">Distribution of labels on standardized returns on closing prices of SPY.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Though time bars are the most common format for financial data, there can be potential problems with over-reliance on time bars. Time
bars exhibit high seasonality, as trading behavior may be quite different at the open or close versus midday; thus it will not be
informative to apply the same threshold on a non-uniform distribution. Solutions include applying the fixed horizon method to tick or
volume bars instead of time bars, using data sampled at the same time every day (e.g. closing prices) or inputting a dynamic threshold
as a pd.Series corresponding to the timestamps in the dataset. However, the fixed horizon method will always fail to capture information
about the path of the prices [Lopez de Prado, 2018].</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><strong>Underlying Literature</strong></p>
<p>The following sources describe this method in more detail:</p>
<ul class="simple">
<li><p><strong>Advances in Financial Machine Learning, Chapter 3.2</strong> <em>by</em> Marcos Lopez de Prado (p. 43-44).</p></li>
<li><p><strong>Machine Learning for Asset Managers, Chapter 5.2</strong> <em>by</em> Marcos Lopez de Prado (p. 65-66).</p></li>
</ul>
</div>
<section id="module-mlfinlab.labeling.fixed_time_horizon">
<span id="implementation"></span><h4>Implementation<a class="headerlink" href="#module-mlfinlab.labeling.fixed_time_horizon" title="Permalink to this heading">¶</a></h4>
<p>Chapter 3.2 Fixed-Time Horizon Method, in Advances in Financial Machine Learning, by M. L. de Prado.</p>
<p>Work “Classification-based Financial Markets Prediction using Deep Neural Networks” by Dixon et al. (2016) describes how
labeling data this way can be used in training deep neural networks to predict price movements.</p>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.labeling.fixed_time_horizon.fixed_time_horizon">
<span class="sig-name descname"><span class="pre">fixed_time_horizon</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">resample_by</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lag</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">standardized</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.labeling.fixed_time_horizon.fixed_time_horizon" title="Permalink to this definition">¶</a></dt>
<dd><p>Fixed-Time Horizon Labeling Method.</p>
<p>Originally described in the book Advances in Financial Machine Learning, Chapter 3.2, p.43-44.</p>
<p>Returns 1 if return is greater than the threshold, -1 if less, and 0 if in between. If no threshold is
provided then it will simply take the sign of the return.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prices</strong> – (pd.Series or pd.DataFrame) Time-indexed stock prices used to calculate returns.</p></li>
<li><p><strong>threshold</strong> – (float or pd.Series) When the absolute value of return exceeds the threshold, the observation is
labeled with 1 or -1, depending on the sign of the return. If return is less, it’s labeled as 0.
Can be dynamic if threshold is inputted as a pd.Series, and threshold.index must match prices.index.
If resampling is used, the index of threshold must match the index of prices after resampling.
If threshold is negative, then the directionality of the labels will be reversed. If no threshold
is provided, it is assumed to be 0 and the sign of the return is returned.</p></li>
<li><p><strong>resample_by</strong> – <p>(str) If not None, the resampling period for price data prior to calculating returns. ‘B’ = per
business day, ‘W’ = week, ‘M’ = month, etc. Will take the last observation for each period.
For full details see <a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects">here.</a></p>
</p></li>
<li><p><strong>lag</strong> – (bool) If True, returns will be lagged to make them forward-looking.</p></li>
<li><p><strong>standardized</strong> – (bool) Whether returns are scaled by mean and standard deviation.</p></li>
<li><p><strong>window</strong> – (int) If standardized is True, the rolling window period for calculating the mean and standard
deviation of returns.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.Series or pd.DataFrame) -1, 0, or 1 denoting whether the return for each observation is
less/between/greater than the threshold at each corresponding time index. First or last row will be
NaN, depending on lag.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h4>
<p>Below is an example on how to use the Fixed Horizon labeling technique on real data.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">mlfinlab.labeling</span> <span class="kn">import</span> <span class="n">fixed_time_horizon</span>

<span class="c1"># Import price data.</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../Sample-Data/stock_prices.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">custom_threshold</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)),</span> <span class="n">index</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>

<span class="c1"># Create labels.</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">fixed_time_horizon</span><span class="p">(</span><span class="n">prices</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">lag</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create labels with a dynamic threshold.</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">fixed_time_horizon</span><span class="p">(</span><span class="n">prices</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="n">custom_threshold</span><span class="p">,</span> <span class="n">lag</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create labels with standardization.</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">fixed_time_horizon</span><span class="p">(</span><span class="n">prices</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">lag</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">standardized</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Create labels after resampling weekly with standardization.</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">fixed_time_horizon</span><span class="p">(</span><span class="n">prices</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">lag</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                            <span class="n">standardized</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="research-notebook">
<h4>Research Notebook<a class="headerlink" href="#research-notebook" title="Permalink to this heading">¶</a></h4>
<p>The following research notebook can be used to better understand the Fixed Horizon labeling technique.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Labelling/Labels%20Fixed%20Horizon/Fixed%20Time%20Horizon.ipynb">Fixed Horizon Example</a></p></li>
</ul>
</section>
</section>
<span id="document-labeling/labeling_matrix_flags"></span><section id="labeling-matrix-flags">
<span id="implementations-labeling-matrix-flags"></span><h3>Labeling Matrix Flags<a class="headerlink" href="#labeling-matrix-flags" title="Permalink to this heading">¶</a></h3>
<p>The matrix flag labeling method is meant to capture trends in financial prices data, such as bull and bear flags.
A bull flag occurs when a stock’s price rapidly increases, followed by a downwards trending consolidation period, followed by a breakout
increase in price confirming the original increase. As defined, “A bull flag pattern is a horizontal or downward
sloping flag of consolidation followed by a sharp rise in the positive direction, the breakout.” [Leigh et al. 2002].
Being able to identify the early stages of the breakout process can lead to a profitable strategy of buying the breakout and
then selling some number of days later, when the price has theoretically stabilized again.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><strong>Literature Reference</strong></p>
<blockquote>
<div><p>Labeling price data according to a template to identify patterns in price changes is featured in the following paper:
<a class="reference external" href="https://www.sciencedirect.com/science/article/abs/pii/S0957417415002110">Cervelló-Royo, R., Guijarro, F. and Michniuk, K., 2015. Stock market trading rule based on pattern recognition and technical
analysis: Forecasting the DJIA index with intraday data.</a></p>
<p>This method was originally introduced in <a class="reference external" href="http://chart-patterns.technicalanalysis.org.uk/LMPR02.pdf">Leigh, W., Modani, N., Purvis, R. and Roberts, T., 2002. Stock market trading rule discovery
using technical charting heuristics.</a>, which describes this method in more
detail.</p>
</div></blockquote>
</div>
<p>Cervelló-Royo et al. expand on Leigh et al.’s work by proposing a new bull flag pattern which ameliorates some weaknesses in Leigh’s original template,
such as the possibility of false positives given the path the stock took. Additionally, he applies this bull flag labeling method to intraday
candlestick data, rather than just closing prices.</p>
<p>The bull flag labeling pattern requires the use of a template to apply to the transformed price data. Below is an example bull flag
template.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="_images/bull_flag_template.png"><img alt="bull flag template" src="_images/bull_flag_template.png" style="width: 450.59999999999997px; height: 340.2px;" /></a>
<figcaption>
<p><span class="caption-text">Bull flag template, from Cervelló-Royo et al. (2015), originally proposed by Leigh et al. (2002). The first 7 columns represent
consolidation, while final 3 columns represent the early stages of the breakout.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The matrix flag labeling method, in order to find a single day’s fit, consists of the following steps:</p>
<blockquote>
<div><ul class="simple">
<li><p>The data window is taken, which consists of the current day’s price and a number of days preceding.</p></li>
<li><p>The prices in the data window are ranked and decile cutoffs are found. Each price is mapped to the corresponding decile.</p></li>
<li><p>The data window is split into 10 chronological buckets each containing a tenth of the data window.</p></li>
<li><p>Each data window bucket is converted to a column of 10 elements, with the first bucket corresponding to the leftmost column, the second
to the second from left, and so on until the last bucket corresponds to the rightmost column. Each row in a column corresponds to decile over
the entire data window, with the top row corresponding to top 90-100th percentile, second row corresponding to the 80-90th percentile, and
so on.</p></li>
<li><p>The mapping between price and decile over the data window is applied, and each row consists of the proportion of points in the bucket which
falls into each decile. For example, suppose the second bucket contains 50% points in the 20-30th percentile, and 50% points in the 0-10th
percentile. The corresponding column would be, from top to bottom, [0, 0, 0, 0, 0, 0, 0, 0.5, 0, 0.5]. This process is done for all ten
buckets, until a 10 by 10 matrix is formed.</p></li>
<li><p>This 10 by 10 matrix is multiplied element-wise by the template. The sum of all elements of this resulting matrix is calculated to be the
total fit for the day.</p></li>
<li><p>If a threshold is given, the fit is converted to a categorical label, with the positive class given if the fit equals or exceeds the
threshold, and the negative class otherwise. The value of the threshold depends on how strict of a classifier the user
desires, and the allowable values based on the template matrix.</p></li>
</ul>
</div></blockquote>
<p>The following shows the identified bull flag regions on price data in MSFT stock from 2019-2020 using the original template shown above, with
a time window of 40 days.</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="_images/msft_bull_flag40.png"><img alt="bull flag msft" src="_images/msft_bull_flag40.png" style="width: 518.4px; height: 345.59999999999997px;" /></a>
<figcaption>
<p><span class="caption-text">Bull flag template from Leigh et al. (2002) applied to MSFT data. Green dots show when the template has identified the region as a
bull flag breakout point.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The user should know what kind of timescale is desired when deciding the data window for this method. Using a small data window will catch
small, short-lived trends, while missing longer-term trends, and vice versa for a large data window. Additionally, the choice of template
determines which kind of pattern is tracked, and should be customized with respect to the data. A template which is optimal for 15 minute
bars, for example, may not work nearly as well for hourly or daily bars. The user may choose a preset template, or input their own. Currently,
the allowable pre-set templates are the bear and bull versions of the templates used in Leigh’s and Cervelló-Royo’s papers.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="_images/matrix_templates.png"><img alt="templates" src="_images/matrix_templates.png" style="width: 919.8px; height: 323.4px;" /></a>
<figcaption>
<p><span class="caption-text">Left: Leigh’s bull flag template. Right: Cervelló-Royo’s bull flag template. The bear versions can be found by flipping
the bull versions about the horizontal axis. The highest possible fit using the Leigh template is 10, and using Cervelló-Royo
is 5.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><strong>Simple Example</strong></p>
<blockquote>
<div><p>It’s perhaps easiest to illustrate this process with a simple example. Suppose we have the following data window of 20
prices [100, 102, …, 118, 120, 118, …, 104, 102] i.e. rising monotonically to 120 and then falling back to 102.
The decile cutoffs are then [102, 104, …, 120]. Note that these cutoffs are right inclusive, so a value of 102 would
fit into the (100, 102) percentile.</p>
<p>We split the data into ten chronological subsets such that the first subset is [100, 102]. 100% of elements in the first subset
fall into the lowest decile, so the corresponding column would be [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]. The second subset is [104, 106],
of which 1 out of 2 prices is in the 2nd decile, and the other is in the 3rd decile, so the column would be [0, 0, 0, 0, 0, 0, 0, 0.5, 0.5, 0].
This is done until the entire 10 by 10 matrix is generated. The columns in this matrix represent the 10 chronological subsets, and each
element in the column represents the proportion of points in each subset corresponding to its respective decile over the entire data window.</p>
<p>This matrix is then multiplied element-wise by the template, and resulting values are summed to get the total template fit. The higher
the fit, the better match to the template. Using the template shown above, the highest possible fit is 10.</p>
</div></blockquote>
</div>
<section id="module-mlfinlab.labeling.matrix_flags">
<span id="implementation"></span><h4>Implementation<a class="headerlink" href="#module-mlfinlab.labeling.matrix_flags" title="Permalink to this heading">¶</a></h4>
<p>Matrix Flag labeling method.</p>
<dl class="py class">
<dt class="sig sig-object py" id="mlfinlab.labeling.matrix_flags.MatrixFlagLabels">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">MatrixFlagLabels</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">template_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.labeling.matrix_flags.MatrixFlagLabels" title="Permalink to this definition">¶</a></dt>
<dd><p>The Matrix Flag labeling method is featured in the paper: Cervelló-Royo, R., Guijarro, F. and Michniuk, K., 2015.
Stock market trading rule based on pattern recognition and technical analysis: Forecasting the DJIA index with
intraday data.</p>
<p>The method of applying a matrix template was first introduced, and explained in greater detail, in the paper:
Leigh, W., Modani, N., Purvis, R. and Roberts, T., 2002. Stock market trading rule discovery using technical
charting heuristics.</p>
<p>Cervelló-Royo et al. expand on Leigh et al.’s work by proposing a new bull flag pattern which ameliorates some
weaknesses in Leigh’s original template. Additionally, he applies this bull flag labeling method to intraday
candlestick data, rather than just closing prices.</p>
<p>To find the total weight for a given day, the current price as well as the preceding window days number of prices is
used. The data window is split into 10 buckets each containing a chronological tenth of the data window. Each point
in 1 bucket is put into a decile corresponding to a position in a column based on percentile relative to the entire
data window. Bottom 10% on lowest row, next 10% on second lowest row etc.
The proportion of points in each decile is reported to finalize the column. The first tenth of the data is
transformed to the leftmost column, the next tenth to the next column on the right and so on until finally a 10 by
10 matrix is achieved. This matrix is then multiplied element-wise with the 10 by 10 template, and the sum of all
columns is the total weight for the day. If desired, the user can specify a threshold to determine positive and
negative classes. The value of the threshold depends on how strict of a classifier the user desires, and the
allowable values based on the template matrix.</p>
<dl class="py method">
<dt class="sig sig-object py" id="mlfinlab.labeling.matrix_flags.MatrixFlagLabels.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">template_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.labeling.matrix_flags.MatrixFlagLabels.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prices</strong> – (pd.Series) Price data for one stock.</p></li>
<li><p><strong>window</strong> – (int) Length of preceding data window used when generating the fit matrix for one day.</p></li>
<li><p><strong>template_name</strong> – (str) Name of the an available template in the template library. Allowable names:
<code class="docutils literal notranslate"><span class="pre">leigh_bear</span></code>, <code class="docutils literal notranslate"><span class="pre">leigh_bull</span></code>, <code class="docutils literal notranslate"><span class="pre">cervelloroyo_bear</span></code>, <code class="docutils literal notranslate"><span class="pre">cervellororo_bull</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlfinlab.labeling.matrix_flags.MatrixFlagLabels.apply_labeling_matrix">
<span class="sig-name descname"><span class="pre">apply_labeling_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.labeling.matrix_flags.MatrixFlagLabels.apply_labeling_matrix" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>threshold</strong> – (float) If None, labels will be returned numerically as the score for the day. If not None,
then labels are returned categorically, with the positive category for labels that are equal to
or exceed the threshold.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.Series) Total scores for the data series on each eligible day (meaning for indices self.window and
onwards).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlfinlab.labeling.matrix_flags.MatrixFlagLabels.set_template">
<span class="sig-name descname"><span class="pre">set_template</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">template</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.labeling.matrix_flags.MatrixFlagLabels.set_template" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>template</strong> – (pd.DataFrame) Template to override the default template. Must be a 10 by 10 pd.DataFrame.
NaN values not allowed, as they will not automatically be treated as zeros.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h4>
<p>Below is an example on how to use the matrix flags labeling method.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">yfinance</span> <span class="k">as</span> <span class="nn">yf</span>
<span class="kn">from</span> <span class="nn">mlfinlab.labeling.matrix_flags</span> <span class="kn">import</span> <span class="n">MatrixFlagLabels</span>

<span class="c1"># Import price data</span>
<span class="n">msft</span> <span class="o">=</span> <span class="n">yf</span><span class="o">.</span><span class="n">Ticker</span><span class="p">(</span><span class="s2">&quot;MSFT&quot;</span><span class="p">)</span>
<span class="n">hist</span> <span class="o">=</span> <span class="n">msft</span><span class="o">.</span><span class="n">history</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="s1">&#39;2020-1-1&#39;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;2020-6-01&#39;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">hist</span><span class="p">[</span><span class="s1">&#39;Close&#39;</span><span class="p">]</span>

<span class="c1"># Initialize with a window of 60 days.</span>
<span class="n">Flags</span> <span class="o">=</span> <span class="n">MatrixFlagLabels</span><span class="p">(</span><span class="n">prices</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">template_name</span><span class="o">=</span><span class="s1">&#39;leigh_bull&#39;</span><span class="p">)</span>

<span class="c1"># Get numerical weights based on the template (for days 60 and onwards).</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">Flags</span><span class="o">.</span><span class="n">apply_labeling_matrix</span><span class="p">()</span>

<span class="c1"># Get categorical labels based on whether the day&#39;s weight is above 2.5.</span>
<span class="n">categorical</span> <span class="o">=</span> <span class="n">Flags</span><span class="o">.</span><span class="n">apply_labeling_matrix</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>

<span class="c1"># Change the template from pre-set to user defined.</span>
<span class="n">new_template</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)))</span>
<span class="n">Flags</span><span class="o">.</span><span class="n">set_template</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="n">new_template</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="research-notebook">
<h4>Research Notebook<a class="headerlink" href="#research-notebook" title="Permalink to this heading">¶</a></h4>
<p>The following research notebook can be used to better understand the matrix flags labeling technique.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Labelling/Labels%20Matrix%20Flags/Matrix%20Flag%20Labels.ipynb">Matrix Flags Example</a></p></li>
</ul>
</section>
</section>
<span id="document-labeling/labeling_excess_median"></span><section id="excess-over-median">
<span id="implementations-labeling-excess-median"></span><h3>Excess Over Median<a class="headerlink" href="#excess-over-median" title="Permalink to this heading">¶</a></h3>
<p>Labeling according to excess over median is a labeling method used in the following paper <a class="reference external" href="https://link.springer.com/article/10.1057/jam.2012.17">Zhu, M., Philpotts, F. and
Stevenson, M., 2012. The benefits of tree-based models for stock selection.
Journal of Asset Management, 13(6), pp.437-448.</a></p>
<p>In this method, a cross-sectional dataset of close prices of many different stocks are used, which is converted to
returns. The median return at each time index is calculated and used as a proxy for market return. The median return is
then subtracted from each observation’s return to find the numerical excess return over median. If desired, the
numerical values can be converted to categorical values according to the sign of the excess return. The labels can then
be used in training regression and classification models.</p>
<p>At time <span class="math notranslate nohighlight">\(t\)</span>:</p>
<div class="math notranslate nohighlight">
\begin{gather*}
P_t = \{p_{t,0}, p_{t,1}, \dots, p_{t,n}\} \\

R_t = \{r_{t,0}, r_{t,1}, \dots, r_{t,n}\} \\

m_t = median(R_t) \\

L(R_t) = \{r_{t,0} - m_t, r_{t,1} - m_t, \dots, r_{t,n} - m_t\}
\end{gather*}</div><p>If categorical rather than numerical labels are desired:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
  L(r_{t,n}) = \begin{cases}
  -1 &amp;\ \text{if} \ \ r_{t,n} - m_t &lt; 0\\
  0 &amp;\ \text{if} \ \ r_{t,n} - m_t = 0\\
  1 &amp;\ \text{if} \ \ r_{t,n} - m_t &gt; 0\\
  \end{cases}
\end{split}
\end{equation}\end{split}\]</div>
<p>If desired, the user can specify a resampling period to apply to the price data prior to calculating returns. The user can
also lag the returns to make them forward-looking. In the paper by Zhu et al., the authors use monthly forward-looking labels.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="_images/distribution_over_median_monthly_forward.png"><img alt="Distribution Over Median" src="_images/distribution_over_median_monthly_forward.png" style="width: 432.0px; height: 288.0px;" /></a>
<figcaption>
<p><span class="caption-text">Distribution of monthly forward stock returns. This is the labeling method used in the paper by Zhu et al.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<section id="module-mlfinlab.labeling.excess_over_median">
<span id="implementation"></span><h4>Implementation<a class="headerlink" href="#module-mlfinlab.labeling.excess_over_median" title="Permalink to this heading">¶</a></h4>
<p>Return in excess of median method.</p>
<p>Described in “The benefits of tree-based models for stock selection”, Zhu et al. (2012). Data labeled this way can be
used in regression and classification models to predict stock returns over market.</p>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.labeling.excess_over_median.excess_over_median">
<span class="sig-name descname"><span class="pre">excess_over_median</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">binary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">resample_by</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lag</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.labeling.excess_over_median.excess_over_median" title="Permalink to this definition">¶</a></dt>
<dd><p>Return in excess of median labeling method. Sourced from “The benefits of tree-based models for stock selection”
Zhu et al. (2012).</p>
<p>Returns a DataFrame containing returns of stocks over the median of all stocks in the portfolio, or returns a
DataFrame containing signs of those returns. In the latter case, an observation may be labeled as 0 if it itself is
the median.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prices</strong> – (pd.DataFrame) Close prices of all stocks in the market that are used to establish the median.
Returns on each stock are then compared to the median for the given timestamp.</p></li>
<li><p><strong>binary</strong> – (bool) If False, the numerical value of excess returns over median will be given. If True, then only
the sign of the excess return over median will be given (-1 or 1). A label of 0 will be given if
the observation itself is the median. According to Zhu et al., categorical labels can alleviate
issues with extreme outliers present with numerical labels.</p></li>
<li><p><strong>resample_by</strong> – (str) If not None, the resampling period for price data prior to calculating returns. ‘B’ = per
business day, ‘W’ = week, ‘M’ = month, etc. Will take the last observation for each period.
For full details see <a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects">here.</a></p></li>
<li><p><strong>lag</strong> – (bool) If True, returns will be lagged to make them forward-looking.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) Numerical returns in excess of the market median return, or sign of return depending on
whether binary is False or True respectively.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h4>
<p>Below is an example on how to create labels of excess over median from real data.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">yfinance</span> <span class="k">as</span> <span class="nn">yf</span>
<span class="kn">from</span> <span class="nn">mlfinlab.labeling</span> <span class="kn">import</span> <span class="n">excess_over_median</span>

<span class="c1"># Import price data</span>
<span class="n">tickers</span> <span class="o">=</span> <span class="s2">&quot;AAPL MSFT AMZN GOOG&quot;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">yf</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="n">tickers</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="s2">&quot;2019-01-01&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;2020-05-01&quot;</span><span class="p">,</span> <span class="n">group_by</span><span class="o">=</span><span class="s2">&quot;ticker&quot;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="s1">&#39;Adj Close&#39;</span><span class="p">)]</span>
<span class="n">data</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">droplevel</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Get returns over median numerically</span>
<span class="n">numerical</span> <span class="o">=</span> <span class="n">excess_over_median</span><span class="p">(</span><span class="n">prices</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lag</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Get returns over median as a categorical label</span>
<span class="n">binary</span> <span class="o">=</span> <span class="n">excess_over_median</span><span class="p">((</span><span class="n">prices</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lag</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Get monthly forward-looking returns</span>
<span class="n">monthly_forward</span> <span class="o">=</span> <span class="n">excess_over_median</span><span class="p">((</span><span class="n">prices</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;M&#39;</span><span class="p">,</span> <span class="n">lag</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="research-notebooks">
<h4>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h4>
<p>The following research notebooks can be used to better understand labeling excess over median.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Labelling/Labels%20Excess%20Over%20Median/Excess%20Over%20Median.ipynb">Excess Over Median Example</a></p></li>
</ul>
</section>
</section>
<span id="document-labeling/labeling_raw_return"></span><section id="raw-returns">
<span id="implementations-labeling-raw-return"></span><h3>Raw Returns<a class="headerlink" href="#raw-returns" title="Permalink to this heading">¶</a></h3>
<p>Labeling data by raw returns is the most simple and basic method of labeling financial data for machine learning. Raw returns can
be calculated either on a simple or logarithmic basis. Using returns rather than prices is usually preferred for financial time series
data because returns are usually stationary, unlike prices. This means that returns across different assets, or the same asset
at different times, can be directly compared with each other. The same cannot be said of price differences, since the magnitude of the
price change is highly dependent on the starting price, which varies with time.</p>
<p>The simple return for an observation with
price <span class="math notranslate nohighlight">\(p_t\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> relative to its price at time <span class="math notranslate nohighlight">\(t-1\)</span> is as follows:</p>
<div class="math notranslate nohighlight">
\[r_t = \frac{p_{t}}{p_{t-1}} - 1\]</div>
<p>And the logarithmic return is:</p>
<div class="math notranslate nohighlight">
\[r_t = log(p_t) - log(p_{t-1})\]</div>
<p>The label <span class="math notranslate nohighlight">\(L_t\)</span> is simply equal to <span class="math notranslate nohighlight">\(r_t\)</span>, or to the sign of <span class="math notranslate nohighlight">\(r_t\)</span>, if binary labeling is desired.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
  L_{t} = \begin{cases}
  -1 &amp;\ \text{if} \ \ r_t &lt; 0\\
  0 &amp;\ \text{if} \ \ r_t = 0\\
  1 &amp;\ \text{if} \ \ r_t &gt; 0
  \end{cases}
\end{split}
\end{equation}\end{split}\]</div>
</div></blockquote>
<p>If desired, the user can specify a resampling period to apply to the price data prior to calculating returns. The user
can also lag the returns to make them forward-looking.</p>
<p>The following shows the distribution of logarithmic daily returns on Microsoft stock during the time period between January
2010 and May 2020.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="_images/Raw_returns_distribution.png"><img alt="raw returns image" src="_images/Raw_returns_distribution.png" style="width: 388.8px; height: 259.2px;" /></a>
<figcaption>
<p><span class="caption-text">Distribution of logarithmic returns on MSFT.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<section id="module-mlfinlab.labeling.raw_return">
<span id="implementation"></span><h4>Implementation<a class="headerlink" href="#module-mlfinlab.labeling.raw_return" title="Permalink to this heading">¶</a></h4>
<p>Labeling Raw Returns.</p>
<p>Most basic form of labeling based on raw return of each observation relative to its previous value.</p>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.labeling.raw_return.raw_return">
<span class="sig-name descname"><span class="pre">raw_return</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">binary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logarithmic</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">resample_by</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lag</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.labeling.raw_return.raw_return" title="Permalink to this definition">¶</a></dt>
<dd><p>Raw returns labeling method.</p>
<p>This is the most basic and ubiquitous labeling method used as a precursor to almost any kind of financial data
analysis or machine learning. User can specify simple or logarithmic returns, numerical or binary labels, a
resample period, and whether returns are lagged to be forward looking.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prices</strong> – (pd.Series or pd.DataFrame) Time-indexed price data on stocks with which to calculate return.</p></li>
<li><p><strong>binary</strong> – (bool) If False, will return numerical returns. If True, will return the sign of the raw return.</p></li>
<li><p><strong>logarithmic</strong> – (bool) If False, will calculate simple returns. If True, will calculate logarithmic returns.</p></li>
<li><p><strong>resample_by</strong> – (str) If not None, the resampling period for price data prior to calculating returns. ‘B’ = per
business day, ‘W’ = week, ‘M’ = month, etc. Will take the last observation for each period.
For full details see <a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects">here.</a></p></li>
<li><p><strong>lag</strong> – (bool) If True, returns will be lagged to make them forward-looking.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.Series or pd.DataFrame) Raw returns on market data. User can specify whether returns will be based on
simple or logarithmic return, and whether the output will be numerical or categorical.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h4>
<p>Below is an example on how to use the raw returns labeling method.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.labeling</span> <span class="kn">import</span> <span class="n">raw_return</span>

<span class="c1"># Import price data</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../Sample-Data/stock_prices.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create labels numerically based on simple returns</span>
<span class="n">returns</span> <span class="o">=</span> <span class="n">raw_returns</span><span class="p">(</span><span class="n">prices</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">lag</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create labels categorically based on logarithmic returns</span>
<span class="n">returns</span> <span class="o">=</span> <span class="n">raw_returns</span><span class="p">(</span><span class="n">prices</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">logarithmic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">lag</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create labels categorically on weekly data with forward looking log returns.</span>
<span class="n">returns</span> <span class="o">=</span> <span class="n">raw_returns</span><span class="p">(</span><span class="n">prices</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">logarithmic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">lag</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="research-notebook">
<h4>Research Notebook<a class="headerlink" href="#research-notebook" title="Permalink to this heading">¶</a></h4>
<p>The following research notebook can be used to better understand the raw return labeling technique.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Labelling/Labels%20Raw%20Return/Raw%20Return.ipynb">Raw Return Example</a></p></li>
</ul>
</section>
</section>
<span id="document-labeling/labeling_vs_benchmark"></span><section id="return-versus-benchmark">
<span id="implementations-labeling-vs-benchmark"></span><h3>Return Versus Benchmark<a class="headerlink" href="#return-versus-benchmark" title="Permalink to this heading">¶</a></h3>
<p>Labeling versus benchmark is featured in the paper <a class="reference external" href="https://www.sciencedirect.com/science/article/abs/pii/S0957417415003334">Evaluating multiple classifiers for stock price direction prediction, by Ballings et al.,
2015.</a> In this paper, the authors label yearly forward
stock returns against a predetermined benchmark, and use that labeled data to compare the performance of several machine
learning algorithms in predicting long term price movements.</p>
<p>Labeling against benchmark is a simple method of labeling financial data in which time-indexed returns are labeled according to
whether they exceed a set value. The benchmark can be either a constant value, or a pd.Series of values with an index matching
that of the returns. The labels can be the numerical value of how much each observation’s return exceeds the benchmark, or the sign
of the excess.</p>
<p>At time <span class="math notranslate nohighlight">\(t\)</span>, given that price of a stock is <span class="math notranslate nohighlight">\(p_{t, n}\)</span>, benchmark is <span class="math notranslate nohighlight">\(B_t\)</span> and return is:</p>
<div class="math notranslate nohighlight">
\[r_{t,n} = \frac{p_{t,n}}{p_{t-1,n}} - 1\]</div>
<p>Note that <span class="math notranslate nohighlight">\(B_t\)</span> is a scalar value corresponding to the benchmark at time <span class="math notranslate nohighlight">\(t\)</span>, while <span class="math notranslate nohighlight">\(B\)</span> is the vector of all benchmarks
across all timestamps. The labels are:</p>
<div class="math notranslate nohighlight">
\[L(r_{t,n}) = r_{t,n} - B_t\]</div>
<p>If categorical labels are desired:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
  L(r_{t, n}) = \begin{cases}
  -1 &amp;\ \text{if} \ \ r_{t,n} &lt; B_t\\
  0 &amp;\ \text{if} \ \ r_{t,n} = B_t\\
  1 &amp;\ \text{if} \ \ r_{t,n} &gt; B_t\\
  \end{cases}
\end{split}
\end{equation}\end{split}\]</div>
</div></blockquote>
<p>The simplest method of labeling is just returning the sign of the return. However, sometimes it is desirable to quantify the return
compared to a benchmark to better contextualize the returns. This is commonly done by using the mean or median of multiple stocks in the market.
However, that data may not always be available, and sometimes the user might wish a specify a constant or more custom benchmark to compare
returns against. Note that these benchmarks are unidirectional only. If the user would like a benchmark that captures the absolute value of the
returns, then the fixed horizon method should be used instead.</p>
<p>If desired, the user can specify a <a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects">resampling period</a>
to apply to the price data prior to calculating returns. The user can also lag the returns to make them forward-looking.
In the paper by Ballings et al., the authors use yearly forward returns, and compare them to benchmark values
of 15%, 25%, and 35%.</p>
<p>The following shows the returns for MSFT stock during March-April 2020, compared to the return of SPY as a benchmark during
the same time period. Green dots represent days when MSFT outperformed SPY, and red dots represent days when MSFT underperformed
SPY.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="_images/MSFT_Return_vs_Benchmark.png"><img alt="labeling vs benchmark" src="_images/MSFT_Return_vs_Benchmark.png" style="width: 388.8px; height: 259.2px;" /></a>
<figcaption>
<p><span class="caption-text">Comparison of MSFT return to SPY return.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><strong>Underlying Literature</strong></p>
<p>This labeling method is sourced from the following:
- Chapter 5.5.1 of <strong>Machine Learning for Factor Investing</strong>, <em>by</em> Coqueret, G. and Guida, T. (2020).</p>
</div>
<section id="module-mlfinlab.labeling.return_vs_benchmark">
<span id="implementation"></span><h4>Implementation<a class="headerlink" href="#module-mlfinlab.labeling.return_vs_benchmark" title="Permalink to this heading">¶</a></h4>
<p>Return in excess of a given benchmark.</p>
<p>Chapter 5, Machine Learning for Factor Investing, by Coqueret and Guida, (2020).</p>
<p>Work “Evaluating multiple classifiers for stock price direction prediction” by Ballings et al. (2015) uses this method
to label yearly returns over a predetermined value to compare the performance of several machine learning algorithms.</p>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.labeling.return_vs_benchmark.return_over_benchmark">
<span class="sig-name descname"><span class="pre">return_over_benchmark</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">benchmark</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">binary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">resample_by</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lag</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.labeling.return_vs_benchmark.return_over_benchmark" title="Permalink to this definition">¶</a></dt>
<dd><p>Return over benchmark labeling method. Sourced from Chapter 5.5.1 of Machine Learning for Factor Investing,
by Coqueret, G. and Guida, T. (2020).</p>
<p>Returns a Series or DataFrame of numerical or categorical returns over a given benchmark. The time index of the
benchmark must match those of the price observations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prices</strong> – (pd.Series or pd.DataFrame) Time indexed prices to compare returns against a benchmark.</p></li>
<li><p><strong>benchmark</strong> – (pd.Series or float) Benchmark of returns to compare the returns from prices against for labeling.
Can be a constant value, or a Series matching the index of prices. If no benchmark is given, then it
is assumed to have a constant value of 0.</p></li>
<li><p><strong>binary</strong> – (bool) If False, labels are given by their numerical value of return over benchmark. If True,
labels are given according to the sign of their excess return.</p></li>
<li><p><strong>resample_by</strong> – (str) If not None, the resampling period for price data prior to calculating returns. ‘B’ = per
business day, ‘W’ = week, ‘M’ = month, etc. Will take the last observation for each period.
For full details see <a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects">here.</a></p></li>
<li><p><strong>lag</strong> – (bool) If True, returns will be lagged to make them forward-looking.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.Series or pd.DataFrame) Excess returns over benchmark. If binary, the labels are -1 if the
return is below the benchmark, 1 if above, and 0 if it exactly matches the benchmark.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h4>
<p>Below is an example on how to use the return over benchmark labeling technique on real data.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.labeling</span> <span class="kn">import</span> <span class="n">return_vs_benchmark</span>

<span class="c1"># Import price data.</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../Sample-Data/stock_prices.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Get returns in SPY to be used as a benchmark.</span>
<span class="n">spy_returns</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;SPY&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">pct_change</span><span class="p">()</span>

<span class="c1"># Create labels using SPY as a benchmark.</span>
<span class="n">numerical_labels</span> <span class="o">=</span> <span class="n">return_vs_benchmark</span><span class="p">(</span><span class="n">prices</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">benchmark</span><span class="o">=</span><span class="n">spy_returns</span><span class="p">)</span>

<span class="c1"># Create labels categorically.</span>
<span class="n">binary_labels</span> <span class="o">=</span> <span class="n">return_vs_benchmark</span><span class="p">(</span><span class="n">prices</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">benchmark</span><span class="o">=</span><span class="n">spy_returns</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Label yearly forward returns, with the benchmark being an 25% increase in price.</span>
<span class="n">yearly_labels</span> <span class="o">=</span> <span class="n">return_vs_benchmark</span><span class="p">(</span><span class="n">prices</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">benchmark</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span><span class="p">,</span>
                                    <span class="n">lag</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="research-notebook">
<h4>Research Notebook<a class="headerlink" href="#research-notebook" title="Permalink to this heading">¶</a></h4>
<p>The following research notebook can be used to better understand the return against benchmark labeling technique.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Labelling/Labeling%20vs%20Benchmark/Labeling%20vs%20Benchmark.ipynb">Return Over Benchmark Example</a></p></li>
</ul>
</section>
</section>
<span id="document-labeling/labeling_excess_mean"></span><section id="excess-over-mean">
<span id="implementations-labeling-excess-mean"></span><h3>Excess Over Mean<a class="headerlink" href="#excess-over-mean" title="Permalink to this heading">¶</a></h3>
<p>Using cross-sectional data on returns of many different stocks, each observation is labeled according to whether, or how much,
its return exceeds the mean return. It is a common practice to label observations based on whether the return is positive or negative.
However, this may produce unbalanced classes, as during market booms the probability of a positive return is much higher, and
during market crashes they are lower (Coqueret and Guida, 2020). Labeling according to a benchmark such as mean market return
alleviates this issue.</p>
<p>A dataframe containing forward returns is calculated from close prices. The mean return of all stocks at time <span class="math notranslate nohighlight">\(t\)</span>  in the
dataframe is used to represent the market return, and excess returns are calculated by subtracting the mean return from each stock’s return
over the time period <span class="math notranslate nohighlight">\(t\)</span>. The numerical returns can then be used as-is (for regression analysis), or can be relabeled to
represent their sign (for classification analysis).</p>
<p>At time <span class="math notranslate nohighlight">\(t\)</span>:</p>
<div class="math notranslate nohighlight">
\begin{gather*}
P_t = \{p_{t,0}, p_{t,1}, ..., p_{t,n}\} \\

R_t = \{r_{t,0}, r_{t,1}, ..., r_{t,n}\} \\

\mu_t = mean(R_t) \\

L(R_t) = \{r_{t,0} - \mu_t, r_{t,1} - \mu_t, ..., r_{t,n} - \mu_t\}
\end{gather*}</div><p>If categorical rather than numerical labels are desired:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
  L(r_{t,n}) = \begin{cases}
  -1 &amp;\ \text{if} \ \ r_{t,n} - \mu_t &lt; 0\\
  0 &amp;\ \text{if} \ \ r_{t,n} - \mu_t = 0\\
  1 &amp;\ \text{if} \ \ r_{t,n} - \mu_t &gt; 0\\
  \end{cases}
\end{split}
\end{equation}\end{split}\]</div>
<p>If desired, the user can specify a <a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects">resampling period</a>
to apply to the price data prior to calculating returns. The user can also lag the returns to make them forward-looking.</p>
<p>The following shows the distribution of numerical excess over mean for a set of 20 stocks for the time period between Jan 2019
and May 2020.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="_images/distribution_over_mean.png"><img alt="labeling over mean" src="_images/distribution_over_mean.png" style="width: 432.0px; height: 288.0px;" /></a>
<figcaption>
<p><span class="caption-text">Distribution of returns over mean for 20 stocks.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><strong>Underlying Literature</strong></p>
<blockquote>
<div><p>Labeling according to excess over mean is a labeling method mentioned in Chapter 5.5.1 of the book
Machine Learning For Factor Investing, by Coqueret, G. and Guida, T., 2020.</p>
</div></blockquote>
</div>
<section id="module-mlfinlab.labeling.excess_over_mean">
<span id="implementation"></span><h4>Implementation<a class="headerlink" href="#module-mlfinlab.labeling.excess_over_mean" title="Permalink to this heading">¶</a></h4>
<p>Return in excess of mean method.</p>
<p>Chapter 5, Machine Learning for Factor Investing, by Coqueret and Guida, (2020).</p>
<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.labeling.excess_over_mean.excess_over_mean">
<span class="sig-name descname"><span class="pre">excess_over_mean</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">binary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">resample_by</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lag</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.labeling.excess_over_mean.excess_over_mean" title="Permalink to this definition">¶</a></dt>
<dd><p>Return in excess of mean labeling method. Sourced from Chapter 5.5.1 of Machine Learning for Factor Investing,
by Coqueret, G. and Guida, T. (2020).</p>
<p>Returns a DataFrame containing returns of stocks over the mean of all stocks in the portfolio. Returns a DataFrame
of signs of the returns if binary is True. In this case, an observation may be labeled as 0 if it itself is the
mean.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prices</strong> – (pd.DataFrame) Close prices of all tickers in the market that are used to establish the mean. NaN
values are ok. Returns on each ticker are then compared to the mean for the given timestamp.</p></li>
<li><p><strong>binary</strong> – (bool) If False, the numerical value of excess returns over mean will be given. If True, then only
the sign of the excess return over mean will be given (-1 or 1). A label of 0 will be given if
the observation itself equal to the mean.</p></li>
<li><p><strong>resample_by</strong> – (str) If not None, the resampling period for price data prior to calculating returns. ‘B’ = per
business day, ‘W’ = week, ‘M’ = month, etc. Will take the last observation for each period.
For full details see <a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects">here.</a></p></li>
<li><p><strong>lag</strong> – (bool) If True, returns will be lagged to make them forward-looking.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.DataFrame) Numerical returns in excess of the market mean return, or sign of return depending on
whether binary is False or True respectively.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h4>
<p>Below is an example on how to create labels of excess over mean.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">yfinance</span> <span class="k">as</span> <span class="nn">yf</span>
<span class="kn">from</span> <span class="nn">mlfinlab.labeling</span> <span class="kn">import</span> <span class="n">excess_over_mean</span>

<span class="c1"># Import price data.</span>
<span class="n">tickers</span> <span class="o">=</span> <span class="s2">&quot;AAPL MSFT AMZN GOOG&quot;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">yf</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="n">tickers</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="s2">&quot;2019-01-01&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;2020-05-01&quot;</span><span class="p">,</span> <span class="n">group_by</span><span class="o">=</span><span class="s2">&quot;ticker&quot;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="s1">&#39;Adj Close&#39;</span><span class="p">)]</span>
<span class="n">data</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">droplevel</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Get returns over mean numerically.</span>
<span class="n">numerical</span> <span class="o">=</span> <span class="n">excess_over_mean</span><span class="p">(</span><span class="n">prices</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">lag</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Get returns over mean as a categorical label.</span>
<span class="n">categorical</span> <span class="o">=</span> <span class="n">excess_over_mean</span><span class="p">(</span><span class="n">prices</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">lag</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Get categorical forward looking monthly labels.</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">excess_over_mean</span><span class="p">(</span><span class="n">prices</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;M&#39;</span><span class="p">,</span> <span class="n">lag</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="research-notebooks">
<h4>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h4>
<p>The following research notebooks can be used to better understand labeling excess over mean.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Labelling/Labels%20Excess%20Over%20Mean/excess_over_mean.ipynb">Excess Over Mean Example</a></p></li>
</ul>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-implementations/sampling"></span><section id="sampling">
<span id="implementations-sampling"></span><h3>Sampling<a class="headerlink" href="#sampling" title="Permalink to this heading">¶</a></h3>
<p>In financial machine learning, samples are not independent. For the most part, traditional machine learning algorithms
assume that samples are IID, in the case of financial machine learning samples are neither identically distributed nor
independent. In this section we will tackle the problem of samples dependency.</p>
<p>As you will remember, we mostly label our data sets using the triple-barrier method. Each label in triple-barrier event
has a label index and a label end time (t1) which corresponds to time when one of barriers were touched.</p>
<section id="sample-uniqueness">
<h4>Sample Uniqueness<a class="headerlink" href="#sample-uniqueness" title="Permalink to this heading">¶</a></h4>
<p>Let’s look at an example of 3 samples: A, B, C.</p>
<p>Imagine that:</p>
<ul class="simple">
<li><p>A was generated at <span class="math notranslate nohighlight">\(t_1\)</span> and triggered on <span class="math notranslate nohighlight">\(t_8\)</span></p></li>
<li><p>B was generated at <span class="math notranslate nohighlight">\(t_3\)</span> and triggered on <span class="math notranslate nohighlight">\(t_6\)</span></p></li>
<li><p>C was generated on <span class="math notranslate nohighlight">\(t_7\)</span> and triggered on <span class="math notranslate nohighlight">\(t_9\)</span></p></li>
</ul>
<p>In this case we see that A used information about returns on <span class="math notranslate nohighlight">\([t_1,t_8]\)</span> to generate label-endtime which overlaps
with <span class="math notranslate nohighlight">\([t_3, t_6]\)</span> which was used by B, however C didn’t use any returns information which was used by to label
other samples. Here we would like to introduce the concept of concurrency.</p>
<p>We say that labels <span class="math notranslate nohighlight">\(y_i\)</span> and <span class="math notranslate nohighlight">\(y_j\)</span> are concurrent at <span class="math notranslate nohighlight">\(t\)</span> if they are a function of at least one
common return at <span class="math notranslate nohighlight">\(r_{t-1,t}\)</span></p>
<p>In terms of concurrency label C is the most ‘pure’ as it doesn’t use any piece of information from other labels, while
A is the ‘dirtiest’ as it uses information from both B and C. By understanding average label uniqueness you can measure
how ‘pure’ your dataset is based on concurrency of labels. We can measure average label uniqueness using
get_av_uniqueness_from_triple_barrier function from the mlfinlab package.</p>
<p>This function is the orchestrator to derive average sample uniqueness from a dateset labeled by the triple barrier method.</p>
<p>An example of calculating average uniqueness given that we have already have our barrier events can be seen below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mlfinlab.sampling.concurrent</span> <span class="kn">import</span> <span class="n">get_av_uniqueness_from_triple_barrier</span>

<span class="n">barrier_events</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">close_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>

<span class="n">av_unique</span> <span class="o">=</span> <span class="n">get_av_uniqueness_from_triple_barrier</span><span class="p">(</span><span class="n">barrier_events</span><span class="p">,</span> <span class="n">close_prices</span><span class="o">.</span><span class="n">close</span><span class="p">,</span>
                                                  <span class="n">num_threads</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>We would like to build our model in such a way that it takes into account label concurrency (overlapping samples).
In order to do that we need to look at the bootstrapping algorithm of a Random Forest.</p>
</section>
<section id="sequential-bootstrapping">
<h4>Sequential Bootstrapping<a class="headerlink" href="#sequential-bootstrapping" title="Permalink to this heading">¶</a></h4>
<p>The key power of ensemble learning techniques is bagging (which is bootstrapping with replacement). The key idea behind
bagging is to randomly choose samples for each decision tree. In this case trees become diverse and by averaging predictions
of diverse trees built on randomly selected samples and random subset of features data scientists make the algorithm much
less prone to overfit.</p>
<p>However, in our case we would not only like to randomly choose samples but also choose samples which are unique and non-concurrent.
But how can we solve this problem? Here comes Sequential Bootstrapping algorithm.</p>
<p>The key idea behind Sequential Bootstrapping is to select samples in such a way that on each iteration we maximize average
uniqueness of selected subsamples.</p>
<section id="implementation">
<h5>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h5>
<p>The core functions behind Sequential Bootstrapping are implemented in mlfinlab and can be seen below:</p>
</section>
<section id="example">
<h5>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h5>
<p>An example of Sequential Bootstrap using a a toy example from the book can be seen below.</p>
<p>Consider a set of labels <span class="math notranslate nohighlight">\(\left\{y_i\right\}_{i=0,1,2}\)</span> where:</p>
<ul class="simple">
<li><p>label <span class="math notranslate nohighlight">\(y_0\)</span> is a function of return <span class="math notranslate nohighlight">\(r_{0,2}\)</span></p></li>
<li><p>label <span class="math notranslate nohighlight">\(y_1\)</span> is a function of return <span class="math notranslate nohighlight">\(r_{2,3}\)</span></p></li>
<li><p>label <span class="math notranslate nohighlight">\(y_2\)</span> is a function of return <span class="math notranslate nohighlight">\(r_{4,5}\)</span></p></li>
</ul>
<p>The first thing we need to do is to build and indicator matrix. Columns of this matrix correspond to samples and rows
correspond to price returns timestamps which were used during samples labelling. In our case indicator matrix is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ind_mat</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="n">ind_mat</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">ind_mat</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">ind_mat</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<p>One can use get_ind_matrix method from mlfinlab to build indicator matrix from triple-barrier events.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">triple_barrier_ind_mat</span> <span class="o">=</span> <span class="n">get_ind_matrix</span><span class="p">(</span><span class="n">barrier_events</span><span class="p">)</span>
</pre></div>
</div>
<p>We can get average label uniqueness on indicator matrix using get_ind_mat_average_uniqueness function from mlfinlab.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ind_mat_uniqueness</span> <span class="o">=</span> <span class="n">get_ind_mat_average_uniqueness</span><span class="p">(</span><span class="n">triple_barrier_ind_mat</span><span class="p">)</span>
</pre></div>
</div>
<p>Let’s get the first sample average uniqueness (we need to filter out zeros to get unbiased result).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">first_sample</span> <span class="o">=</span> <span class="n">ind_mat_uniqueness</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">first_sample</span><span class="p">[</span><span class="n">first_sample</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="o">&gt;&gt;</span> <span class="mf">0.26886446886446885</span>

<span class="n">av_unique</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="o">&gt;&gt;</span> <span class="n">tW</span>    <span class="mf">0.238776</span>
</pre></div>
</div>
<p>As you can see it is quite close to values generated by <strong>get_av_uniqueness_from_triple_barrier</strong> function call.</p>
<p>Let’s move back to our example. In Sequential Bootstrapping algorithm we start with an empty array of samples
(<span class="math notranslate nohighlight">\(\phi\)</span>) and loop through all samples to get the probability of chosing the sample based on average uniqueness of
reduced indicator matrix constructed from [previously chosen columns] + sample.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">phi</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">while</span> <span class="n">length</span><span class="p">(</span><span class="n">phi</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">number</span> <span class="n">of</span> <span class="n">samples</span> <span class="n">to</span> <span class="n">bootstrap</span><span class="p">:</span>
    <span class="n">average_uniqueness_array</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">:</span>
        <span class="n">previous_columns</span>  <span class="o">=</span> <span class="n">phi</span>
        <span class="n">ind_mat_reduced</span> <span class="o">=</span> <span class="n">ind_mat</span><span class="p">[</span><span class="n">previous_columns</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span>
        <span class="n">average_uniqueness_array</span><span class="p">[</span><span class="n">sample</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_ind_mat_average_uniqueness</span><span class="p">(</span><span class="n">ind_mat_reduced</span><span class="p">)</span>

    <span class="c1"># Normalise so that probabilities sum up to 1</span>
    <span class="n">probability_array</span> <span class="o">=</span> <span class="n">average_uniqueness_array</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">average_uniqueness_array</span><span class="p">)</span>
    <span class="n">chosen_sample</span> <span class="o">=</span> <span class="n">random_choice</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">probability</span> <span class="o">=</span> <span class="n">probability_array</span><span class="p">)</span>
    <span class="n">phi</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">chosen_sample</span><span class="p">)</span>
</pre></div>
</div>
<p>For performance increase we optimized and parallesied for-loop using numba, which corresponds to bootstrap_loop_run function.</p>
<p>Now let’s finish the example:</p>
<p>To be as close to the mlfinlab implementation let’s convert ind_mat to numpy matrix</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ind_mat</span> <span class="o">=</span> <span class="n">ind_mat</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
<p><strong>1st Iteration:</strong></p>
<p>On the first step all labels will have equal probalities as average uniqueness of matrix with 1 column is 1. Say we have chosen 1 on the first step</p>
<p><strong>2nd Iteration</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">phi</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># Sample chosen from the 2st step</span>
<span class="n">uniqueness_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">):</span>
    <span class="n">ind_mat_reduced</span> <span class="o">=</span> <span class="n">ind_mat</span><span class="p">[:,</span> <span class="n">phi</span> <span class="o">+</span> <span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    <span class="n">label_uniqueness</span> <span class="o">=</span> <span class="n">get_ind_mat_average_uniqueness</span><span class="p">(</span><span class="n">ind_mat_reduced</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># The last value corresponds to appended i</span>
    <span class="n">uniqueness_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">label_uniqueness</span><span class="p">[</span><span class="n">label_uniqueness</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="n">prob_array</span> <span class="o">=</span> <span class="n">uniqueness_array</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">uniqueness_array</span><span class="p">)</span>

<span class="n">prob_array</span>
<span class="o">&gt;&gt;</span> <span class="n">array</span><span class="p">([</span><span class="mf">0.35714285714285715</span><span class="p">,</span> <span class="mf">0.21428571428571427</span><span class="p">,</span> <span class="mf">0.42857142857142855</span><span class="p">],</span>
  <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
</pre></div>
</div>
<p>Probably the second chosen feature will be 2 (prob_array[2] = 0.42857 which is the largest probability). As you can
see up till now the algorithm has chosen two the least concurrent labels (1 and 2).</p>
<p><strong>3rd Iteration</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">phi</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>
<span class="n">uniqueness_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">):</span>
    <span class="n">ind_mat_reduced</span> <span class="o">=</span> <span class="n">ind_mat</span><span class="p">[:,</span> <span class="n">phi</span> <span class="o">+</span> <span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    <span class="n">label_uniqueness</span> <span class="o">=</span> <span class="n">get_ind_mat_average_uniqueness</span><span class="p">(</span><span class="n">ind_mat_reduced</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">uniqueness_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">label_uniqueness</span><span class="p">[</span><span class="n">label_uniqueness</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="n">prob_array</span> <span class="o">=</span> <span class="n">uniqueness_array</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">uniqueness_array</span><span class="p">)</span>

<span class="n">prob_array</span>
<span class="o">&gt;&gt;</span> <span class="n">array</span><span class="p">([</span><span class="mf">0.45454545454545453</span><span class="p">,</span> <span class="mf">0.2727272727272727</span><span class="p">,</span> <span class="mf">0.2727272727272727</span><span class="p">],</span>
  <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
</pre></div>
</div>
<p>Sequential Bootstrapping tries to minimise the probability of repeated samples so as you can see the most probable sample
would be 0 with 1 and 2 already selected.</p>
<p><strong>4th Iteration</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">phi</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">uniqueness_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">):</span>
    <span class="n">ind_mat_reduced</span> <span class="o">=</span> <span class="n">ind_mat</span><span class="p">[:,</span> <span class="n">phi</span> <span class="o">+</span> <span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    <span class="n">label_uniqueness</span> <span class="o">=</span> <span class="n">get_ind_mat_average_uniqueness</span><span class="p">(</span><span class="n">ind_mat_reduced</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">uniqueness_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">label_uniqueness</span><span class="p">[</span><span class="n">label_uniqueness</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="n">prob_array</span> <span class="o">=</span> <span class="n">uniqueness_array</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">uniqueness_array</span><span class="p">)</span>

<span class="n">prob_array</span>
<span class="o">&gt;&gt;</span> <span class="n">array</span><span class="p">([</span><span class="mf">0.32653061224489793</span><span class="p">,</span> <span class="mf">0.3061224489795918</span><span class="p">,</span> <span class="mf">0.36734693877551017</span><span class="p">],</span>
  <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
</pre></div>
</div>
<p>The most probable sample would be 2 in this case.</p>
<p>After 4 steps of sequential bootstrapping our drawn samples are [1, 2, 0, 2].</p>
<p>Let’s see how this example is solved by the mlfinlab implementation. To reproduce that:</p>
<ol class="arabic simple">
<li><p>we need to set warmup to [1], which corresponds to phi = [1] on the first step</p></li>
<li><p>verbose = True to print updated probabilities</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">samples</span> <span class="o">=</span> <span class="n">seq_bootstrap</span><span class="p">(</span><span class="n">ind_mat</span><span class="p">,</span> <span class="n">sample_length</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">warmup_samples</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="o">&gt;&gt;</span> <span class="p">[</span><span class="mf">0.33333333</span> <span class="mf">0.33333333</span> <span class="mf">0.33333333</span><span class="p">]</span>
<span class="o">&gt;&gt;</span> <span class="p">[</span><span class="mf">0.35714286</span> <span class="mf">0.21428571</span> <span class="mf">0.42857143</span><span class="p">]</span>
<span class="o">&gt;&gt;</span> <span class="p">[</span><span class="mf">0.45454545</span> <span class="mf">0.27272727</span> <span class="mf">0.27272727</span><span class="p">]</span>
<span class="o">&gt;&gt;</span> <span class="p">[</span><span class="mf">0.32653061</span> <span class="mf">0.30612245</span> <span class="mf">0.36734694</span><span class="p">]</span>

<span class="n">samples</span>
<span class="o">&gt;&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
<p>As you can see the first 2 iterations of algorithm yield the same probabilities, however sometimes the algorithm
randomly chooses not the 2 sample on 2nd iteration that is why further probabilities are different from the example above.
However, if you repeat the process several times you’ll see that on average drawn sample equal to the one from the example</p>
</section>
<section id="monte-carlo-experiment">
<h5>Monte-Carlo Experiment<a class="headerlink" href="#monte-carlo-experiment" title="Permalink to this heading">¶</a></h5>
<p>Let’s see how sequential bootstrapping increases average label uniqueness on this example by generating 3 samples using
sequential bootstrapping and 3 samples using standard random choise, repeat the experiment 10000 times and record
corresponding label uniqueness in each experiment</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">standard_unq_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span> <span class="c1"># Array of random sampling uniqueness</span>
<span class="n">seq_unq_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span> <span class="c1"># Array of Sequential Bootstapping uniqueness</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10000</span><span class="p">):</span>
    <span class="n">bootstrapped_samples</span> <span class="o">=</span> <span class="n">seq_bootstrap</span><span class="p">(</span><span class="n">ind_mat</span><span class="p">,</span> <span class="n">sample_length</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">random_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">ind_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

    <span class="n">random_unq</span> <span class="o">=</span> <span class="n">get_ind_mat_average_uniqueness</span><span class="p">(</span><span class="n">ind_mat</span><span class="p">[:,</span> <span class="n">random_samples</span><span class="p">])</span>
    <span class="n">random_unq_mean</span> <span class="o">=</span> <span class="n">random_unq</span><span class="p">[</span><span class="n">random_unq</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">sequential_unq</span> <span class="o">=</span> <span class="n">get_ind_mat_average_uniqueness</span><span class="p">(</span><span class="n">ind_mat</span><span class="p">[:,</span> <span class="n">bootstrapped_samples</span><span class="p">])</span>
    <span class="n">sequential_unq_mean</span> <span class="o">=</span> <span class="n">sequential_unq</span><span class="p">[</span><span class="n">sequential_unq</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">standard_unq_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">random_unq_mean</span>
    <span class="n">seq_unq_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">sequential_unq_mean</span>
</pre></div>
</div>
<p>KDE plots of label uniqueness support the fact that sequential bootstrapping gives higher average label uniqueness</p>
<a class="reference internal image-reference" href="_images/monte_carlo_bootstrap.png"><img alt="_images/monte_carlo_bootstrap.png" class="align-center" src="_images/monte_carlo_bootstrap.png" style="width: 475.8px; height: 327.6px;" /></a>
<p>We can compare average label uniqueness using sequential bootstrap vs label uniqueness using standard random sampling
by setting compare parameter to True. We have massively increased the performance of Sequential Bootstrapping which was
described in the book. For comparison generating 50 samples from 8000 barrier-events would take 3 days, we have reduced
time to 10-12 seconds which decreases by increasing number of CPUs.</p>
<p>Let’s apply sequential bootstrapping to our full data set and draw 50 samples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Standard</span> <span class="n">uniqueness</span><span class="p">:</span> <span class="mf">0.9465875370919882</span>
<span class="n">Sequential</span> <span class="n">uniqueness</span><span class="p">:</span> <span class="mf">0.9913169319826338</span>
</pre></div>
</div>
<p>Sometimes you would see that standard bootstrapping gives higher uniqueness, however as it was shown in Monte-Carlo
example, on average Sequential Bootstrapping algorithm has higher average uniqueness.</p>
</section>
</section>
<section id="sample-weights">
<h4>Sample Weights<a class="headerlink" href="#sample-weights" title="Permalink to this heading">¶</a></h4>
<p>mlfinlab supports two methods of applying sample weights. The first is weighting an observation based on its given return
as well as average uniqueness. The second is weighting an observation based on a time decay.</p>
<section id="by-returns-and-average-uniqueness">
<h5>By Returns and Average Uniqueness<a class="headerlink" href="#by-returns-and-average-uniqueness" title="Permalink to this heading">¶</a></h5>
<p>The following function utilizes a samples average uniqueness and its return to compute sample weights:</p>
<p>This function can be utilized as shown below assuming we have already found our barrier events</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mlfinlab.sample_weights.attribution</span> <span class="kn">import</span> <span class="n">get_weights_by_return</span>

<span class="n">barrier_events</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">close_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>


<span class="n">sample_weights</span> <span class="o">=</span> <span class="n">get_weights_by_return</span><span class="p">(</span><span class="n">barrier_events</span><span class="p">,</span> <span class="n">close_prices</span><span class="o">.</span><span class="n">close</span><span class="p">,</span>
                                       <span class="n">num_threads</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="by-time-decay">
<h5>By Time Decay<a class="headerlink" href="#by-time-decay" title="Permalink to this heading">¶</a></h5>
<p>The following function assigns sample weights using a time decay factor</p>
<p>This function can be utilized as shown below assuming we have already found our barrier events</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mlfinlab.sample_weights.attribution</span> <span class="kn">import</span> <span class="n">get_weights_by_time_decay</span>


<span class="n">barrier_events</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">close_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>


<span class="n">sample_weights</span> <span class="o">=</span>  <span class="n">get_weights_by_time_decay</span><span class="p">(</span><span class="n">barrier_events</span><span class="p">,</span> <span class="n">close_prices</span><span class="o">.</span><span class="n">close</span><span class="p">,</span>
                                            <span class="n">num_threads</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="research-notebooks">
<h4>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h4>
<p>The following research notebooks can be used to better understand the previously discussed sampling methods</p>
<section id="sample-uniqueness-and-weights">
<h5>Sample Uniqueness and Weights<a class="headerlink" href="#sample-uniqueness-and-weights" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Sample%20Weights/Chapter4_Exercises.ipynb">Sample Uniqueness and Weights</a></p></li>
</ul>
</section>
<section id="id2">
<h5>Sequential Bootstrapping<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Sample%20Weights/Sequential_Bootstrapping.ipynb">Sequential Bootstrapping</a></p></li>
</ul>
</section>
</section>
</section>
<span id="document-implementations/sb_bagging"></span><section id="sequentially-bootstrapped-bagging-classifier-regressor">
<span id="implementations-sb-bagging"></span><h3>Sequentially Bootstrapped Bagging Classifier/Regressor<a class="headerlink" href="#sequentially-bootstrapped-bagging-classifier-regressor" title="Permalink to this heading">¶</a></h3>
<p>In sampling section we have shown that sampling should be done by Sequential Bootstrapping.
SequentiallyBootstrappedBaggingClassifier and SequentiallyBootstrappedBaggingRegressor extend <a class="reference external" href="https://scikit-learn.org/">sklearn</a>’s
BaggingClassifier/Regressor by using Sequential Bootstrapping instead of random sampling.</p>
<p>In order to build indicator matrix we need Triple Barrier Events (samples_info_sets) and price bars used to label
training data set. That is why samples_info_sets and price bars are input parameters for classifier/regressor.</p>
<section id="implementation">
<h4>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h4>
</section>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h4>
<p>An example of using SequentiallyBootstrappedBaggingClassifier</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">mlfinlab.ensemble</span> <span class="kn">import</span> <span class="n">SequentiallyBootstrappedBaggingClassifier</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;X_FILE_PATH&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;y_FILE_PATH&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">triple_barrier_events</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;BARRIER_FILE_PATH&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">price_bars</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;PRICE_BARS_FILE_PATH&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="n">triple_barrier_events</span> <span class="o">=</span> <span class="n">triple_barrier_events</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="p">:]</span> <span class="c1"># take only train part</span>
<span class="n">triple_barrier_events</span> <span class="o">=</span> <span class="n">triple_barrier_events</span><span class="p">[(</span><span class="n">triple_barrier_events</span><span class="o">.</span><span class="n">index</span> <span class="o">&gt;=</span> <span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">&amp;</span>
                                              <span class="p">(</span><span class="n">triple_barrier_events</span><span class="o">.</span><span class="n">index</span> <span class="o">&lt;=</span> <span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">max</span><span class="p">())]</span>

<span class="n">base_est</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;entropy&#39;</span><span class="p">,</span> <span class="n">bootstrap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                  <span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced_subsample&#39;</span><span class="p">)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">SequentiallyBootstrappedBaggingClassifier</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">base_est</span><span class="p">,</span>
                                                <span class="n">samples_info_sets</span><span class="o">=</span><span class="n">triple_barrier_events</span><span class="o">.</span><span class="n">t1</span><span class="p">,</span>
                                                <span class="n">price_bars</span><span class="o">=</span><span class="n">price_bars</span><span class="p">,</span> <span class="n">oob_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<span id="document-implementations/feature_importance"></span><section id="feature-importance">
<span id="implementations-feature-importance"></span><h3>Feature Importance<a class="headerlink" href="#feature-importance" title="Permalink to this heading">¶</a></h3>
<p class="centered">
<strong><strong>Backtesting is not a research tool. Feature importance is. (Lopez de Prado)</strong></strong></p><div class="line-block">
<div class="line"><br /></div>
</div>
<section id="mdi-mda-and-sfi-feature-importance">
<h4>MDI, MDA, and SFI Feature Importance<a class="headerlink" href="#mdi-mda-and-sfi-feature-importance" title="Permalink to this heading">¶</a></h4>
<p>The book describes three methods to get importance scores:</p>
<ol class="arabic simple">
<li><p><strong>Mean Decrease Impurity (MDI)</strong>: This score can be obtained from tree-based classifiers and corresponds to sklearn’s
feature_importances attribute. MDI uses in-sample (IS) performance to estimate feature importance.</p></li>
<li><p><strong>Mean Decrease Accuracy (MDA)</strong>: This method can be applied to any classifier, not only tree based. MDA uses
out-of-sample (OOS) performance in order to estimate feature importance.</p></li>
<li><p><strong>Single Feature Importance (SFI)</strong>: MDA and MDI feature suffer from substitution effects. If two features are highly
correlated, one of them will be considered as important while the other one will be redundant. SFI is a OOS feature
importance estimator which doesn’t suffer from substitution effects because it estimates each feature importance separately.</p></li>
</ol>
<section id="implementation">
<h5>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h5>
</section>
<section id="example">
<h5>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h5>
<p>An example showing how to use various feature importance functions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">log_loss</span>

<span class="kn">from</span> <span class="nn">mlfinlab.ensemble</span> <span class="kn">import</span> <span class="n">SequentiallyBootstrappedBaggingClassifier</span>
<span class="kn">from</span> <span class="nn">mlfinlab.feature_importance</span> <span class="kn">import</span> <span class="p">(</span><span class="n">mean_decrease_impurity</span><span class="p">,</span> <span class="n">mean_decrease_accuracy</span><span class="p">,</span> <span class="n">single_feature_importance</span><span class="p">,</span> <span class="n">plot_feature_importance</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlfinlab.cross_validation</span> <span class="kn">import</span> <span class="n">PurgedKFold</span><span class="p">,</span> <span class="n">ml_cross_val_score</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;X_FILE_PATH.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;y_FILE_PATH.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">triple_barrier_events</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;BARRIER_FILE_PATH&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">price_bars</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;PRICE_BARS_FILE_PATH&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="n">triple_barrier_events</span> <span class="o">=</span> <span class="n">triple_barrier_events</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">X_train</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="p">:]</span> <span class="c1"># Take only train part</span>
<span class="n">triple_barrier_events</span> <span class="o">=</span> <span class="n">triple_barrier_events</span><span class="p">[(</span><span class="n">triple_barrier_events</span><span class="o">.</span><span class="n">index</span> <span class="o">&gt;=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">&amp;</span>
                                              <span class="p">(</span><span class="n">triple_barrier_events</span><span class="o">.</span><span class="n">index</span> <span class="o">&lt;=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">max</span><span class="p">())]</span>

<span class="n">cv_gen</span> <span class="o">=</span> <span class="n">PurgedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">samples_info_sets</span><span class="o">=</span><span class="n">triple_barrier_events</span><span class="o">.</span><span class="n">t1</span><span class="p">)</span>

<span class="n">base_est</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;entropy&#39;</span><span class="p">,</span> <span class="n">bootstrap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                  <span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced_subsample&#39;</span><span class="p">)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">SequentiallyBootstrappedBaggingClassifier</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">base_est</span><span class="p">,</span> <span class="n">samples_info_sets</span><span class="o">=</span><span class="n">triple_barrier_events</span><span class="o">.</span><span class="n">t1</span><span class="p">,</span>
                                                <span class="n">price_bars</span><span class="o">=</span><span class="n">price_bars</span><span class="p">,</span> <span class="n">oob_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">oos_score</span> <span class="o">=</span> <span class="n">ml_cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv_gen</span><span class="o">=</span><span class="n">cv_gen</span><span class="p">,</span> <span class="n">sample_weight_train</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">accuracy_score</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">mdi_feature_imp</span> <span class="o">=</span> <span class="n">mean_decrease_impurity</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">mda_feature_imp</span> <span class="o">=</span> <span class="n">mean_decrease_accuracy</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv_gen</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">log_loss</span><span class="p">)</span>
<span class="n">sfi_feature_imp</span> <span class="o">=</span> <span class="n">single_feature_importance</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv_gen</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">accuracy_score</span><span class="p">)</span>

<span class="n">plot_feature_importance</span><span class="p">(</span><span class="n">mdi_feature_imp</span><span class="p">,</span> <span class="n">oob_score</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">oob_score_</span><span class="p">,</span> <span class="n">oos_score</span><span class="o">=</span><span class="n">oos_score</span><span class="p">,</span>
                        <span class="n">save_fig</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_path</span><span class="o">=</span><span class="s1">&#39;mdi_feat_imp.png&#39;</span><span class="p">)</span>
<span class="n">plot_feature_importance</span><span class="p">(</span><span class="n">mda_feature_imp</span><span class="p">,</span> <span class="n">oob_score</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">oob_score_</span><span class="p">,</span> <span class="n">oos_score</span><span class="o">=</span><span class="n">oos_score</span><span class="p">,</span>
                        <span class="n">save_fig</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_path</span><span class="o">=</span><span class="s1">&#39;mda_feat_imp.png&#39;</span><span class="p">)</span>
<span class="n">plot_feature_importance</span><span class="p">(</span><span class="n">sfi_feature_imp</span><span class="p">,</span> <span class="n">oob_score</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">oob_score_</span><span class="p">,</span> <span class="n">oos_score</span><span class="o">=</span><span class="n">oos_score</span><span class="p">,</span>
                        <span class="n">save_fig</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_path</span><span class="o">=</span><span class="s1">&#39;sfi_feat_imp.png&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The following are the resulting images from the MDI, MDA, and SFI feature importances respectively:</p>
<a class="reference internal image-reference" href="_images/mdi_feat_imp.png"><img alt="_images/mdi_feat_imp.png" class="align-center" src="_images/mdi_feat_imp.png" style="width: 690.0px; height: 339.6px;" /></a>
<a class="reference internal image-reference" href="_images/mda_feat_imp.png"><img alt="_images/mda_feat_imp.png" class="align-center" src="_images/mda_feat_imp.png" style="width: 710.4000000000001px; height: 350.8px;" /></a>
<a class="reference internal image-reference" href="_images/sfi_feat_imp.png"><img alt="_images/sfi_feat_imp.png" class="align-center" src="_images/sfi_feat_imp.png" style="width: 692.8000000000001px; height: 338.8px;" /></a>
</section>
<section id="research-notebook">
<h5>Research Notebook<a class="headerlink" href="#research-notebook" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Feature%20Importance/Chapter8_Exercises_Feature_Importance.ipynb">Answering Questions on MDI, MDA, and SFI Feature Importance</a></p></li>
</ul>
<hr class="docutils" />
<div class="line-block">
<div class="line"><br /></div>
</div>
</section>
</section>
<section id="clustered-feature-importance">
<h4>Clustered Feature Importance<a class="headerlink" href="#clustered-feature-importance" title="Permalink to this heading">¶</a></h4>
<p>In the book Machine Learning for Asset Managers, as an approach to deal with substitution effect Clustered Feature
Importance was introduced. It clusters similar features and applies feature importance analysis (like MDA and MDI) at
the cluster level. The value add of clustering is that the clusters are mutually dissimilar and hence reduces the substitution
effects.</p>
<p>It can be implemented in two steps as described in the book:</p>
<ol class="arabic simple">
<li><p><strong>Features Clustering</strong>: As a first step we need to generate the clusters or subsets of features we want to analyse
with feature importance methods. This can be done using the feature cluster module. It implement the method of generating
feature clusters as in the book.</p></li>
<li><p><strong>Clustered Importance</strong>: Now that we have identified the number and composition of the clusters of features.
We can use this information to apply MDI and MDA on groups of similar features, rather than on individual features.
Clustered Feature Importance can be implemented by simply passing the feature clusters obtained in Step-1 to the
clustered_subsets argument of the MDI or MDA feature importance algorithm.</p></li>
</ol>
<p>Ways by Cluster Feature Importance can be applied:</p>
<ol class="arabic simple">
<li><p>Clustered MDI (code Snippet 6.4 page 86 ): We compute the clustered MDI as the sum of the MDI values of the features
that constitute that cluster. If there is one feature per cluster, then MDI and clustered MDI are the same.</p></li>
<li><p>Clustered MDA (code Snippet 6.5 page 87 ): As an extension to normal MDA to tackle multi-collinearity and
(linear or non-linear) substitution effect. Its implementation was also discussed by Dr. Marcos Lopez de Prado
in the <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3517595">Clustered Feature Importance (Presentaion Slides)</a>.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The implementation of Clustered feature importance is included in the functions for MDI and MDA.</p>
</div>
<section id="id1">
<h5>Example<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">log_loss</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection._split</span> <span class="kn">import</span> <span class="n">KFold</span>

<span class="kn">from</span> <span class="nn">mlfinlab.util.generate_dataset</span> <span class="kn">import</span> <span class="n">get_classification_data</span>
<span class="kn">from</span> <span class="nn">mlfinlab.feature_importance</span> <span class="kn">import</span> <span class="p">(</span><span class="n">mean_decrease_impurity</span><span class="p">,</span> <span class="n">mean_decrease_accuracy</span><span class="p">,</span>
                                         <span class="n">plot_feature_importance</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlfinlab.cross_validation</span> <span class="kn">import</span>  <span class="n">ml_cross_val_score</span>
<span class="kn">from</span> <span class="nn">mlfinlab.clustering.feature_clusters</span> <span class="kn">import</span> <span class="n">get_feature_clusters</span>

<span class="c1"># Create Clusters</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">get_classification_data</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
                               <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">.1</span><span class="p">)</span>
<span class="n">feature_clusters</span> <span class="o">=</span> <span class="n">get_feature_clusters</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dependence_metric</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">n_clusters</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># Fit model</span>
<span class="n">clf_base</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;entropy&#39;</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced&#39;</span><span class="p">,</span>
                                  <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">clf_base</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                        <span class="n">max_samples</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">oob_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Score model</span>
<span class="n">cv_gen</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">oos_score</span> <span class="o">=</span> <span class="n">ml_cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv_gen</span><span class="o">=</span><span class="n">cv_gen</span><span class="p">,</span> <span class="n">sample_weight_train</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                               <span class="n">scoring</span><span class="o">=</span><span class="n">accuracy_score</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Feature Importance</span>
<span class="n">clustered_mdi</span> <span class="o">=</span> <span class="n">mean_decrease_impurity</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span>
                                       <span class="n">clustered_subsets</span><span class="o">=</span><span class="n">feature_clusters</span><span class="p">)</span>
<span class="n">clustered_mda</span> <span class="o">=</span> <span class="n">mean_decrease_accuracy</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv_gen</span><span class="p">,</span>
                                       <span class="n">clustered_subsets</span><span class="o">=</span><span class="n">feature_clusters</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">log_loss</span><span class="p">)</span>

<span class="c1"># Plot</span>
<span class="n">plot_feature_importance</span><span class="p">(</span><span class="n">clustered_mdi</span><span class="p">,</span> <span class="n">oob_score</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">oob_score_</span><span class="p">,</span> <span class="n">oos_score</span><span class="o">=</span><span class="n">oos_score</span><span class="p">,</span>
                        <span class="n">save_fig</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_path</span><span class="o">=</span><span class="s1">&#39;clustered_mdi.png&#39;</span><span class="p">)</span>
<span class="n">plot_feature_importance</span><span class="p">(</span><span class="n">clustered_mda</span><span class="p">,</span> <span class="n">oob_score</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">oob_score_</span><span class="p">,</span> <span class="n">oos_score</span><span class="o">=</span><span class="n">oos_score</span><span class="p">,</span>
                        <span class="n">save_fig</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_path</span><span class="o">=</span><span class="s1">&#39;clustered_mda.png&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The following are the resulting images from the Clustered MDI &amp; Clustered MDA feature importances respectively:</p>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="_images/clustered_mdi.png"><img alt="Clustered MDI" src="_images/clustered_mdi.png" style="width: 503.99999999999994px; height: 403.2px;" /></a>
<figcaption>
<p><span class="caption-text">Clustered MDI</span><a class="headerlink" href="#id8" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-center" id="id9">
<a class="reference internal image-reference" href="_images/clustered_mda.png"><img alt="Clustered MDA" src="_images/clustered_mda.png" style="width: 503.99999999999994px; height: 403.2px;" /></a>
<figcaption>
<p><span class="caption-text">Clustered MDA</span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="id2">
<h5>Research Notebook<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h5>
<p>The following research notebooks can be used to better understand the Clustered Feature Importance and its implementations.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Feature%20Importance/Cluster_Feature_Importance.ipynb">Clustered Feature Importance</a></p></li>
</ul>
<hr class="docutils" />
<div class="line-block">
<div class="line"><br /></div>
</div>
</section>
</section>
<section id="model-fingerprints-algorithm">
<h4>Model Fingerprints Algorithm<a class="headerlink" href="#model-fingerprints-algorithm" title="Permalink to this heading">¶</a></h4>
<p>Another way to get a better understanding of a machine learning model is to understand how feature values influence model predictions. Feature effects can be decomposed into 3 components(fingerprints):</p>
<ul class="simple">
<li><p><strong>Linear component</strong></p></li>
<li><p><strong>Non-linear component</strong></p></li>
<li><p><strong>Pairwise interaction component</strong></p></li>
</ul>
<p>Yimou Li, David Turkington, and Alireza Yazdani published a paper in the Journal of Financial Data Science <a class="reference external" href="https://jfds.pm-research.com/content/early/2019/12/11/jfds.2019.1.023">‘Beyond the Black Box: An Intuitive Approach to Investment Prediction with Machine Learning’</a> which describes in details the algorithm of extracting <strong>linear</strong>, <strong>non-linear</strong> and <strong>pairwise</strong> feature effects.
This module implements the algorithm described in the article.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>I would like to highlight that this algorithm is one of the tools that our team uses the most! There are 2 classes which
inherit from an abstract base class, you only need to instantiate the child classes.</p>
</div>
<section id="id4">
<h5>Implementation<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h5>
</section>
<section id="id5">
<h5>Example<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="nn">mlfinlab.feature_importance</span> <span class="kn">import</span> <span class="n">RegressionModelFingerprint</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span> <span class="c1"># Get a dataset</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;feature_names&#39;</span><span class="p">],</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">])</span>

<span class="c1"># Fit the model</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">reg_fingerprint</span> <span class="o">=</span> <span class="n">RegressionModelFingerprint</span><span class="p">()</span>
<span class="n">reg_fingerprint</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">reg</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">num_values</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">pairwise_combinations</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;CRIM&#39;</span><span class="p">,</span> <span class="s1">&#39;ZN&#39;</span><span class="p">),</span>
                                                                  <span class="p">(</span><span class="s1">&#39;RM&#39;</span><span class="p">,</span> <span class="s1">&#39;AGE&#39;</span><span class="p">),</span>
                                                                  <span class="p">(</span><span class="s1">&#39;LSTAT&#39;</span><span class="p">,</span> <span class="s1">&#39;DIS&#39;</span><span class="p">)])</span>
<span class="n">reg_fingerprint</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span> <span class="c1"># Fit the model</span>

<span class="c1"># Get linear non-linear effects and pairwise effects</span>
<span class="n">linear_effect</span><span class="p">,</span> <span class="n">non_linear_effect</span><span class="p">,</span> <span class="n">pair_wise_effect</span> <span class="o">=</span> <span class="n">reg_fingerprint</span><span class="o">.</span><span class="n">get_effects</span><span class="p">()</span>

<span class="c1"># Plot the results</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">reg_fingerprint</span><span class="o">.</span><span class="n">plot_effects</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="_images/effects.png"><img alt="_images/effects.png" class="align-center" src="_images/effects.png" style="width: 518.4px; height: 432.0px;" /></a>
<hr class="docutils" />
<div class="line-block">
<div class="line"><br /></div>
</div>
</section>
</section>
<section id="pca-features-and-analysis">
<h4>PCA Features and Analysis<a class="headerlink" href="#pca-features-and-analysis" title="Permalink to this heading">¶</a></h4>
<p>A partial solution to solve substitution effects is to orthogonalize features - apply PCA to them. However, PCA can be used
not only to reduce the dimension of your data set, but also to understand whether the patterns detected by feature importance are valid.</p>
<p>Suppose, that you derive orthogonal features using PCA. Your PCA analysis has determined that some features are more
‘principal’ than others, without any knowledge of the labels (unsupervised learning). That is, PCA has ranked features
without any possible overfitting in a classification sense.</p>
<p>When your MDI, MDA, SFI analysis selects as most important (using label information) the same features that PCA chose as
principal (ignoring label information), this constitutes confirmatory evidence that the pattern identified by the ML
algorithm is not entirely overfit. Here is the example plot of MDI feature importance vs PCA eigen values:</p>
<a class="reference internal image-reference" href="_images/pca_correlation_analysis.png"><img alt="_images/pca_correlation_analysis.png" class="align-center" src="_images/pca_correlation_analysis.png" style="width: 640.0px; height: 480.0px;" /></a>
<section id="id6">
<h5>Implementation<a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h5>
</section>
<section id="id7">
<h5>Example<a class="headerlink" href="#id7" title="Permalink to this heading">¶</a></h5>
<p>Let’s see how PCA feature extraction is analysis are done using mlfinlab functions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.feature_importance.orthogonal</span> <span class="kn">import</span> <span class="p">(</span><span class="n">get_orthogonal_features</span><span class="p">,</span>
                                                    <span class="n">feature_pca_analysis</span><span class="p">)</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;X_FILE_PATH.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">feat_imp</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FEATURE_IMP_PATH.csv&#39;</span><span class="p">)</span>

<span class="n">pca_features</span> <span class="o">=</span> <span class="n">get_orthogonal_features</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">correlation_dict</span> <span class="o">=</span> <span class="n">feature_pca_analysis</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">feat_imp</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<span id="document-implementations/cross_validation"></span><section id="cross-validation">
<span id="implementations-cross-validation"></span><h3>Cross Validation<a class="headerlink" href="#cross-validation" title="Permalink to this heading">¶</a></h3>
<section id="purged-and-embargo-cross-validation">
<h4>Purged and Embargo Cross Validation<a class="headerlink" href="#purged-and-embargo-cross-validation" title="Permalink to this heading">¶</a></h4>
<p>This implementation is based on Chapter 7 of the book Advances in Financial Machine Learning. The purpose of performing
cross validation is to reduce the probability of over-fitting and the book recommends it as the main tool of research.
There are two innovations compared to the classical K-Fold Cross Validation implemented in <a class="reference external" href="https://scikit-learn.org/">sklearn</a>.</p>
<ol class="arabic simple">
<li><p>The first one is a process called <strong>purging</strong> which removes from the <em>training</em> set those samples that are build with
information that overlaps samples in the <em>testing</em> set. More details on this in section 7.4.1, page 105.</p></li>
</ol>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="_images/purging.png"><img alt="purging image" src="_images/purging.png" style="width: 478.2px; height: 402.0px;" /></a>
<figcaption>
<p><span class="caption-text">Image showing the process of <strong>purging</strong>. Figure taken from page 107 of the book.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>2. The second innovation is a process called <strong>embargo</strong> which removes a number of observations from the <em>end</em> of the
test set. This further prevents leakage where the purging process is not enough. More details on this in section 7.4.2, page 107.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="_images/embargo.png"><img alt="embargo image" src="_images/embargo.png" style="width: 476.4px; height: 390.0px;" /></a>
<figcaption>
<p><span class="caption-text">Image showing the process of <strong>embargo</strong>. Figure taken from page 108 of the book.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<section id="module-mlfinlab.cross_validation.cross_validation">
<span id="implementation"></span><h5>Implementation<a class="headerlink" href="#module-mlfinlab.cross_validation.cross_validation" title="Permalink to this heading">¶</a></h5>
<p>Implements the book chapter 7 on Cross Validation for financial data.</p>
<dl class="py class">
<dt class="sig sig-object py" id="mlfinlab.cross_validation.cross_validation.PurgedKFold">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">PurgedKFold</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_splits</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">samples_info_sets</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Series</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pct_embargo</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">float</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.cross_validation.cross_validation.PurgedKFold" title="Permalink to this definition">¶</a></dt>
<dd><p>Extend KFold class to work with labels that span intervals</p>
<p>The train is purged of observations overlapping test-label intervals
Test set is assumed contiguous (shuffle=False), w/o training samples in between</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_splits</strong> – (int) The number of splits. Default to 3</p></li>
<li><p><strong>samples_info_sets</strong> – (pd.Series) The information range on which each record is constructed from
<em>samples_info_sets.index</em>: Time when the information extraction started.
<em>samples_info_sets.value</em>: Time when the information extraction ended.</p></li>
<li><p><strong>pct_embargo</strong> – (float) Percent that determines the embargo size.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mlfinlab.cross_validation.cross_validation.PurgedKFold.split">
<span class="sig-name descname"><span class="pre">split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Series</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.cross_validation.cross_validation.PurgedKFold.split" title="Permalink to this definition">¶</a></dt>
<dd><p>The main method to call for the PurgedKFold class</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> – (pd.DataFrame) Samples dataset that is to be split</p></li>
<li><p><strong>y</strong> – (pd.Series) Sample labels series</p></li>
<li><p><strong>groups</strong> – (array-like), with shape (n_samples,), optional
Group labels for the samples used while splitting the dataset into
train/test set.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(tuple) [train list of sample indices, and test list of sample indices]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.cross_validation.cross_validation.ml_cross_val_score">
<span class="sig-name descname"><span class="pre">ml_cross_val_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">classifier:</span> <span class="pre">~sklearn.base.ClassifierMixin,</span> <span class="pre">X:</span> <span class="pre">~pandas.core.frame.DataFrame,</span> <span class="pre">y:</span> <span class="pre">~pandas.core.series.Series,</span> <span class="pre">cv_gen:</span> <span class="pre">~sklearn.model_selection._split.BaseCrossValidator,</span> <span class="pre">sample_weight_train:</span> <span class="pre">~numpy.ndarray</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">sample_weight_score:</span> <span class="pre">~numpy.ndarray</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">scoring:</span> <span class="pre">~typing.Callable[[~numpy.array,</span> <span class="pre">~numpy.array],</span> <span class="pre">float]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">log_loss&gt;</span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.cross_validation.cross_validation.ml_cross_val_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Advances in Financial Machine Learning, Snippet 7.4, page 110.</p>
<p>Using the PurgedKFold Class.</p>
<p>Function to run a cross-validation evaluation of the using sample weights and a custom CV generator.</p>
<p>Note: This function is different to the book in that it requires the user to pass through a CV object. The book
will accept a None value as a default and then resort to using PurgedCV, this also meant that extra arguments had to
be passed to the function. To correct this we have removed the default and require the user to pass a CV object to
the function.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cv_gen</span> <span class="o">=</span> <span class="n">PurgedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">,</span> <span class="n">samples_info_sets</span><span class="o">=</span><span class="n">samples_info_sets</span><span class="p">,</span> <span class="n">pct_embargo</span><span class="o">=</span><span class="n">pct_embargo</span><span class="p">)</span>
<span class="n">scores_array</span> <span class="o">=</span> <span class="n">ml_cross_val_score</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv_gen</span><span class="p">,</span> <span class="n">sample_weight_train</span><span class="o">=</span><span class="n">sample_train</span><span class="p">,</span>
                                  <span class="n">sample_weight_score</span><span class="o">=</span><span class="n">sample_score</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">accuracy_score</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>classifier</strong> – (ClassifierMixin) A sk-learn Classifier object instance.</p></li>
<li><p><strong>X</strong> – (pd.DataFrame) The dataset of records to evaluate.</p></li>
<li><p><strong>y</strong> – (pd.Series) The labels corresponding to the X dataset.</p></li>
<li><p><strong>cv_gen</strong> – (BaseCrossValidator) Cross Validation generator object instance.</p></li>
<li><p><strong>sample_weight_train</strong> – (np.array) Sample weights used to train the model for each record in the dataset.</p></li>
<li><p><strong>sample_weight_score</strong> – (np.array) Sample weights used to evaluate the model quality.</p></li>
<li><p><strong>scoring</strong> – (Callable) A metric scoring, can be custom sklearn metric.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(np.array) The computed score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mlfinlab.cross_validation.cross_validation.ml_get_train_times">
<span class="sig-name descname"><span class="pre">ml_get_train_times</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">samples_info_sets</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Series</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_times</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Series</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Series</span></span></span><a class="headerlink" href="#mlfinlab.cross_validation.cross_validation.ml_get_train_times" title="Permalink to this definition">¶</a></dt>
<dd><p>Advances in Financial Machine Learning, Snippet 7.1, page 106.</p>
<p>Purging observations in the training set</p>
<p>This function find the training set indexes given the information on which each record is based
and the range for the test set.
Given test_times, find the times of the training observations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>samples_info_sets</strong> – (pd.Series) The information range on which each record is constructed from
<em>samples_info_sets.index</em>: Time when the information extraction started.
<em>samples_info_sets.value</em>: Time when the information extraction ended.</p></li>
<li><p><strong>test_times</strong> – (pd.Series) Times for the test dataset.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(pd.Series) Training set</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="combinatorial-purged-cv-cpcv">
<h4>Combinatorial Purged CV (CPCV)<a class="headerlink" href="#combinatorial-purged-cv-cpcv" title="Permalink to this heading">¶</a></h4>
<p>This implementation is based on Chapter 12 of the book Advances in Financial Machine Learning.</p>
<p>Given a number φ of backtest paths targeted by the researcher, CPCV generates the precise number of combinations of
training/testing sets needed to generate those paths, while purging training observations that contain leaked information.</p>
<p>CPCV can be used for both for cross-validation and backtesting. Instead of backtesting using single path, the researcher
may use CPCV to generate various train/test splits resulting in various paths.</p>
<p>CPCV algorithm:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Partition T observations into N groups without shuffling, where groups n = 1, … , N − 1 are of size [T∕N], and the Nth group is of size T − [T∕N] (N − 1).</p></li>
<li><p>Compute all possible training/testing splits, where for each split N − k groups constitute the training set and k groups constitute the testing set.</p></li>
<li><p>For any pair of labels (y_i , y_j), where y_i belongs to the training set and y j belongs to the testing set, apply the PurgedKFold class to purge y_i if y_i spans over a
period used to determine label y j . This class will also apply an embargo, should some testing samples predate some training samples.</p></li>
<li><p>Fit classifiers ( N ) on the N−k training sets, and produce forecasts on the respective N−k testing sets.</p></li>
<li><p>Compute the φ [N, k] backtest paths. You can calculate one Sharpe ratio from each path, and from that derive the empirical distribution of the strategy’s
Sharpe ratio (rather than a single Sharpe ratio, like WF or CV).</p></li>
<li><p>When combinatorial splits were generated, CombinatorialPurgedKFold class contains backtest paths formed from train/test splits.</p></li>
</ol>
</div></blockquote>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="_images/combinatorial_cv.png"><img alt="Combinatorial Cross-Validation" src="_images/combinatorial_cv.png" style="width: 541.0px; height: 147.0px;" /></a>
<figcaption>
<p><span class="caption-text">Image showing splits for <strong>CPCV(6,2)</strong></span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<section id="id1">
<h5>Implementation<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h5>
<span class="target" id="module-mlfinlab.cross_validation.combinatorial"></span><p>Implements the Combinatorial Purged Cross-Validation class from Chapter 12</p>
<dl class="py class">
<dt class="sig sig-object py" id="mlfinlab.cross_validation.combinatorial.CombinatorialPurgedKFold">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">CombinatorialPurgedKFold</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_splits</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_test_splits</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">int</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">samples_info_sets</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Series</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pct_embargo</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">float</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.cross_validation.combinatorial.CombinatorialPurgedKFold" title="Permalink to this definition">¶</a></dt>
<dd><p>Advances in Financial Machine Learning, Chapter 12.</p>
<p>Implements Combinatial Purged Cross Validation (CPCV)</p>
<p>The train is purged of observations overlapping test-label intervals
Test set is assumed contiguous (shuffle=False), w/o training samples in between</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_splits</strong> – (int) The number of splits. Default to 3</p></li>
<li><p><strong>samples_info_sets</strong> – (pd.Series) The information range on which each record is constructed from
<em>samples_info_sets.index</em>: Time when the information extraction started.
<em>samples_info_sets.value</em>: Time when the information extraction ended.</p></li>
<li><p><strong>pct_embargo</strong> – (float) Percent that determines the embargo size.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mlfinlab.cross_validation.combinatorial.CombinatorialPurgedKFold.split">
<span class="sig-name descname"><span class="pre">split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Series</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">None</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlfinlab.cross_validation.combinatorial.CombinatorialPurgedKFold.split" title="Permalink to this definition">¶</a></dt>
<dd><p>The main method to call for the PurgedKFold class</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> – (pd.DataFrame) Samples dataset that is to be split</p></li>
<li><p><strong>y</strong> – (pd.Series) Sample labels series</p></li>
<li><p><strong>groups</strong> – (array-like), with shape (n_samples,), optional
Group labels for the samples used while splitting the dataset into
train/test set.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(tuple) [train list of sample indices, and test list of sample indices]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="research-notebooks">
<h4>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Cross-Validation%20in%20Finance/Chapter7_Exercises_CrossValidation.ipynb">Answering Chapter 7: Cross-Validation Questions</a></p></li>
</ul>
</section>
</section>
<span id="document-implementations/EF3M"></span><section id="exact-fit-using-first-3-moments-ef3m">
<h3>Exact Fit using first 3 Moments (EF3M)<a class="headerlink" href="#exact-fit-using-first-3-moments-ef3m" title="Permalink to this heading">¶</a></h3>
<p>The EF3M algorithm was introduced in a paper by Marcos Lopez de Prado and Matthew D. Foreman, titled “A mixture of
Gaussians approach to mathematical portfolio oversight: the <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1931734">EF3M algorithm</a>”.</p>
<p>The abstract reads: “An analogue can be made between: (a) the slow pace at which species adapt to an environment, which
often results in the emergence of a new distinct species out of a once homogeneous genetic pool, and (b) the slow
changes that take place over time within a fund, mutating its investment style. A fund’s track record provides a sort
of genetic marker, which we can use to identify mutations. This has motivated our use of a biometric procedure to detect
the emergence of a new investment style within a fund’s track record. In doing so, we answer the question: “What is the probability that
a particular PM’s performance is departing from the reference distribution used to allocate her capital?”  The EF3M
approach, inspired by evolutionary biology, may help detect early stages of an evolutionary divergence in an investment
style, and trigger a decision to review a fund’s capital allocation.”</p>
<p>The Exact Fit of the first 3 Moments (EF3M) algorithm allows the parameters of a mixture of Gaussian distributions to
be estimated given the first 5 moments of the mixture distribution, as well as the assumption that the mixture
distribution is composed of a number of Gaussian distributions.</p>
<p>A more thorough investigation into the algorithm can be found within our <a class="reference external" href="https://github.com/hudson-and-thames/research">Research</a> repository</p>
<a class="reference internal image-reference" href="_images/ef3m.png"><img alt="_images/ef3m.png" class="align-center" src="_images/ef3m.png" style="width: 480.8px; height: 381.6px;" /></a>
<section id="m2n">
<h4>M2N<a class="headerlink" href="#m2n" title="Permalink to this heading">¶</a></h4>
<p>A class for determining the means, standard deviations, and mixture proportion of a given distribution from it’s first four or five statistical moments.</p>
</section>
<section id="utility-functions-for-fitting-of-distribution-mixtures">
<h4>Utility Functions For Fitting Of Distribution Mixtures<a class="headerlink" href="#utility-functions-for-fitting-of-distribution-mixtures" title="Permalink to this heading">¶</a></h4>
</section>
<section id="research-notebooks">
<h4>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h4>
<p>The following research notebooks can be used to better understand bet sizing.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Bet%20Sizing/Chapter10_Exercises.ipynb">Chapter 10 Exercise Notebook</a></p></li>
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Bet%20Sizing/ef3m_testing.ipynb">EF3M Algorithm Test Cases</a></p></li>
</ul>
</section>
</section>
<span id="document-implementations/bet_sizing"></span><section id="bet-sizing">
<h3>Bet Sizing<a class="headerlink" href="#bet-sizing" title="Permalink to this heading">¶</a></h3>
<p>“There are fascinating parallels between strategy games and investing. Some of the best portfolio managers I have worked
with are excellent poker players, perhaps more so than chess players. One reason is bet sizing, for which Texas Hold’em
provides a great analogue and training ground. Your ML algorithm can achieve high accuracy, but if you do not size your
bets properly, your investment strategy will inevitably lose money. In this chapter we will review a few approaches to
size bets from ML predictions.” Advances in Financial Machine Learning, Chapter 10: Bet Sizing, pg 141.</p>
<p>The code in this directory falls under 3 submodules:</p>
<ol class="arabic simple">
<li><p>Bet Sizing: We have extended the code from the book in an easy to use format for practitioners to use going forward.</p></li>
<li><p>EF3M: An implementation of the EF3M algorithm.</p></li>
<li><p>Chapter10_Snippets: Documented and adjusted snippets from the book for users to experiment with.</p></li>
</ol>
<a class="reference internal image-reference" href="_images/bet_sizing.png"><img alt="_images/bet_sizing.png" class="align-center" src="_images/bet_sizing.png" style="width: 464.5px; height: 368.5px;" /></a>
<section id="bet-sizing-methods">
<h4>Bet Sizing Methods<a class="headerlink" href="#bet-sizing-methods" title="Permalink to this heading">¶</a></h4>
<p>Functions for bet sizing are implemented based on the approaches described in chapter 10.</p>
<section id="bet-sizing-from-predicted-probability">
<h5>Bet Sizing From Predicted Probability<a class="headerlink" href="#bet-sizing-from-predicted-probability" title="Permalink to this heading">¶</a></h5>
<p>Assuming a machine learning algorithm has predicted a series of investment positions, one can use the probabilities of each of these predictions to derive the size of that specific bet.</p>
</section>
<section id="dynamic-bet-sizes">
<h5>Dynamic Bet Sizes<a class="headerlink" href="#dynamic-bet-sizes" title="Permalink to this heading">¶</a></h5>
<p>Assuming one has a series of forecasted prices for a given investment product, that forecast and the current market price and position can be used to dynamically calculate the bet size.</p>
</section>
<section id="strategy-independent-bet-sizing-approaches">
<h5>Strategy-Independent Bet Sizing Approaches<a class="headerlink" href="#strategy-independent-bet-sizing-approaches" title="Permalink to this heading">¶</a></h5>
<p>These approaches consider the number of concurrent active bets and their sides, and sets the bet size is such a way that reserves some cash for the possibility that the trading signal strengthens before it weakens.</p>
</section>
<section id="additional-utility-functions-for-bet-sizing">
<h5>Additional Utility Functions For Bet Sizing<a class="headerlink" href="#additional-utility-functions-for-bet-sizing" title="Permalink to this heading">¶</a></h5>
</section>
</section>
<section id="chapter-10-code-snippets">
<h4>Chapter 10 Code Snippets<a class="headerlink" href="#chapter-10-code-snippets" title="Permalink to this heading">¶</a></h4>
<p>Chapter 10 of “Advances in Financial Machine Learning” contains a number of Python code snippets, many of which are used to create the top level bet sizing functions. These functions can be found in <code class="docutils literal notranslate"><span class="pre">mlfinlab.bet_sizing.ch10_snippets.py</span></code>.</p>
<section id="snippets-for-bet-sizing-from-probabilities">
<h5>Snippets For Bet Sizing From Probabilities<a class="headerlink" href="#snippets-for-bet-sizing-from-probabilities" title="Permalink to this heading">¶</a></h5>
</section>
<section id="snippets-for-dynamic-bet-sizing">
<h5>Snippets for Dynamic Bet Sizing<a class="headerlink" href="#snippets-for-dynamic-bet-sizing" title="Permalink to this heading">¶</a></h5>
</section>
</section>
<section id="research-notebooks">
<h4>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h4>
<p>The following research notebooks can be used to better understand bet sizing.</p>
<section id="exercises-from-chapter-10">
<h5>Exercises From Chapter 10<a class="headerlink" href="#exercises-from-chapter-10" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Bet%20Sizing/Chapter10_Exercises.ipynb">Chapter 10 Exercise Notebook</a></p></li>
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Bet%20Sizing/ef3m_testing.ipynb">EF3M Algorithm Test Cases</a></p></li>
</ul>
</section>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-implementations/onc"></span><blockquote>
<div></div></blockquote>
<section id="optimal-number-of-clusters-onc">
<span id="implementations-clustering"></span><h3>Optimal Number of Clusters (ONC)<a class="headerlink" href="#optimal-number-of-clusters-onc" title="Permalink to this heading">¶</a></h3>
<p>The ONC algorithm detects the optimal number of K-Means clusters using a correlation matrix as input.</p>
<p>Clustering is a process of grouping a set of elements where elements within a group (cluster) are more similar than
elements from different groups. A popular clustering algorithm is the K-means algorithm that guarantees the convergence
in a finite number of steps.</p>
<p>The K-means algorithm has two caveats. It requires the user to set the number of clusters in advance and the initialization
of clusters is random. Consequently, the effectiveness of the algorithm is random. The ONC algorithm proposed by
Marcos Lopez de Prado addresses these two issues.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><strong>Underlying Literature</strong></p>
<p>The following sources elaborate extensively on the topic:</p>
<ul class="simple">
<li><p><strong>Detection of false investment strategies using unsupervised learning methods</strong> <em>by</em> Marcos Lopez de Prado <em>and</em> Lewis, M.J. <a class="reference external" href="https://papers.ssrn.com/sol3/abstract_id=3167017">available here</a>. <em>Describes the ONC algorithm in detail. The code in this module is based on the code written by the researchers.</em></p></li>
<li><p><strong>Machine Learning for Asset Managers</strong> <em>by</em> Marcos Lopez de Prado <a class="reference external" href="https://www.cambridge.org/core/books/machine-learning-for-asset-managers/6D9211305EA2E425D33A9F38D0AE3545">available here</a>. <em>Features additional descriptions of the algorithm and includes exercises to understand the topic in more detail.</em></p></li>
<li><p><strong>Clustering (Presentation Slides)</strong> <em>by</em> Marcos Lopez de Prado <em>and</em> Lewis, M.J. <a class="reference external" href="https://papers.ssrn.com/sol3/abstract_id=3512998">available here</a>. <em>Briefly describes the logic behind the ONC algorithm.</em></p></li>
<li><p><strong>Codependence (Presentation Slides)</strong> <em>by</em> Marcos Lopez de Prado <em>and</em> Lewis, M.J. <a class="reference external" href="https://papers.ssrn.com/sol3/abstract_id=3512994">available here</a>. <em>Explains why the angular distance metric is used to get distances between elements.</em></p></li>
</ul>
</div>
<p>Based on the <strong>Detection of false investment strategies using unsupervised learning methods</strong> paper, this is how the
distances and scores are calculated:</p>
<p>Distances between the elements in the ONC algorithm are calculated using the same angular distance used in the HRP algorithm:</p>
<div class="math notranslate nohighlight">
\[D_{i,j} = \sqrt{\frac{1}{2}(1 - \rho_{i,j})}\]</div>
<p>where <span class="math notranslate nohighlight">\(\rho_{i,j}\)</span> is the correlation between elements <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> .</p>
<p>Distances between distances in the clustering algorithm are calculated as:</p>
<div class="math notranslate nohighlight">
\[\hat{D_{i,j}} = \sqrt{\sum_{k}(D_{i,k} - D_{j,k})^{2}}\]</div>
<p>Silhouette scores are calculated as:</p>
<div class="math notranslate nohighlight">
\[S_i = \frac{b_i - a_i}{max\{a_i,b_i\}}\]</div>
<p>where <span class="math notranslate nohighlight">\(a_i\)</span> the average distance between element <span class="math notranslate nohighlight">\(i\)</span> and all other components in the same cluster,
and <span class="math notranslate nohighlight">\(b_i\)</span> is the smallest  average  distance  between <span class="math notranslate nohighlight">\(i\)</span> and  all  the  nodes  in  any other  cluster.</p>
<p>The measure of clustering quality <span class="math notranslate nohighlight">\(q\)</span> or <span class="math notranslate nohighlight">\(t-score\)</span>:</p>
<div class="math notranslate nohighlight">
\[q= \frac{E[\{S_i\}]}{\sqrt{V[\{S_i\}]}}\]</div>
<p>where <span class="math notranslate nohighlight">\(E[\{S_i\}]\)</span> is the mean of the silhouette scores for each cluster, and <span class="math notranslate nohighlight">\(V[\{S_i\}]\)</span> is the
variance of the silhouette scores for each cluster.</p>
<p>The ONC algorithm structure is described in the work <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3167017">Detection of false investment strategies using unsupervised learning methods</a>
using the following diagrams:</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="_images/onc_base_clustering.png"><img alt="ONC Base Clustering" src="_images/onc_base_clustering.png" style="width: 529.0px; height: 742.0px;" /></a>
<figcaption>
<p><span class="caption-text">Structure of ONC’s base clustering stage.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>In the base clustering stage first, the distances between the elements are calculated, then the algorithm iterates through
a set of possible number of clusters <span class="math notranslate nohighlight">\(N\)</span> times to find the best clustering from <span class="math notranslate nohighlight">\(N\)</span> runs of K-means for every
possible number of clusters. For each iteration, a clustering result is evaluated using the t-statistic of the silhouette scores.</p>
<p>The clustering result with the best t-statistic is picked, the correlation matrix is reordered so that clustered elements
are positioned close to each other.</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="_images/onc_higher_level.png"><img alt="Structure of ONC’s higher-level stage" src="_images/onc_higher_level.png" style="width: 539.0px; height: 840.0px;" /></a>
<figcaption>
<p><span class="caption-text">Structure of ONC’s higher-level stage.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>On a higher level, the average t-score of the clusters from the base clustering stage is calculated. If more than two
clusters have a t-score below average, these clusters go through the base clustering stage again. This process is
recursively repeated.</p>
<p>Then, based on the t-statistic of the old and new clusterings it is checked whether the new clustering is better than
the original one. If not, the old clustering is kept, otherwise, the new one is taken.</p>
<p>The output of the algorithm is the reordered correlation matrix, clustering result, and silhouette scores.</p>
<section id="implementation">
<h4>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h4>
</section>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h4>
<p>An example showing how the ONC algorithm is used can be seen below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.clustering</span> <span class="kn">import</span> <span class="n">onc</span>

<span class="c1"># Import dataframe of returns for assets</span>
<span class="n">asset_returns</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">DATA_PATH</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Calculate correlation matrix of returns</span>
<span class="n">assets_corr</span> <span class="o">=</span> <span class="n">asset_returns</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>

<span class="c1"># Output of the ONC algorithm with 10 simulations for each number of clusters tested</span>
<span class="n">assets_corr_onc</span><span class="p">,</span> <span class="n">clusters</span><span class="p">,</span> <span class="n">silh_scores</span> <span class="o">=</span> <span class="n">onc</span><span class="o">.</span><span class="n">get_onc_clusters</span><span class="p">(</span><span class="n">assets_corr</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="research-notebooks">
<h4>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h4>
<p>The following research notebooks can be used to better understand the Optimal Number of Clusters algorithm.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Clustering/ONC/Optimal_Number_of_Clusters.ipynb">Optimal Number of Clusters Example</a></p></li>
</ul>
</section>
</section>
<span id="document-implementations/feature_clusters"></span><section id="feature-clusters">
<span id="implementations-feature-clusters"></span><h3>Feature Clusters<a class="headerlink" href="#feature-clusters" title="Permalink to this heading">¶</a></h3>
<p>This module implements the clustering of features to generate a feature subset described in the book Machine Learning for
Asset Manager (snippet 6.5.2.1 page-85). This subsets can be further utilised for getting Clustered Feature Importance
using the clustered_subsets argument in the Mean Decreased Impurity (MDI) and Mean Decreased Accuracy (MDA) algorithm.</p>
<p>The algorithm projects the observed features into a metric space by applying the dependence metric function, either correlation
based or information theory based (see the codependence section). Information-theoretic metrics have the advantage of
recognizing redundant features that are the result of nonlinear combinations of informative features.</p>
<p>Next, we need to determine the optimal number of clusters. The user can either specify the number cluster to use, this will apply a
hierarchical clustering on the defined distance matrix of the dependence matrix for a given linkage method for clustering,
or the user can use the ONC algorithm which uses K-Means clustering, to automate these task.</p>
<p>The caveat of this process is that some silhouette scores may be low due to one feature being a combination of multiple features across clusters.
This is a problem, because ONC cannot assign one feature to multiple clusters. Hence, the following transformation may help
reduce the multicollinearity of the system:</p>
<p>For each cluster <span class="math notranslate nohighlight">\(k = 1 . . . K\)</span>, replace the features included in that cluster with residual features, so that it
do not contain any information outside cluster <span class="math notranslate nohighlight">\(k\)</span>. That is let <span class="math notranslate nohighlight">\(D_{k}\)</span> be the subset of index
features <span class="math notranslate nohighlight">\(D = {1,...,F}\)</span> included in cluster <span class="math notranslate nohighlight">\(k\)</span>, where:</p>
<div class="math notranslate nohighlight">
\[D_{k}\subset{D}\ , ||D_{k}|| &gt; 0 \ , \forall{k}\ ; \ D_{k} \bigcap D_{l} = \Phi\ , \forall k \ne l\ ; \bigcup \limits _{k=1} ^{k} D_{k} = D\]</div>
<p>Then, for a given feature <span class="math notranslate nohighlight">\(X_{i}\)</span> where <span class="math notranslate nohighlight">\(i \in D_{k}\)</span>, we compute the residual feature <span class="math notranslate nohighlight">\(\hat \varepsilon _{i}\)</span>
by fitting the following equation for regression:</p>
<div class="math notranslate nohighlight">
\[X_{n,j} = \alpha _{i} + \sum \limits _{j \in \bigcup _{l&lt;k}} \ D_{l} \beta _{i,j} X_{n,j} + \varepsilon _{n,i}\]</div>
<p>Where <span class="math notranslate nohighlight">\(n = 1,\dots,N\)</span> is the index of observations per feature. Note if the degrees of freedom in the above regression
are too low, one option is to use as regressors linear combinations of the features within each cluster by following a
minimum variance weighting scheme so that only <span class="math notranslate nohighlight">\(K-1\)</span> betas need to be estimated. This transformation is not necessary
if the silhouette scores clearly indicate that features belong to their respective clusters.</p>
<section id="implementation">
<h4>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h4>
<p>This module creates clustered subsets of features described in the presentation slides: <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3517595">Clustered Feature Importance</a>
by Marcos Lopez de Prado.</p>
</section>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h4>
<p>An example showing how to generate feature subsets or clusters for a give feature DataFrame.
The example will generate 4 clusters by Hierarchical Clustering for given specification.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.clustering.feature_clusters</span> <span class="kn">import</span> <span class="n">get_feature_clusters</span>

<span class="c1"># Read the a csv file containing only features</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;X_FILE_PATH.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">feat_subs</span> <span class="o">=</span> <span class="n">get_feature_clusters</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dependence_metric</span><span class="o">=</span><span class="s1">&#39;information_variation&#39;</span><span class="p">,</span>
                                 <span class="n">distance_metric</span><span class="o">=</span><span class="s1">&#39;angular&#39;</span><span class="p">,</span> <span class="n">linkage_method</span><span class="o">=</span><span class="s1">&#39;singular&#39;</span><span class="p">,</span>
                                 <span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="research-notebook">
<h4>Research Notebook<a class="headerlink" href="#research-notebook" title="Permalink to this heading">¶</a></h4>
<p>The for better understanding of its implementations see the notebook on Clustered Feature Importance.</p>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-implementations/backtesting"></span><section id="backtesting-by-campbell-and-yan">
<span id="implementations-backtesting"></span><h3>Backtesting by Campbell and Yan<a class="headerlink" href="#backtesting-by-campbell-and-yan" title="Permalink to this heading">¶</a></h3>
<p>The Backtesting module contains algorithms presented in the paper of Campbell R. Harvey and Yan Liu.
These algorithms are focused on adjusting the reported Sharpe ratios to multiple testing and calculating the required
mean return for a strategy at a given level of significance.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><strong>Underlying Literature</strong></p>
<p>The following sources elaborate extensively on the topic:</p>
<ul class="simple">
<li><p><strong>Backtesting</strong> <em>by</em> Campbell R. Harvey <em>and</em> Yan Liu. <a class="reference external" href="https://papers.ssrn.com/abstract_id=2345489">available here</a>. <em>The paper provides a deeper understanding of the Haircut Sharpe ratio and Profit Hurdle algorithms. The code in this module is based on the code written by the researchers.</em></p></li>
<li><p><strong>… and the Cross-section of Expected Returns.</strong> <em>by</em> Harvey, C.R., Y. Liu, and H. Zhu. <a class="reference external" href="https://faculty.fuqua.duke.edu/~charvey/Research/Published_Papers/P118_and_the_cross.PDF">available here</a>. <em>Describes a structural model to capture trading strategies’ underlying distribution, referred to as the HLZ model.</em></p></li>
<li><p><strong>The Statistics of Sharpe Ratios.</strong> <em>by</em> Lo, A. <a class="reference external" href="https://alo.mit.edu/wp-content/uploads/2017/06/The-Statistics-of-Sharpe-Ratios.pdf">available here</a>. <em>Gives a broader understanding of Sharpe ratio adjustments to autocorrelation and different time periods</em></p></li>
</ul>
</div>
<section id="haircut-sharpe-ratio">
<h4>Haircut Sharpe Ratio<a class="headerlink" href="#haircut-sharpe-ratio" title="Permalink to this heading">¶</a></h4>
<p>Adjusts the Sharpe Ratio due to multiple testing.</p>
<p>This algorithm lets the user calculate the Sharpe ratio adjustments and the corresponding haircuts based on the key
parameters of the data used in the strategy backtests. For each of the adjustment methods - Bonferroni, Holm,
BHY (Benjamini, Hochberg, and Yekutieli) and the Average the algorithm calculates an adjusted p-value,
haircut Sharpe ratio, and the haircut.</p>
<p>The haircut is the percentage difference between the original Sharpe ratio and the new Sharpe ratio.</p>
<p>The inputs of the method include information about the returns that were used to calculate the observed Sharpe ratio.
In particular:</p>
<ul class="simple">
<li><p>At what frequency were the returns observed.</p></li>
<li><p>The number of returns observed.</p></li>
<li><p>Observed Sharpe ratio.</p></li>
<li><p>Information on if an observed Sharpe ratio is annualized and if it’s adjusted to the autocorrelation of returns (described in the paper by Lo, A.).</p></li>
<li><p>Autocorrelation coefficient of returns.</p></li>
<li><p>The number of tests in multiple testing allowed (described in the first two papers from the introduction).</p></li>
<li><p>Average correlation among strategy returns.</p></li>
</ul>
<p>Adjustment methods include:</p>
<ul class="simple">
<li><p>Bonferroni</p></li>
<li><p>Holm</p></li>
<li><p>Benjamini, Hochberg, and Yekutieli (BHY)</p></li>
<li><p>Average of the methods above</p></li>
</ul>
<p>The method returns np.array of adjusted p-values, adjusted Sharpe ratios, and haircuts as rows. Elements in a row are
ordered by adjustment methods in the following way [Bonferroni, Holm, BHY, Average].</p>
<p>Haircut Sharpe Ratio algorithm consists of the following steps:</p>
<ol class="arabic simple">
<li><p>We are given the observed Sharpe ratio <span class="math notranslate nohighlight">\(SR\)</span> in <span class="math notranslate nohighlight">\(T\)</span> periods, based on this information we can calculate the
p-value of a single test <span class="math notranslate nohighlight">\(p^S\)</span>.</p></li>
<li><p>Assuming that <span class="math notranslate nohighlight">\(N\)</span> other strategies have been tried and that the average correlation of returns from the strategies
is <span class="math notranslate nohighlight">\(\rho\)</span> , we use the HLZ model to generate <span class="math notranslate nohighlight">\(N\)</span> number of t-statistics from the model. We also transform the
calculated <span class="math notranslate nohighlight">\(p^S\)</span> to a t-statistic.</p></li>
<li><p>This <span class="math notranslate nohighlight">\(N+1\)</span> t-statistics are transformed again to p-values, taking into account the data mining adjustment.</p></li>
<li><p>This set of <span class="math notranslate nohighlight">\(N+1\)</span> p-values are fed to two models described above (Holm and BHY) to get the adjusted p-values
with each of the methods. (Bonferroni adjustment is calculated using only the <span class="math notranslate nohighlight">\(p^S\)</span> and <span class="math notranslate nohighlight">\(N\)</span>)</p></li>
<li><p>The steps 2-4 are repeated multiple times (simulations).</p></li>
<li><p>For each of the two methods, we eventually have a set of <span class="math notranslate nohighlight">\(p^M\)</span> values adjusted. The median of this set is the final
adjusted p-value of the method. So, we obtained p-values for each of the three methods. We then calculate the average
p-value as the Average of the methods.</p></li>
<li><p>The obtained p-values of each method can be then transformed back to Sharpe ratios and the haircuts can be calculated.</p></li>
</ol>
<section id="implementation">
<h5>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h5>
</section>
<section id="example">
<h5>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h5>
<p>An example showing how Haircut Sharpe Ratios method is used can be seen below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.backtests</span> <span class="kn">import</span> <span class="n">CampbellBacktesting</span>

<span class="c1"># Specify the desired number of simulations</span>
<span class="n">backtesting</span> <span class="o">=</span> <span class="n">CampbellBacktesting</span><span class="p">(</span><span class="mi">4000</span><span class="p">)</span>

<span class="c1"># In this example, annualized Sharpe ratio of 1, not adjusted to autocorrelation of returns</span>
<span class="c1"># at 0.1, calculated on monthly observations of returns for two years (24 total observations),</span>
<span class="c1"># with 10 multiple testing and average correlation among returns of 0.4.</span>
<span class="n">haircuts</span> <span class="o">=</span> <span class="n">backtesting</span><span class="o">.</span><span class="n">haircut_sharpe_ratios</span><span class="p">(</span><span class="n">sampling_frequency</span><span class="o">=</span><span class="s1">&#39;M&#39;</span><span class="p">,</span> <span class="n">num_obs</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
                                             <span class="n">sharpe_ratio</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">annualized</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                             <span class="n">autocorr_adjusted</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rho_a</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                             <span class="n">num_mult_test</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>

<span class="c1"># Adjsuted Sharpe ratios by method used</span>
<span class="n">sr_adj_bonferroni</span> <span class="o">=</span> <span class="n">haircuts</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sr_adj_holm</span> <span class="o">=</span> <span class="n">haircuts</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
<span class="n">sr_adj_bhy</span> <span class="o">=</span> <span class="n">haircuts</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span>
<span class="n">sr_adj_average</span> <span class="o">=</span> <span class="n">haircuts</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
</section>
</section>
<section id="profit-hurdle">
<h4>Profit Hurdle<a class="headerlink" href="#profit-hurdle" title="Permalink to this heading">¶</a></h4>
<p>This algorithm calculates the Required Mean Return of a strategy at a given level of significance adjusted due to multiple testing.</p>
<p>The method described below works only with characteristics of monthly returns that have no autocorrelation.</p>
<p>The inputs of the method include information about returns data. In particular:</p>
<ul class="simple">
<li><p>The number of tests in multiple testing allowed (described in the first two papers from the introduction).</p></li>
<li><p>Number of monthly returns observed.</p></li>
<li><p>Significance level.</p></li>
<li><p>Annual return volatility.</p></li>
<li><p>Average correlation among strategy returns.</p></li>
</ul>
<p>Adjustment methods include:</p>
<ul class="simple">
<li><p>Bonferroni</p></li>
<li><p>Holm</p></li>
<li><p>Benjamini, Hochberg, and Yekutieli (BHY)</p></li>
<li><p>Average of the methods above</p></li>
</ul>
<p>Profit Hurdle algorithm consists of the following steps:</p>
<ol class="arabic simple">
<li><p>We are given the significance level <span class="math notranslate nohighlight">\(p\)</span>, strategy volatility <span class="math notranslate nohighlight">\(\sigma\)</span>, the number of observations <span class="math notranslate nohighlight">\(T\)</span> ,
and the number of tests that have been concluded <span class="math notranslate nohighlight">\(T\)</span> .</p></li>
<li><p>Using the HLZ model, we generate <span class="math notranslate nohighlight">\(N\)</span> t-statistics assuming that the average correlation of returns is <span class="math notranslate nohighlight">\(\rho\)</span> .</p></li>
<li><p>Using two methods (Holm and BHY) we calculate the threshold t-statistic that matches the <span class="math notranslate nohighlight">\(p\)</span> significance level.</p></li>
<li><p>The steps 2-3 are repeated multiple times (simulations).</p></li>
<li><p>For the two methods (Holm and BHY) we have a set of t-statistics. We then take the median of t-statistics in each set
and call it a t-statistic for the method. T-ststistic for Bonferroni is calculated based on <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(N\)</span>, as
in the previous algorithm (Haircut Sharpe Ratios).</p></li>
<li><p>The obtained t-statistics of each method can be then transformed to mean monthly returns. We then calculate the average
mean monthly return as the Average of the methods returns.</p></li>
</ol>
<section id="id1">
<h5>Implementation<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h5>
<p>The method returns np.array of minimum average monthly returns by the method as elements. The order of the elements by
method is [Bonferroni, Holm, BHY, Average].</p>
</section>
<section id="id2">
<h5>Example<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h5>
<p>An example showing how Profit Hurdle method is used can be seen below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.backtests</span> <span class="kn">import</span> <span class="n">CampbellBacktesting</span>

<span class="c1"># Specify the desired number of simulations</span>
<span class="n">backtesting</span> <span class="o">=</span> <span class="n">CampbellBacktesting</span><span class="p">(</span><span class="mi">4000</span><span class="p">)</span>

<span class="c1"># In this example, monthly observations of returns for two years (24 total observations),</span>
<span class="c1"># with 10 multiple testing, significance level of 5% and 10% annual volatility and average</span>
<span class="c1"># correlation among returns of 0.4.</span>
<span class="n">monthly_ret</span> <span class="o">=</span> <span class="n">backtesting</span><span class="o">.</span><span class="n">profit_hurdle</span><span class="p">(</span><span class="n">num_mult_test</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_obs</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">alpha_sig</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                                        <span class="n">vol_anu</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>

<span class="c1"># Minimum Average Monthly Returns by method used</span>
<span class="n">monthly_ret_bonferroni</span> <span class="o">=</span> <span class="n">monthly_ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">monthly_ret_holm</span> <span class="o">=</span> <span class="n">monthly_ret</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">monthly_ret_bhy</span> <span class="o">=</span> <span class="n">monthly_ret</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">monthly_ret_average</span> <span class="o">=</span> <span class="n">monthly_ret</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
</section>
</section>
<section id="research-notebooks">
<h4>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h4>
<p>The following research notebooks can be used to better understand how the algorithms within this module work and how
they can be used on real data.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Backtest%20Overfitting/Backtesting.ipynb">Backtesting Notebook</a></p></li>
</ul>
</section>
</section>
<span id="document-implementations/backtest_statistics"></span><section id="backtest-statistics">
<span id="implementations-backtest-statistics"></span><h3>Backtest Statistics<a class="headerlink" href="#backtest-statistics" title="Permalink to this heading">¶</a></h3>
<p>The Backtest Statistics module contains functions related to characteristic analysis of returns and target positions.
These include:</p>
<ul class="simple">
<li><p>Sharpe ratios (annualised, probabilistic, deflated).</p></li>
<li><p>Information ratio.</p></li>
<li><p>Minimum Required Track Record Length.</p></li>
<li><p>Concentration of bets for positive and negative returns.</p></li>
<li><p>Drawdown &amp; Time Under Water.</p></li>
<li><p>Average holding period from a series of positions.</p></li>
<li><p>Filtering flips and flattenings from a series of returns.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><strong>Underlying Literature</strong></p>
<p>The following sources elaborate extensively on the topic:</p>
<ul class="simple">
<li><p><strong>Advances in Financial Machine Learning, Chapter 14 &amp; 15</strong> <em>by</em> Marcos Lopez de Prado. (Page numbers in the code are referring to the pages in this book.)</p></li>
<li><p><strong>The Sharpe Ratio Efficient Frontier</strong> <em>by</em> David H. Bailey <em>and</em> Marcos Lopez de Prado <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1821643">available here</a>. <em>Provides a deeper understanding of Sharpe ratios implemented and Minimum track record length.</em></p></li>
</ul>
</div>
<section id="annualized-sharpe-ratio">
<h4>Annualized Sharpe Ratio<a class="headerlink" href="#annualized-sharpe-ratio" title="Permalink to this heading">¶</a></h4>
<p>Calculates Annualized Sharpe Ratio for pd.Series of normal or log returns.</p>
<p>A usual metric of returns in relation to risk. Also takes into account number of return entries per year and risk-free rate.
Risk-free rate should be given for the same period the returns are given. For example, if the input returns are observed
in 3 months, the risk-free rate given should be the 3-month risk-free rate.</p>
<p>Calculated as:</p>
<div class="math notranslate nohighlight">
\[SharpeRatio = \frac{E[Returns] - RiskFreeRate}{\sqrt{V[Returns]}} * \sqrt{n}\]</div>
<p>Generally, the higher Sharpe Ratio is, the better.</p>
<section id="example">
<h5>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h5>
<p>An example showing how Annualized Sharpe Ratio function is used with monthly cumulative returns data:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.backtest_statistics</span> <span class="kn">import</span> <span class="n">sharpe_ratio</span>

<span class="n">sr</span> <span class="o">=</span> <span class="n">sharpe_ratio</span><span class="p">(</span><span class="n">returns</span><span class="p">,</span> <span class="n">entries_per_year</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="probabilistic-sharpe-ratio">
<h4>Probabilistic Sharpe Ratio<a class="headerlink" href="#probabilistic-sharpe-ratio" title="Permalink to this heading">¶</a></h4>
<p>Calculates the probabilistic Sharpe ratio (PSR) that provides an adjusted estimate of SR, by removing the inflationary
effect caused by short series with skewed and/or fat-tailed returns.</p>
<p>Given a user-defined benchmark Sharpe ratio and an observed Sharpe ratio, PSR estimates the probability that SR ̂is
greater than a hypothetical SR.</p>
<p>If PSR exceeds 0.95, then SR is higher than the hypothetical (benchmark) SR at the standard significance level of 5%.</p>
<p>Formula for calculation:</p>
<div class="math notranslate nohighlight">
\[PSR[SR^{*}] = Z[\frac{(SR - SR^{*})\sqrt{T-1}}{\sqrt{1-\gamma_3*SR+\frac{\gamma_{4}-1}{4}*SR^2}}]\]</div>
<p>Where:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(SR^{*}\)</span> - benchmark Sharpe ratio</p>
<p><span class="math notranslate nohighlight">\(SR\)</span> - estimate od Sharpe ratio</p>
<p><span class="math notranslate nohighlight">\(Z[..]\)</span> - cumulative distribution function (CDF) of the standard Normal distribution</p>
<p><span class="math notranslate nohighlight">\(T\)</span> - number of observed returns</p>
<p><span class="math notranslate nohighlight">\(\gamma_3\)</span> - skewness of the returns</p>
<p><span class="math notranslate nohighlight">\(\gamma_4\)</span> - kurtosis of the returns</p>
</div></blockquote>
<section id="id1">
<h5>Example<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h5>
<p>An example showing how Probabilistic Sharpe Ratio function is used with an example of data with normal returns:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.backtest_statistics</span> <span class="kn">import</span> <span class="n">probabilistic_sharpe_ratio</span>

<span class="n">psr</span> <span class="o">=</span> <span class="n">probabilistic_sharpe_ratio</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="deflated-sharpe-ratio">
<h4>Deflated Sharpe Ratio<a class="headerlink" href="#deflated-sharpe-ratio" title="Permalink to this heading">¶</a></h4>
<p>Calculates the deflated Sharpe ratio (DSR) - a PSR where the rejection threshold is adjusted to reflect the
multiplicity of trials. DSR is estimated as PSR[SR∗], where the benchmark Sharpe ratio, SR∗, is no longer user-defined,
but calculated from SR estimate trails.</p>
<p>DSR corrects SR for inflationary effects caused by non-Normal returns, track record length, and multiple testing/selection
bias.</p>
<p>Given a user-defined benchmark Sharpe ratio and an observed Sharpe estimates (or their properties - standard deviations
and number of trails), DSR estimates the probability that SR is greater than a hypothetical SR. Allows the output of the
hypothetical (benchmark) SR.</p>
<p>If DSR exceeds 0.95, then SR is higher than the hypothetical (benchmark) SR at the standard significance level of 5%.</p>
<p>Hypothetical SR is calculated as:</p>
<div class="math notranslate nohighlight">
\[SR^{*} = \sqrt{V[\{SR_{n}\}]}((1-\gamma)*Z^{-1}[1-\frac{1}{N}+\gamma*Z^{-1}[1-\frac{1}{N}*e^{-1}]\]</div>
<p>Where:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(SR^{*}\)</span> - benchmark Sharpe ratio</p>
<p><span class="math notranslate nohighlight">\(\{SR_{n}\}\)</span> - trails of SR estimates</p>
<p><span class="math notranslate nohighlight">\(Z[..]\)</span> - cumulative distribution function (CDF) of the standard Normal distribution</p>
<p><span class="math notranslate nohighlight">\(N\)</span> - number of SR trails</p>
<p><span class="math notranslate nohighlight">\(\gamma\)</span> - Euler-Mascheroni constant</p>
<p><span class="math notranslate nohighlight">\(e\)</span> - Euler constant</p>
</div></blockquote>
<section id="id2">
<h5>Example<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h5>
<p>An example showing how Deflated Sharpe Ratio function with list of SR estimates as well as properties of SR estimates
and benchmark output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.backtest_statistics</span> <span class="kn">import</span> <span class="n">deflated_sharpe_ratio</span>

<span class="n">dsr</span> <span class="o">=</span> <span class="n">deflated_sharpe_ratio</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">dsr</span> <span class="o">=</span> <span class="n">deflated_sharpe_ratio</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span> <span class="mi">200</span><span class="p">,</span> <span class="n">estimates_param</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">benchmark_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="information-ratio">
<h4>Information Ratio<a class="headerlink" href="#information-ratio" title="Permalink to this heading">¶</a></h4>
<p>Calculates Annualized Information Ratio for a given pandas Series of normal or log returns.</p>
<p>It is the annualized ratio between the average excess return and the tracking error. The excess return is measured as
the portfolio’s return in excess of the benchmark’s return. The tracking error is estimated as the standard deviation of
the excess returns.</p>
<p>Benchmark should be provided as a return for the same time period as that between input returns. For example, for the
daily observations it should be the benchmark of daily returns.</p>
<p>Calculated as:</p>
<div class="math notranslate nohighlight">
\[InformationRatio = \frac{E[Returns - Benchmark]}{\sqrt{V[Returns - Benchmark]}} * \sqrt{n}\]</div>
<section id="id3">
<h5>Example<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h5>
<p>An example showing how Annualized Information Ratio function is used with monthly cumulative returns data:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.backtest_statistics</span> <span class="kn">import</span> <span class="n">information_ratio</span>

<span class="n">information_r</span> <span class="o">=</span> <span class="n">information_ratio</span><span class="p">(</span><span class="n">returns</span><span class="p">,</span> <span class="n">benchmark</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">entries_per_year</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="minimum-track-record-length">
<h4>Minimum Track Record Length<a class="headerlink" href="#minimum-track-record-length" title="Permalink to this heading">¶</a></h4>
<p>Calculates the Minimum Track Record Length - “How long should a track record be in order to have statistical confidence
that its Sharpe ratio is above a given threshold?”</p>
<p>If a track record is shorter than MinTRL, we do not  have  enough  confidence that  the  observed Sharpe ratio ̂is  above
the  designated Sharpe ratio threshold.</p>
<p>MinTRLis expressed in terms of number of observations, not annual or calendar terms.</p>
<p>Minimum Track Record Length is calculated as:</p>
<div class="math notranslate nohighlight">
\[MinTRL = 1 + [1-\gamma_3*SR+\frac{\gamma_{4}-1}{4}*SR^2]*(\frac{Z_{\alpha}}{SR-SR^{*}})^2\]</div>
<p>Where:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(SR^{*}\)</span> - benchmark Sharpe ratio</p>
<p><span class="math notranslate nohighlight">\(SR\)</span> - estimate od Sharpe ratio</p>
<p><span class="math notranslate nohighlight">\(Z_{\alpha}\)</span> - Z score of desired significance level</p>
<p><span class="math notranslate nohighlight">\(\gamma_3\)</span> - skewness of the returns</p>
<p><span class="math notranslate nohighlight">\(\gamma_4\)</span> - kurtosis of the returns</p>
</div></blockquote>
<section id="id4">
<h5>Example<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h5>
<p>An example showing how Minimum Track Record Length function is used with an example of data with normal returns:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.backtest_statistics</span> <span class="kn">import</span> <span class="n">minimum_track_record_length</span>

<span class="n">min_record_length</span> <span class="o">=</span> <span class="n">minimum_track_record_length</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="bets-concentration">
<h4>Bets Concentration<a class="headerlink" href="#bets-concentration" title="Permalink to this heading">¶</a></h4>
<p>Concentration of returns measures the uniformity of returns from bets. Metric is inspired by Herfindahl-Hirschman Index
and is calculated as follows:</p>
<div class="math notranslate nohighlight">
\[Weight_{i} = \frac{Return_{i}}{\sum_{i}Return_{i}}\]</div>
<div class="math notranslate nohighlight">
\[SumSquares = \sum_{i}Weight_{i}^2\]</div>
<div class="math notranslate nohighlight">
\[HHI = \frac{SumSquares - \frac{1}{i}}{1 - \frac{1}{i}}\]</div>
<p>The closer the concentration is to 0, the more uniform the distribution of returns (When 0, returns are uniform). If the
concentration value is close to 1, returns highly concentrated (When 1, only one non-zero return).</p>
<p>Returns <span class="math notranslate nohighlight">\(nan\)</span> if less than 3 returns in series.</p>
<section id="id5">
<h5>Example<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h5>
<p>An example showing how Bets Concentration function is used can be seen below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.backtest_statistics</span> <span class="kn">import</span> <span class="n">bets_concentration</span>

<span class="n">concentration</span> <span class="o">=</span> <span class="n">bets_concentration</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="all-bets-concentration">
<h4>All Bets Concentration<a class="headerlink" href="#all-bets-concentration" title="Permalink to this heading">¶</a></h4>
<p>Concentration of returns measures the uniformity of returns from bets. Metric is inspired by Herfindahl-Hirschman Index
and is calculated as follows:</p>
<div class="math notranslate nohighlight">
\[Weight_{i} = \frac{Return_{i}}{\sum_{i}Return_{i}}\]</div>
<div class="math notranslate nohighlight">
\[SumSquares = \sum_{i}Weight_{i}^2\]</div>
<div class="math notranslate nohighlight">
\[HHI = \frac{SumSquares - \frac{1}{i}}{1 - \frac{1}{i}}\]</div>
<p>The closer the concentration is to 0, the more uniform the distribution of returns (When 0, returns are uniform). If the
concentration is close to 1, returns highly concentrated (When 1, only one non-zero return).</p>
<p>This function calculates concentration separately for positive returns, negative returns and concentration of bets
grouped by time intervals (daily, monthly etc.) separately.</p>
<ul class="simple">
<li><p>If concentration of positive returns is low, there is no right fat tail in returns distribution.</p></li>
<li><p>If concentration of negative returns is low, there is no left fat tail in returns distribution.</p></li>
<li><p>If after time grouping is less than 2 observations, returns third element as nan.</p></li>
</ul>
<section id="id6">
<h5>Example<a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h5>
<p>An example showing how All Bets Concentration function is used with weekly group data:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.backtest_statistics</span> <span class="kn">import</span> <span class="n">all_bets_concentration</span>

<span class="n">pos_concentr</span><span class="p">,</span> <span class="n">neg_concentr</span><span class="p">,</span> <span class="n">week_concentr</span> <span class="o">=</span> <span class="n">all_bets_concentration</span><span class="p">(</span><span class="n">returns</span><span class="p">,</span> <span class="n">frequency</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="drawdown-and-time-under-water">
<h4>Drawdown and Time Under Water<a class="headerlink" href="#drawdown-and-time-under-water" title="Permalink to this heading">¶</a></h4>
<p>Intuitively, a drawdown is the maximum loss suffered by an investment between two consecutive high-watermarks.</p>
<p>The time under water is the time elapsed between a high watermark and the moment the PnL (profit and loss) exceeds the previous maximum PnL.</p>
<p>Input a series of cumulated returns, or account balance. Can be in dollars or other currency, then the function returns the respective drawdowns.</p>
<p>The function returns two series:</p>
<ol class="arabic simple">
<li><p>Drawdown series index is time of a high watermark and the drawdown value.</p></li>
<li><p>Time under water index is time of a high watermark and how much time passed till next high watermark is reached, in years. Also includes time between the last high watermark and last observation in returns as the last Time under water element. Without this element the estimations of Time under water can be biased.</p></li>
</ol>
<section id="id7">
<h5>Example<a class="headerlink" href="#id7" title="Permalink to this heading">¶</a></h5>
<p>An example showing how Drawdown and Time Under Water function is used with account data in dollars:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.backtest_statistics</span> <span class="kn">import</span> <span class="n">drawdown_and_time_under_water</span>

<span class="n">drawdown</span><span class="p">,</span> <span class="n">tuw</span> <span class="o">=</span> <span class="n">drawdown_and_time_under_water</span><span class="p">(</span><span class="n">returns</span><span class="p">,</span> <span class="n">dollars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="average-holding-period">
<h4>Average Holding Period<a class="headerlink" href="#average-holding-period" title="Permalink to this heading">¶</a></h4>
<p>Parameters of the algorithm are calculated as follows:</p>
<ol class="arabic simple">
<li><p>When the size of the position is increasing</p></li>
</ol>
<p>Updating EntryTime - time when a trade was opened, adjusted by increases in positions. This takes into account the weight
of the position increase.</p>
<div class="math notranslate nohighlight">
\[EntTime_{0} = \frac{EntTime_{-1}*Weight_{-1} + TimeSinceTradeStart*(Weight_{0}-Weight_{-1})}{Weight_{0}}\]</div>
<ol class="arabic simple" start="2">
<li><p>When the size of a bet is decreasing.</p></li>
</ol>
<p>Capturing the <span class="math notranslate nohighlight">\(HoldingTime = (EntryTime - CurrentTime)\)</span> as well as <span class="math notranslate nohighlight">\(Weight\)</span> of the closed position.
If entire position is closed, setting <span class="math notranslate nohighlight">\(EntryTime\)</span> to <span class="math notranslate nohighlight">\(CurrentTime\)</span>.</p>
<ol class="arabic simple" start="3">
<li><p>Finally, calculating, using values captured in step 2.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[AverageHoldingTime = \frac{\sum_{i}(HoldingTime_{i}*Weight_{i})}{\sum_{i}Weight_{i}}\]</div>
<p>If no closed trades in the series, output is <span class="math notranslate nohighlight">\(nan\)</span></p>
<section id="id8">
<h5>Example<a class="headerlink" href="#id8" title="Permalink to this heading">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.backtest_statistics</span> <span class="kn">import</span> <span class="n">average_holding_period</span>

<span class="n">avg_holding_period</span> <span class="o">=</span> <span class="n">average_holding_period</span><span class="p">(</span><span class="n">target_positions</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="flattening-and-flips">
<h4>Flattening and Flips<a class="headerlink" href="#flattening-and-flips" title="Permalink to this heading">¶</a></h4>
<p>Points of Flipping: When target position changes sign (For example, changing from 1.5 (long position) to -0.5 (short position) on the next timestamp)</p>
<p>Points of Flattening: When target position changes from nonzero to zero (For example, changing from 1.5 (long position) to 0 (no positions) on the next timestamp)</p>
<section id="id9">
<h5>Example<a class="headerlink" href="#id9" title="Permalink to this heading">¶</a></h5>
<p>An example showing how Flattening and Flips function is used can be seen below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlfinlab.backtest_statistics</span> <span class="kn">import</span> <span class="n">timing_of_flattening_and_flips</span>

<span class="n">flattening_and_flips_timestamps</span> <span class="o">=</span> <span class="n">timing_of_flattening_and_flips</span><span class="p">(</span><span class="n">target_positions</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="research-notebooks">
<h4>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h4>
<p>The following research notebooks can be used to better understand how the statistics within this module can be used on
real data.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Backtest%20Statistics/Chapter14_BacktestStatistics.ipynb">Chapter 14 Exercise Notebook</a></p></li>
</ul>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-portfolio_optimisation/risk_metrics"></span><span class="target" id="portfolio-optimisation-risk-metrics"></span><section id="risk-metrics">
<h3>Risk Metrics<a class="headerlink" href="#risk-metrics" title="Permalink to this heading">¶</a></h3>
<p>The RiskMetrics class contains functions for calculation of common risk metrics used by investment professionals.
The list of supported metrics will grow with future updates of the package.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><h4> Underlying Literature </h4></p>
<p>The following sources elaborate extensively on the topic:</p>
<ul class="simple">
<li><p><strong>Portfolio Optimization with Drawdown Constraints</strong> <em>by</em> Alexei Chekhlov, Stanislav Uryasev, Michael Zabarankin <a class="reference external" href="https://www.ise.ufl.edu/uryasev/files/2011/11/drawdown.pdf">available here</a>. <em>Introduces a CDaR measure and compares it to the CVaR measure.</em></p></li>
</ul>
</div>
<section id="supported-metrics">
<h4>Supported Metrics<a class="headerlink" href="#supported-metrics" title="Permalink to this heading">¶</a></h4>
<section id="variance">
<h5>Variance<a class="headerlink" href="#variance" title="Permalink to this heading">¶</a></h5>
<p>This measure can be used to compare portfolios based on estimations of the volatility of returns.</p>
<p>The Variance of a portfolio is calculated as follows:</p>
<div class="math notranslate nohighlight">
\[\sigma^{2} = w^{T}\sum w\]</div>
<p>where <span class="math notranslate nohighlight">\(w\)</span> is the vector of weights for instruments in a portfolio, and <span class="math notranslate nohighlight">\(\sum\)</span> is a covariance matrix of assets in a portfolio. Result <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> is a scalar.</p>
</section>
<section id="value-at-risk-var">
<h5>Value at Risk (VaR)<a class="headerlink" href="#value-at-risk-var" title="Permalink to this heading">¶</a></h5>
<p>This measure can be used to compare portfolios based on the amount of investments that can be lost in the next observation,
assuming the returns for assets follow a multivariate normal distribution.</p>
<p>The Value at Risk of a portfolio is calculated as follows:</p>
<div class="math notranslate nohighlight">
\[VaR = Quantile_{\alpha}(R)\]</div>
<p>Where <span class="math notranslate nohighlight">\(\alpha\)</span> is the confidence level to use, and the <span class="math notranslate nohighlight">\(R\)</span> is a set of returns of a portfolio.</p>
<p>VaR of <span class="math notranslate nohighlight">\(0.15\)</span> at <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span> level means that with a <span class="math notranslate nohighlight">\(5\%\)</span> probability the portfolio will
decrease by <span class="math notranslate nohighlight">\(15\%\)</span> on the next observation.</p>
</section>
<section id="expected-shortfall-cvar">
<h5>Expected Shortfall (CVaR)<a class="headerlink" href="#expected-shortfall-cvar" title="Permalink to this heading">¶</a></h5>
<p>This measure can be used to compare portfolios based on the average amount of investments that can be lost in a
worst-case scenario, assuming the returns for assets follow a multivariate normal distribution.</p>
<p>The Expected Shortfall of a portfolio is calculated as follows:</p>
<div class="math notranslate nohighlight">
\[CVaR = E[{R, R &lt; Quantile_{\alpha}(R)}]\]</div>
<p>Where <span class="math notranslate nohighlight">\(\alpha\)</span> is the confidence level to use, and the <span class="math notranslate nohighlight">\(R\)</span> is a set of returns of a portfolio.</p>
<p>CVaR of <span class="math notranslate nohighlight">\(0.15\)</span> at <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span> level means that for <span class="math notranslate nohighlight">\(5\%\)</span> worst cases of returns observations
the average loss of the portfolio value is <span class="math notranslate nohighlight">\(15\%\)</span> per observation.</p>
<p>This picture from Y. Vardanyan demonstrates the differences between the VaR and the CVaR concepts:</p>
<a class="reference internal image-reference" href="_images/var_cvar_concepts.png"><img alt="_images/var_cvar_concepts.png" class="align-center" src="_images/var_cvar_concepts.png" style="width: 648.0px; height: 292.0px;" /></a>
</section>
<section id="conditional-drawdown-at-risk-cdar">
<h5>Conditional Drawdown at Risk (CDaR)<a class="headerlink" href="#conditional-drawdown-at-risk-cdar" title="Permalink to this heading">¶</a></h5>
<p>This measure can be used to compare portfolios based on the average amount of a portfolio drawdown in a
worst-case scenario, assuming the drawdowns follow a normal distribution.</p>
<p>The Expected Shortfall of a portfolio is calculated as follows:</p>
<div class="math notranslate nohighlight">
\begin{align*}
DD_{t} = max_{0 \le \tau \le t}\{w_{\tau}\} - w_{t}
\end{align*}

\begin{align*}
CDaR = E[{DD_{t}, DD_{t} &gt; Quantile_{\alpha}(DD)}]
\end{align*}</div><p>Where <span class="math notranslate nohighlight">\(\alpha\)</span> is the confidence level to use, <span class="math notranslate nohighlight">\(w_{t}\)</span> is the price of a portfolio at time <span class="math notranslate nohighlight">\(t\)</span>,
and <span class="math notranslate nohighlight">\(DD_{t}\)</span> is the maximum historical drawdown up to time <span class="math notranslate nohighlight">\(t\)</span> .</p>
<p>CDaR of <span class="math notranslate nohighlight">\(0.15\)</span> at <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span> level means that for <span class="math notranslate nohighlight">\(5\%\)</span> worst cases of historical portfolio drawdowns,
the average drawdown is <span class="math notranslate nohighlight">\(0.15\)</span> units in which the portfolio price is measured.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p>This risk metric is described in more detail in the work <strong>Portfolio Optimization with Drawdown Constraints</strong> <a class="reference external" href="https://www.ise.ufl.edu/uryasev/files/2011/11/drawdown.pdf">available here</a>.</p></li>
<li><p>VaR, CVaR and CDaR metrics can also be used for individual assets.</p></li>
</ul>
</div>
</section>
</section>
<section id="implementation">
<h4>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h4>
</section>
<section id="example-code">
<h4>Example Code<a class="headerlink" href="#example-code" title="Permalink to this heading">¶</a></h4>
<p>Below is an example of how to use the package functions to calculate risk metrics for a portfolio.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.portfolio_optimization</span> <span class="kn">import</span> <span class="n">RiskMetrics</span>

<span class="c1"># Import dataframe of returns for assets in a portfolio</span>
<span class="n">assets_returns</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">DATA_PATH</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Calculate empirical covariance of assets</span>
<span class="n">assets_cov</span> <span class="o">=</span> <span class="n">assets_returns</span><span class="o">.</span><span class="n">cov</span><span class="p">()</span>

<span class="c1"># Set weights for assets in a portfolio</span>
<span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]</span>

<span class="c1"># Pick a confidence interval</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>

<span class="c1"># Class that contains needed functions</span>
<span class="n">risk_met</span> <span class="o">=</span> <span class="n">RiskMetrics</span><span class="p">()</span>

<span class="c1"># Calculate Variance</span>
<span class="n">Var</span> <span class="o">=</span> <span class="n">risk_met</span><span class="o">.</span><span class="n">calculate_variance</span><span class="p">(</span><span class="n">assets_cov</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

<span class="c1"># Calculate Value at Risk of the first asset</span>
<span class="n">VaR</span> <span class="o">=</span> <span class="n">risk_met</span><span class="o">.</span><span class="n">calculate_value_at_risk</span><span class="p">(</span><span class="n">assets_returns</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">alpha</span><span class="p">)</span>

<span class="c1"># Calculate Expected Shortfall</span>
<span class="n">CVaR</span> <span class="o">=</span> <span class="n">risk_met</span><span class="o">.</span><span class="n">calculate_expected_shortfall</span><span class="p">(</span><span class="n">assets_returns</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">alpha</span><span class="p">)</span>

<span class="c1"># Calculate Conditional Drawdown at Risk</span>
<span class="n">CDaR</span> <span class="o">=</span> <span class="n">risk_met</span><span class="o">.</span><span class="n">calculate_conditional_drawdown_risk</span><span class="p">(</span><span class="n">assets_returns</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">alpha</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<span id="document-portfolio_optimisation/returns_estimation"></span><section id="returns-estimators">
<span id="portfolio-optimisation-returns-estimation"></span><h3>Returns Estimators<a class="headerlink" href="#returns-estimators" title="Permalink to this heading">¶</a></h3>
<p>Accurate estimation of historical asset returns is one of the most important aspects of portfolio optimisation. However, it is
also one of the most difficult to calculate since estimated returns do not correctly reflect the true underlying
returns of a portfolio/asset. MlFinLab’s <code class="xref py py-mod docutils literal notranslate"><span class="pre">ReturnsEstimators</span></code> class provides functions to estimate mean asset returns.
Currently, it is still in active development and we will keep adding new methods to it.</p>
<section id="supported-estimators">
<h4>Supported Estimators<a class="headerlink" href="#supported-estimators" title="Permalink to this heading">¶</a></h4>
<section id="simple-returns">
<h5>Simple returns<a class="headerlink" href="#simple-returns" title="Permalink to this heading">¶</a></h5>
<p>The <cite>calculate_returns</cite> function allows calculating a dataframe of returns from a dataframe of prices.
The calculation is done in the following way:</p>
<div class="math notranslate nohighlight">
\[R_{t} = \frac{P_{t}-P_{t-1}}{P_{t-1}}\]</div>
<p>Where <span class="math notranslate nohighlight">\(R_{t}\)</span> is the return for <span class="math notranslate nohighlight">\(t\)</span> -th observation, and <span class="math notranslate nohighlight">\(P_{t}\)</span> is the price for <span class="math notranslate nohighlight">\(t\)</span> -th observation.</p>
</section>
<section id="annualized-mean-historical-returns">
<h5>Annualized mean historical returns<a class="headerlink" href="#annualized-mean-historical-returns" title="Permalink to this heading">¶</a></h5>
<p>The <cite>calculate_mean_historical_returns</cite> function allows calculating a mean annual return for every element in a dataframe of prices.
The calculation is done in the following way:</p>
<div class="math notranslate nohighlight">
\begin{align*}
R_{t} &amp;= \frac{P_{t}-P_{t-1}}{P_{t-1}}
\end{align*}

\begin{align*}
AnnualizedMeanReturn &amp;= \frac{\sum_{t=0}^{T}{R_{t}}}{T} * N
\end{align*}</div><p>Where <span class="math notranslate nohighlight">\(R_{t}\)</span> is the return for <span class="math notranslate nohighlight">\(t\)</span> -th observation, and <span class="math notranslate nohighlight">\(P_{t}\)</span> is the price for <span class="math notranslate nohighlight">\(t\)</span> -th observation,
<span class="math notranslate nohighlight">\(T\)</span> is the total number of observations, <span class="math notranslate nohighlight">\(N\)</span> is an average number of observations in a year.</p>
</section>
<section id="exponentially-weighted-annualized-mean-of-historical-returns">
<h5>Exponentially-weighted annualized mean of historical returns<a class="headerlink" href="#exponentially-weighted-annualized-mean-of-historical-returns" title="Permalink to this heading">¶</a></h5>
<p>The <cite>calculate_exponential_historical_returns</cite> function allows calculating the exponentially-weighted mean annual return for every element in a dataframe of prices.
The calculation is done in the following way:</p>
<div class="math notranslate nohighlight">
\begin{align*}
R_{t} = \frac{P_{t}-P_{t-1}}{P_{t-1}}
\end{align*}

\begin{align*}
Decay = \frac{2}{span+1}
\end{align*}

\begin{align*}
EWMA(R)_{t} = ((R_{t} - R_{t-1}) * Decay) + R_{t-1}
\end{align*}

\begin{align*}
ExponentialAnnualizedMeanReturn_{Decay} = EWMA(R)_{T} * N
\end{align*}</div><p>Where <span class="math notranslate nohighlight">\(R_{t}\)</span> is the return for <span class="math notranslate nohighlight">\(t\)</span> -th observation, <span class="math notranslate nohighlight">\(P_{t}\)</span> is the price for <span class="math notranslate nohighlight">\(t\)</span> -th observation,
<span class="math notranslate nohighlight">\(T\)</span> is the total number of observations, <span class="math notranslate nohighlight">\(N\)</span> is an average number of observations in a year, <span class="math notranslate nohighlight">\(EWMA(R)_{t}\)</span> is the
<span class="math notranslate nohighlight">\(t\)</span> -th observation of exponentially-weighted moving average of <span class="math notranslate nohighlight">\(R\)</span> .</p>
</section>
</section>
<section id="implementation">
<h4>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h4>
</section>
<section id="example-code">
<h4>Example Code<a class="headerlink" href="#example-code" title="Permalink to this heading">¶</a></h4>
<p>Below is an example of how to use the package functions to calculate various estimators of returns for a portfolio.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.portfolio_optimization</span> <span class="kn">import</span> <span class="n">ReturnsEstimators</span>

<span class="c1"># Import dataframe of prices for assets in a portfolio</span>
<span class="n">asset_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">DATA_PATH</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Class that contains needed functions</span>
<span class="n">ret_est</span> <span class="o">=</span> <span class="n">ReturnsEstimators</span><span class="p">()</span>

<span class="c1"># Calculate simple returns</span>
<span class="n">assets_returns</span> <span class="o">=</span> <span class="n">ret_est</span><span class="o">.</span><span class="n">calculate_returns</span><span class="p">(</span><span class="n">asset_prices</span><span class="p">)</span>

<span class="c1"># Calculate annualised mean historical returns for daily data</span>
<span class="n">assets_annual_returns</span> <span class="o">=</span> <span class="n">ret_est</span><span class="o">.</span><span class="n">calculate_mean_historical_returns</span><span class="p">(</span><span class="n">asset_prices</span><span class="p">,</span> <span class="n">frequency</span><span class="o">=</span><span class="mi">252</span><span class="p">)</span>

<span class="c1"># Calculate exponentially-weighted annualized mean of historical returns for daily data and span of 200</span>
<span class="n">assets_exp_annual_returns</span> <span class="o">=</span> <span class="n">ret_est</span><span class="o">.</span><span class="n">calculate_exponential_historical_returns</span><span class="p">(</span><span class="n">asset_prices</span><span class="p">,</span>
                                                                             <span class="n">frequency</span><span class="o">=</span><span class="mi">252</span><span class="p">,</span>
                                                                             <span class="n">span</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<span id="document-portfolio_optimisation/risk_estimators"></span><span class="target" id="portfolio-optimisation-risk-estimators"></span><section id="risk-estimators">
<h3>Risk Estimators<a class="headerlink" href="#risk-estimators" title="Permalink to this heading">¶</a></h3>
<p>Risk is a very important part of finance and the performance of large number of investment strategies are dependent on the
efficient estimation of underlying portfolio risk. There are different ways of representing risk but the most widely used is a
covariance matrix. This means that an accurate calculation of the covariances is essential for an accurate representation of risk.
This class provides functions for calculating different types of covariance matrices, de-noising, de-toning and other helpful methods.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><h4> Underlying Literature </h4></p>
<p>The following sources elaborate extensively on the topic:</p>
<ul class="simple">
<li><p><strong>Scikit-learn User Guide on Covariance estimation</strong> <a class="reference external" href="https://scikit-learn.org/stable/modules/covariance.html#robust-covariance">available here</a>. <em>Describes the algorithms of covariance matrix estimators in more detail.</em></p></li>
<li><p><strong>Minimum covariance determinant</strong> <em>by</em> Mia Hubert <em>and</em> Michiel Debruyne <a class="reference external" href="https://wis.kuleuven.be/stat/robust/papers/2010/wire-mcd.pdf">available here</a>. <em>Detailed description of the minimum covariance determinant (MCD) estimator.</em></p></li>
<li><p><strong>A well-conditioned estimator for large-dimensional covariance matrices</strong> <em>by</em> Olivier Ledoit <em>and</em> Michael <a class="reference external" href="http://perso.ens-lyon.fr/patrick.flandrin/LedoitWolf_JMA2004.pdf">available here</a>. <em>Introduces the Ledoit-Wolf shrinkage method.</em></p></li>
<li><p><strong>Shrinkage Algorithms for MMSE Covariance Estimation</strong> <em>by</em> Y. Chen, A. Wiesel, Y.C. Eldar and A.O. Hero <a class="reference external" href="https://webee.technion.ac.il/people/YoninaEldar/104.pdf">available here</a>. <em>Introduces the Oracle Approximating shrinkage method.</em></p></li>
<li><p><strong>Minimum Downside Volatility Indices</strong> <em>by</em> Solactive AG - German Index Engineering <a class="reference external" href="https://www.solactive.com/wp-content/uploads/2018/04/Solactive_Minimum-Downside-Volatility-Indices.pdf">available here</a>. <em>Describes examples of use of the Semi-Covariance matrix.</em></p></li>
<li><p><strong>Financial applications of random matrix theory: Old laces and new pieces</strong> <em>by</em> Potter M., J.P. Bouchaud, L. Laloux <a class="reference external" href="https://arxiv.org/abs/physics/0507111">available here</a>. <em>Describes the process of de-noising of the covariance matrix.</em></p></li>
<li><p><strong>A Robust Estimator of the Efficient Frontier</strong> <em>by</em> Marcos Lopez de Prado <a class="reference external" href="https://papers.ssrn.com/sol3/abstract_id=3469961">available here</a>. <em>Describes the Constant Residual Eigenvalue Method for De-noising Covariance/Correlation Matrix.</em></p></li>
<li><p><strong>Machine Learning for Asset Managers</strong> <em>by</em> Marcos Lopez de Prado <a class="reference external" href="https://www.cambridge.org/core/books/machine-learning-for-asset-managers/6D9211305EA2E425D33A9F38D0AE3545">available here</a>. <em>Describes the Targeted Shrinkage De-noising and the De-toning methods for Covariance/Correlation Matrices.</em></p></li>
</ul>
</div>
<section id="minimum-covariance-determinant">
<h4>Minimum Covariance Determinant<a class="headerlink" href="#minimum-covariance-determinant" title="Permalink to this heading">¶</a></h4>
<p>Minimum Covariance Determinant (MCD) is a robust estimator of covariance that was introduced by P.J. Rousseeuw.</p>
<p>Following the <strong>Scikit-learn User Guide on Covariance estimation</strong>:</p>
<p>“The outliers are appearing in real data sets and seriously affect the Empirical covariance estimator and the Covariance estimators with shrinkage.
For this reason, a robust covariance estimator is needed in order to discard/downweight the outliers in the data”.</p>
<p>“The basic idea of the algorithm is to find a set of observations that are not outliers and compute their empirical covariance matrix,
which is then rescaled to compensate for the performed selection of observations”.</p>
<p>Our method is a wrapper for the sklearn MinCovDet class. For more details about the function and its parameters, please
visit <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.covariance.MinCovDet.html">sklearn documentation</a>.</p>
<section id="implementation">
<h5>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h5>
</section>
</section>
<hr class="docutils" />
<section id="maximum-likelihood-covariance-estimator-empirical-covariance">
<h4>Maximum Likelihood Covariance Estimator (Empirical Covariance)<a class="headerlink" href="#maximum-likelihood-covariance-estimator-empirical-covariance" title="Permalink to this heading">¶</a></h4>
<p>Maximum Likelihood Estimator of a sample is an unbiased estimator of the corresponding population’s covariance matrix.</p>
<p>Description of the Empirical Covariance according to the <strong>Scikit-learn User Guide on Covariance estimation</strong>:</p>
<p>“The covariance matrix of a data set is known to be well approximated by the classical maximum likelihood estimator,
provided the number of observations is large enough compared to the number of features (the variables describing the
observations). More precisely, the Maximum Likelihood Estimator of a sample is an unbiased estimator of the corresponding
population’s covariance matrix”.</p>
<p>Our method is a wrapper for the sklearn EmpiricalCovariance class. For more details about the function and its parameters,
please visit <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EmpiricalCovariance.html">sklearn documentation</a>.</p>
<section id="id1">
<h5>Implementation<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h5>
</section>
</section>
<hr class="docutils" />
<section id="covariance-estimator-with-shrinkage">
<h4>Covariance Estimator with Shrinkage<a class="headerlink" href="#covariance-estimator-with-shrinkage" title="Permalink to this heading">¶</a></h4>
<p>Shrinkage allows one to avoid the inability to invert the covariance matrix due to numerical reasons. Shrinkage consists
of reducing the ratio between the smallest and the largest eigenvalues of the empirical covariance matrix.</p>
<p>The following Shrinkage methods are supported:</p>
<ul class="simple">
<li><p>Basic shrinkage</p></li>
<li><p>Ledoit-Wolf shrinkage</p></li>
<li><p>Oracle Approximating shrinkage</p></li>
</ul>
<p><strong>Basic shrinkage</strong></p>
<p>Following the <strong>Scikit-learn User Guide on Covariance estimation</strong>:</p>
<p>“This shrinkage is done by shifting every eigenvalue according to a given offset, which is equivalent to finding the l2-penalized Maximum Likelihood Estimator of the covariance matrix”.</p>
<p>“Shrinkage boils down to a simple a convex transformation”:</p>
<div class="math notranslate nohighlight">
\[\sum_{shrunk} = (1 - \alpha)\sum_{unshrunk} + \alpha\frac{Tr \sum_{unshrunk}}{p}Id\]</div>
<p>“The amount of shrinkage <span class="math notranslate nohighlight">\(\alpha\)</span> is setting a trade-off between bias and variance”.</p>
<p><strong>Ledoit-Wolf shrinkage</strong></p>
<p>“The Ledoit-Wolf shrinkage is based on computing the optimal shrinkage coefficient <span class="math notranslate nohighlight">\(\alpha\)</span> that minimizes the Mean Squared Error between the estimated and the real covariance matrix”.</p>
<p><strong>Oracle Approximating shrinkage</strong></p>
<p>“Assuming that the data are Gaussian distributed, Chen et al. derived a formula aimed at choosing a shrinkage coefficient <span class="math notranslate nohighlight">\(\alpha\)</span>
that yields a smaller Mean Squared Error than the one given by Ledoit and Wolf’s formula”.</p>
<p>“The resulting estimator is known as the Oracle Shrinkage Approximating estimator of the covariance”.</p>
<p>Our methods here are wrappers for the sklearn ShrunkCovariance, LedoitWolf, and OAS classes.
For more details about the function and its parameters, please visit <a class="reference external" href="https://scikit-learn.org/stable/modules/covariance.html#shrunk-covariance">sklearn documentation</a>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Shrinkage methods are described in greater detail in the works listed in the introduction.</p>
</div>
<section id="id2">
<h5>Implementation<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h5>
</section>
</section>
<hr class="docutils" />
<section id="semi-covariance-matrix">
<h4>Semi-Covariance Matrix<a class="headerlink" href="#semi-covariance-matrix" title="Permalink to this heading">¶</a></h4>
<p>Semi-Covariance matrix is used to measure the downside volatility of a portfolio and can be used as a measure to minimize it.
This metric also allows measuring the volatility of returns below a specific threshold.</p>
<p>According to the <strong>Minimum Downside Volatility Indices</strong> paper:</p>
<p>“Each element in the Semi-Covariance matrix is calculated as:</p>
<div class="math notranslate nohighlight">
\[SemiCov_{ij} = \frac{1}{T}\sum_{t=1}^{T}[Min(R_{i,t}-B,0)*Min(R_{j,t}-B,0)]\]</div>
<p>where <span class="math notranslate nohighlight">\(T\)</span> is the number of observations, <span class="math notranslate nohighlight">\(R_{i,t}\)</span> is the return of an asset <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>, and <span class="math notranslate nohighlight">\(B\)</span> is the threshold return.
If the <span class="math notranslate nohighlight">\(B\)</span> is set to zero, the volatility of negative returns is measured.”</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>An example of Semi-Covariance usage can be found <a class="reference external" href="https://www.solactive.com/wp-content/uploads/2018/04/Solactive_Minimum-Downside-Volatility-Indices.pdf">here</a>.</p>
</div>
<section id="id3">
<h5>Implementation<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h5>
</section>
</section>
<hr class="docutils" />
<section id="exponentially-weighted-covariance-matrix">
<h4>Exponentially-Weighted Covariance Matrix<a class="headerlink" href="#exponentially-weighted-covariance-matrix" title="Permalink to this heading">¶</a></h4>
<p>Each element in the Exponentially-weighted Covariance matrix is the last element from an exponentially weighted moving average
series based on series of covariances between returns of the corresponding assets. It’s used to give greater weight to most
relevant observations in computing the covariance.</p>
<p>Each element is calculated as follows:</p>
<div class="math notranslate nohighlight">
\begin{align*}
\sum_{i,j}^{t} = (R_{i}^{t} - Mean(R_{i})) * (R_{j}^{t} - Mean(R_{j}))
\end{align*}

\begin{align*}
Decay = \frac{2}{span+1}
\end{align*}

\begin{align*}
EWMA(\sum_{i,j})_{t} = ((\sum_{i,j}^{t} - \sum_{i,j}^{t-1}) * Decay) + \sum_{i,j}^{t-1}
\end{align*}

\begin{align*}
ExponentialCovariance_{i,j (Decay)} = EWMA(\sum)_{T}
\end{align*}</div><p>Where <span class="math notranslate nohighlight">\(R_{i}^{t}\)</span> is the return of <span class="math notranslate nohighlight">\(i^{th}\)</span> asset for <span class="math notranslate nohighlight">\(t^{th}\)</span> observation,
<span class="math notranslate nohighlight">\(T\)</span> is the total number of observations, <span class="math notranslate nohighlight">\(\sum_{i,j}\)</span> is the series of covariances between <span class="math notranslate nohighlight">\(i^{th}\)</span>
and <span class="math notranslate nohighlight">\(j^{th}\)</span> asset, <span class="math notranslate nohighlight">\(EWMA(\sum)_{t}\)</span> is the <span class="math notranslate nohighlight">\(t^{th}\)</span> observation of exponentially-weighted
moving average of <span class="math notranslate nohighlight">\(\sum\)</span>.</p>
<section id="id4">
<h5>Implementation<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h5>
</section>
</section>
<hr class="docutils" />
<section id="de-noising-and-de-toning-covariance-correlation-matrix">
<h4>De-noising and De-toning Covariance/Correlation Matrix<a class="headerlink" href="#de-noising-and-de-toning-covariance-correlation-matrix" title="Permalink to this heading">¶</a></h4>
<p>Two methods for de-noising are implemented in this module:</p>
<ul class="simple">
<li><p>Constant Residual Eigenvalue Method</p></li>
<li><p>Targeted Shrinkage</p></li>
</ul>
<section id="constant-residual-eigenvalue-de-noising-method">
<h5>Constant Residual Eigenvalue De-noising Method<a class="headerlink" href="#constant-residual-eigenvalue-de-noising-method" title="Permalink to this heading">¶</a></h5>
<p>The main idea behind the Constant Residual Eigenvalue de-noising method is to separate the noise-related eigenvalues from
the signal-related ones. This is achieved by fitting the Marcenko-Pastur distribution of the empirical distribution of
eigenvalues using a Kernel Density Estimate (KDE).</p>
<p>The de-noising function works as follows:</p>
<ul class="simple">
<li><p>The given covariance matrix is transformed to the correlation matrix.</p></li>
<li><p>The eigenvalues and eigenvectors of the correlation matrix are calculated.</p></li>
<li><p>Using the Kernel Density Estimate algorithm a kernel of the eigenvalues is estimated.</p></li>
<li><p>The Marcenko-Pastur pdf is fitted to the KDE estimate using the variance as the parameter for the optimization.</p></li>
<li><p>From the obtained Marcenko-Pastur distribution, the maximum theoretical eigenvalue is calculated using the formula
from the <strong>Instability caused by noise</strong> part of <a class="reference external" href="https://papers.ssrn.com/sol3/abstract_id=3469961">A Robust Estimator of the Efficient Frontier paper</a>.</p></li>
<li><p>The eigenvalues in the set that are below the theoretical value are all set to their average value.
For example, we have a set of 5 eigenvalues sorted in the descending order ( <span class="math notranslate nohighlight">\(\lambda_1\)</span> … <span class="math notranslate nohighlight">\(\lambda_5\)</span> ),
3 of which are below the maximum theoretical value, then we set</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\lambda_3^{NEW} = \lambda_4^{NEW} = \lambda_5^{NEW} = \frac{\lambda_3^{OLD} + \lambda_4^{OLD} + \lambda_5^{OLD}}{3}\]</div>
<ul class="simple">
<li><p>Eigenvalues above the maximum theoretical value are left intact.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\lambda_1^{NEW} = \lambda_1^{OLD}\\\lambda_2^{NEW} = \lambda_2^{OLD}\end{aligned}\end{align} \]</div>
<ul class="simple">
<li><p>The new set of eigenvalues with the set of eigenvectors is used to obtain the new de-noised correlation matrix.
<span class="math notranslate nohighlight">\(\tilde{C}\)</span> is the de-noised correlation matrix, <span class="math notranslate nohighlight">\(W\)</span> is the eigenvectors matrix,
and <span class="math notranslate nohighlight">\(\Lambda\)</span> is the diagonal matrix with new eigenvalues.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\tilde{C} = W \Lambda W'\]</div>
<ul class="simple">
<li><p>To rescale <span class="math notranslate nohighlight">\(\tilde{C}\)</span> so that the main diagonal consists of 1s the following transformation is made.
This is how the final <span class="math notranslate nohighlight">\(C_{denoised}\)</span> is obtained.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[C_{denoised} = \tilde{C} [(diag[\tilde{C}])^\frac{1}{2}(diag[\tilde{C}])^{\frac{1}{2}'}]^{-1}\]</div>
<ul class="simple">
<li><p>The new correlation matrix is then transformed back to the new de-noised covariance matrix.</p></li>
</ul>
<p>(If the correlation matrix is given as an input, the first and the last steps of the algorithm are omitted)</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The Constant Residual Eigenvalue de-noising method is described in more detail in the work
<strong>A Robust Estimator of the Efficient Frontier</strong> <em>by</em> Marcos Lopez de Prado <a class="reference external" href="https://papers.ssrn.com/abstract_id=3469961">available here</a>.</p>
<p>Lopez de Prado suggests that this de-noising algorithm is preferable as it removes the noise while preserving the signal.</p>
</div>
</section>
<section id="targeted-shrinkage-de-noising">
<h5>Targeted Shrinkage De-noising<a class="headerlink" href="#targeted-shrinkage-de-noising" title="Permalink to this heading">¶</a></h5>
<p>The main idea behind the Targeted Shrinkage de-noising method is to shrink the eigenvectors/eigenvalues that are
noise-related. This is done by shrinking the correlation matrix calculated from noise-related eigenvectors/eigenvalues
and then adding the correlation matrix composed from signal-related eigenvectors/eigenvalues.</p>
<p>The de-noising function works as follows:</p>
<ul class="simple">
<li><p>The given covariance matrix is transformed to the correlation matrix.</p></li>
<li><p>The eigenvalues and eigenvectors of the correlation matrix are calculated and sorted in the descending order.</p></li>
<li><p>Using the Kernel Density Estimate algorithm a kernel of the eigenvalues is estimated.</p></li>
<li><p>The Marcenko-Pastur pdf is fitted to the KDE estimate using the variance as the parameter for the optimization.</p></li>
<li><p>From the obtained Marcenko-Pastur distribution, the maximum theoretical eigenvalue is calculated using the formula
from the <strong>Instability caused by noise</strong> part of <a class="reference external" href="https://papers.ssrn.com/sol3/abstract_id=3469961">A Robust Estimator of the Efficient Frontier</a>.</p></li>
<li><p>The correlation matrix composed from eigenvectors and eigenvalues related to noise (eigenvalues below the maximum
theoretical eigenvalue) is shrunk using the <span class="math notranslate nohighlight">\(\alpha\)</span> variable.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[C_n = \alpha W_n \Lambda_n W_n' + (1 - \alpha) diag[W_n \Lambda_n W_n']\]</div>
<ul class="simple">
<li><p>The shrinked noise correlation matrix is summed to the information correlation matrix.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}C_i = W_i \Lambda_i W_i'\\C_{denoised} = C_n + C_i\end{aligned}\end{align} \]</div>
<ul class="simple">
<li><p>The new correlation matrix is then transformed back to the new de-noised covariance matrix.</p></li>
</ul>
<p>(If the correlation matrix is given as an input, the first and the last steps of the algorithm are omitted)</p>
</section>
<section id="de-toning">
<h5>De-toning<a class="headerlink" href="#de-toning" title="Permalink to this heading">¶</a></h5>
<p>De-noised correlation matrix from the previous methods can also be de-toned by excluding a number of first
eigenvectors representing the market component.</p>
<p>According to Lopez de Prado:</p>
<p>“Financial correlation matrices usually incorporate a market component. The market component is characterized by the
first eigenvector, with loadings <span class="math notranslate nohighlight">\(W_{n,1} \approx N^{-\frac{1}{2}}, n = 1, ..., N.\)</span>
Accordingly, a market component affects every item of the covariance matrix. In the context of clustering
applications, it is useful to remove the market component, if it exists (a hypothesis that can be
tested statistically).”</p>
<p>“By removing the market component, we allow a greater portion of the correlation to be explained
by components that affect specific subsets of the securities. It is similar to removing a loud tone
that prevents us from hearing other sounds”</p>
<p>“The detoned correlation matrix is singular, as a result of eliminating (at least) one eigenvector.
This is not a problem for clustering applications, as most approaches do not require the invertibility
of the correlation matrix. Still, <strong>a detoned correlation matrix</strong> <span class="math notranslate nohighlight">\(C_{detoned}\)</span> <strong>cannot be used directly for</strong>
<strong>mean-variance portfolio optimization</strong>.”</p>
<p>The de-toning function works as follows:</p>
<ul class="simple">
<li><p>De-toning is applied on the de-noised correlation matrix.</p></li>
<li><p>The correlation matrix representing the market component is calculated from market component eigenvectors and eigenvalues
and then subtracted from the de-noised correlation matrix. This way the de-toned correlation matrix is obtained.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\hat{C} = C_{denoised} - W_m \Lambda_m W_m'\]</div>
<ul class="simple">
<li><p>De-toned correlation matrix <span class="math notranslate nohighlight">\(\hat{C}\)</span> is then rescaled so that the main diagonal consists of 1s</p></li>
</ul>
<div class="math notranslate nohighlight">
\[C_{detoned} = \hat{C} [(diag[\hat{C}])^\frac{1}{2}(diag[\hat{C}])^{\frac{1}{2}'}]^{-1}\]</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>For a more detailed description of de-noising and de-toning, please read Chapter 2 of the book
<strong>Machine Learning for Asset Managers</strong> <em>by</em> Marcos Lopez de Prado.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>This and above the methods are described in more detail in the Risk Estimators Notebook.</p>
</div>
</section>
<section id="id5">
<h5>Implementation<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h5>
</section>
</section>
<hr class="docutils" />
<section id="example-code">
<h4>Example Code<a class="headerlink" href="#example-code" title="Permalink to this heading">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mlfinlab.portfolio_optimization</span> <span class="kn">import</span> <span class="n">RiskEstimators</span><span class="p">,</span> <span class="n">ReturnsEstimators</span>

<span class="c1"># Import price data</span>
<span class="n">stock_returns</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">DATA_PATH</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Class that have needed functions</span>
<span class="n">risk_estimators</span> <span class="o">=</span> <span class="n">RiskEstimators</span><span class="p">()</span>
<span class="n">returns_estimators</span> <span class="o">=</span> <span class="n">ReturnsEstimators</span><span class="p">()</span>

<span class="c1"># Finding the MCD estimator on price data</span>
<span class="n">min_cov_det</span> <span class="o">=</span> <span class="n">risk_estimators</span><span class="o">.</span><span class="n">minimum_covariance_determinant</span><span class="p">(</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">price_data</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Finding the Empirical Covariance on price data</span>
<span class="n">empirical_cov</span> <span class="o">=</span> <span class="n">risk_estimators</span><span class="o">.</span><span class="n">empirical_covariance</span><span class="p">(</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">price_data</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Finding the Shrinked Covariances on price data with every method</span>
<span class="n">shrinked_cov</span> <span class="o">=</span> <span class="n">risk_estimators</span><span class="o">.</span><span class="n">shrinked_covariance</span><span class="p">(</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">price_data</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                   <span class="n">shrinkage_type</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="n">basic_shrinkage</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Finding the Semi-Covariance on price data</span>
<span class="n">semi_cov</span> <span class="o">=</span> <span class="n">risk_estimators</span><span class="o">.</span><span class="n">semi_covariance</span><span class="p">(</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">price_data</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">threshold_return</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Finding the Exponential Covariance on price data and span of 60</span>
<span class="n">exponential_cov</span> <span class="o">=</span> <span class="n">risk_estimators</span><span class="o">.</span><span class="n">exponential_covariance</span><span class="p">(</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">price_data</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                         <span class="n">window_span</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>

<span class="c1"># Relation of number of observations T to the number of variables N (T/N)</span>
<span class="n">tn_relation</span> <span class="o">=</span> <span class="n">stock_prices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">stock_prices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># The bandwidth of the KDE kernel</span>
<span class="n">kde_bwidth</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="c1"># Series of returns from series of prices</span>
<span class="n">stock_returns</span> <span class="o">=</span> <span class="n">ret_est</span><span class="o">.</span><span class="n">calculate_returns</span><span class="p">(</span><span class="n">stock_prices</span><span class="p">)</span>

<span class="c1"># Finding the simple covariance matrix from a series of returns</span>
<span class="n">cov_matrix</span> <span class="o">=</span> <span class="n">stock_returns</span><span class="o">.</span><span class="n">cov</span><span class="p">()</span>

<span class="c1"># Finding the Constant Residual Eigenvalue De-noised Сovariance matrix</span>
<span class="n">const_resid_denoised</span> <span class="o">=</span> <span class="n">risk_estimators</span><span class="o">.</span><span class="n">denoise_covariance</span><span class="p">(</span><span class="n">cov_matrix</span><span class="p">,</span> <span class="n">tn_relation</span><span class="p">,</span>
                                                          <span class="n">denoise_method</span><span class="o">=</span><span class="s1">&#39;const_resid_eigen&#39;</span><span class="p">,</span>
                                                          <span class="n">detone</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde_bwidth</span><span class="o">=</span><span class="n">kde_bwidth</span><span class="p">)</span>

<span class="c1"># Finding the Targeted Shrinkage De-noised Сovariance matrix</span>
<span class="n">targ_shrink_denoised</span> <span class="o">=</span> <span class="n">risk_estimators</span><span class="o">.</span><span class="n">denoise_covariance</span><span class="p">(</span><span class="n">cov_matrix</span><span class="p">,</span> <span class="n">tn_relation</span><span class="p">,</span>
                                                          <span class="n">denoise_method</span><span class="o">=</span><span class="s1">&#39;target_shrink&#39;</span><span class="p">,</span>
                                                          <span class="n">detone</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde_bwidth</span><span class="o">=</span><span class="n">kde_bwidth</span><span class="p">)</span>

<span class="c1"># Finding the Constant Residual Eigenvalue De-noised and De-toned Сovariance matrix</span>
<span class="n">const_resid_detoned</span> <span class="o">=</span> <span class="n">risk_estimators</span><span class="o">.</span><span class="n">denoise_covariance</span><span class="p">(</span><span class="n">cov_matrix</span><span class="p">,</span> <span class="n">tn_relation</span><span class="p">,</span>
                                                         <span class="n">denoise_method</span><span class="o">=</span><span class="s1">&#39;const_resid_eigen&#39;</span><span class="p">,</span>
                                                         <span class="n">detone</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">market_component</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                                         <span class="n">kde_bwidth</span><span class="o">=</span><span class="n">kde_bwidth</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="research-notebooks">
<h4>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h4>
<p>The following research notebook can be used to better understand how the algorithms within this module can be used on real data.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Portfolio%20Optimisation%20Tutorials/Risk%20Estimators/RiskEstimators.ipynb">Risk Estimators Notebook</a></p></li>
</ul>
</section>
</section>
<span id="document-portfolio_optimisation/mean_variance"></span><span class="target" id="portfolio-optimisation-mean-variance"></span><div class="admonition note">
<p class="admonition-title">Note</p>
<p>The portfolio optimization module contains different algorithms that are used for asset allocation and optimising strategies.
Each algorithm is encapsulated in its own class and has a public method called <code class="docutils literal notranslate"><span class="pre">allocate()</span></code> which calculates the weight
allocations on the specific user data. This way, each implementation can be called in the same way and this makes it simple
for users to use them.</p>
</div>
<section id="mean-variance-optimisation">
<h3>Mean-Variance Optimisation<a class="headerlink" href="#mean-variance-optimisation" title="Permalink to this heading">¶</a></h3>
<p>Traditionally, portfolio optimization is nothing more than a simple mathematical optimization problem, where your objective is to
achieve optimal portfolio allocation bounded by some constraints. It can be mathematically expressed as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp; \underset{\mathbf{x}}{\text{max}} &amp; &amp; f(\mathbf{x}) \\
    &amp; \text{s.t.} &amp; &amp; g(\mathbf{x}) \leq 0 \\
    &amp;&amp;&amp; h(\mathbf{x}) = 0 \\
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(x \in R^n\)</span> and <span class="math notranslate nohighlight">\(h(x), g(x)\)</span> represent convex functions correlating to the equality and inequality constraints
respectively. Based on the mean-variance framework first developed by Harry Markowitz, a portfolio optimization problem can be
formulated as follows,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp; \underset{\mathbf{w}}{\text{min}} &amp; &amp; w^T\sum w \\
    &amp; \text{s.t.} &amp; &amp; \sum_{i=1}^{n}w_{i} = 1 \\
    &amp;&amp;&amp; \mu^Tw = \mu_t \\
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(w\)</span> refers to the set of weights for the portfolio assets, <span class="math notranslate nohighlight">\(\sum\)</span> is the covariance matrix of the assets,
<span class="math notranslate nohighlight">\(\mu\)</span> is the expected asset returns and <span class="math notranslate nohighlight">\(\mu_t\)</span> represents the target portfolio return of the investor. Note that this
represents a very basic (and a specific) use-case of portfolio allocation where the investor wants to minimse the portfolio risk
for a given target return. As the needs of an investor increase, the complexity of the problem also changes with different
objective functions and multitude of constraints governing the optimal set of weights.</p>
<p>The MeanVarianceOptimisation() class (MVO) provides a very flexible framework for many common portfolio allocation problems
encountered in practice. Users need to simply call a master function and by specifying the required parameters, the MVO class does
the hard work by utilising a quadratic optimiser and calculating the optimal set of weights based on the problem and constraints
specified.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><h4> The Quadratic Optimiser </h4>
Many mean-variance objective functions are typical quadratic optimization problems and can be solved by using a black-box
quadratic optimiser. We use <a class="reference external" href="https://www.cvxpy.org/index.html">cvxpy</a> as our quadratic optimiser instead of the more
frequently used <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/optimize.html">scipy.optimize</a>. This was a design choice for the
following reasons:</p>
<ul class="simple">
<li><p>The documentation of cvxpy is better than that of scipy, escpecially the parts related to optimization.</p></li>
<li><p>cvxpy’s code is much more readable and easier to understand.</p></li>
<li><p><strong>Note that cvxpy only supports convex optimization problems as opposed to scipy.optimise which can also tackle concave problems</strong>. Although this might seem as a downside, cvxpy raises clear error notifications if the problem is not convex and the required conditions are not met. This is very important for us as it ensures the solvability of an objective function - if there is no error from cvxpy’s side, the objective function is correct and is guaranteed to run till completion by the optimiser.</p></li>
</ul>
</div>
<section id="supported-portfolio-allocation-solutions">
<h4>Supported Portfolio Allocation Solutions<a class="headerlink" href="#supported-portfolio-allocation-solutions" title="Permalink to this heading">¶</a></h4>
<p>MlFinLab’s <code class="xref py py-mod docutils literal notranslate"><span class="pre">MeanVarianceOptimisation</span></code> class provide some common portfolio optimization problems out-of-the-box. In this section we go over a quick overview of
these:</p>
<section id="inverse-variance">
<h5>Inverse Variance<a class="headerlink" href="#inverse-variance" title="Permalink to this heading">¶</a></h5>
<p>For this solution, the diagonal of the covariance matrix is used for weights allocation.</p>
<div class="math notranslate nohighlight">
\[w_{i} = \frac{\sum^{-1}}{\sum_{j=1}^{N}(\sum_{j,j})^{-1}}\]</div>
<p>where <span class="math notranslate nohighlight">\(w_{i}\)</span> is the weight allocated to the <span class="math notranslate nohighlight">\(i^{th}\)</span> asset in a portfolio, <span class="math notranslate nohighlight">\(\sum_{i,i}\)</span> is the <span class="math notranslate nohighlight">\(i^{th}\)</span>
element on the main diagonal of the covariance matrix of elements in a portfolio and <span class="math notranslate nohighlight">\(N\)</span> is the number of elements in a
portfolio.</p>
<p><strong>Solution String:</strong> <code class="docutils literal notranslate"><span class="pre">inverse_variance</span></code></p>
</section>
<section id="minimum-volatility">
<h5>Minimum Volatility<a class="headerlink" href="#minimum-volatility" title="Permalink to this heading">¶</a></h5>
<p>For this solution, the objective is to generate a portfolio with the least variance. The following optimization problem is
being solved.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp; \underset{\mathbf{w}}{\text{minimise}} &amp; &amp; w^T\sum w \\
    &amp; \text{s.t.} &amp; &amp; \sum_{j=1}^{n}w_{j} = 1 \\
    &amp;&amp;&amp; w_{j} \geq 0, j=1,..,N
\end{align*}\end{split}\]</div>
<p><strong>Solution String:</strong> <code class="docutils literal notranslate"><span class="pre">min_volatility</span></code></p>
</section>
<section id="maximum-sharpe-ratio">
<h5>Maximum Sharpe Ratio<a class="headerlink" href="#maximum-sharpe-ratio" title="Permalink to this heading">¶</a></h5>
<p>For this solution, the objective is (as the name suggests) to maximise the Sharpe Ratio of your portfolio.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp; \underset{\mathbf{w}}{\text{maximise}} &amp; &amp; \frac{\mu^{T}w - R_{f}}{(w^{T}\sum w)^{1/2}} \\
    &amp; \text{s.t.} &amp; &amp; \sum_{j=1}^{n}w_{j} = 1 \\
    &amp;&amp;&amp; w_{j} \geq 0, j=1,..,N
\end{align*}\end{split}\]</div>
<p>A major problem with the above formulation is that the objective function is not convex and this presents a problem for cvxpy
which only accepts convex optimization problems. As a result, the problem can be transformed into an equivalent one, but with
a convex quadratic objective function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp; \underset{\mathbf{w}}{\text{minimise}} &amp; &amp; y^T\sum y \\
    &amp; \text{s.t.} &amp; &amp; (\mu^{T}w - R_{f})^{T}y = 1 \\
    &amp;&amp;&amp; \sum_{j=1}^{N}y_{j} = \kappa, \\
    &amp;&amp;&amp; \kappa \geq 0, \\
    &amp;&amp;&amp; w_{j} = \frac{y_j}{\kappa}, j=1,..,N
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(y\)</span> refer to the set of unscaled weights, <span class="math notranslate nohighlight">\(\kappa\)</span> is the scaling factor and the other symbols refer to
their usual meanings.</p>
<p><strong>Solution String:</strong> <code class="docutils literal notranslate"><span class="pre">max_sharpe</span></code></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><h4> Underlying Literature </h4>
The process of deriving this optimization problem from the standard maximising Sharpe ratio problem is described
in the notes <a class="reference external" href="http://people.stat.sc.edu/sshen/events/backtesting/reference/maximizing%20the%20sharpe%20ratio.pdf">IEOR 4500 Maximizing the Sharpe ratio</a>  from Columbia University.</p>
</div>
</section>
<section id="efficient-risk">
<h5>Efficient Risk<a class="headerlink" href="#efficient-risk" title="Permalink to this heading">¶</a></h5>
<p>For this solution, the objective is to minimise risk given a target return value by the investor. Note that the risk value for
such a portfolio will not be the minimum, which is achieved by the minimum-variance solution. However, the optimiser will find
the set of weights which efficiently allocate risk constrained by the provided target return, hence the name “efficient risk”.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp; \underset{\mathbf{w}}{\text{min}} &amp; &amp; w^T\sum w \\
    &amp; \text{s.t.} &amp; &amp; \mu^Tw = \mu_t\\
    &amp;&amp;&amp; \sum_{j=1}^{n}w_{j} = 1 \\
    &amp;&amp;&amp; w_{j} \geq 0, j=1,..,N \\
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu_t\)</span> is the target portfolio return set by the investor and the other symbols refer to their usual meanings.</p>
<p><strong>Solution String:</strong> <code class="docutils literal notranslate"><span class="pre">efficient_risk</span></code></p>
</section>
<section id="efficient-return">
<h5>Efficient Return<a class="headerlink" href="#efficient-return" title="Permalink to this heading">¶</a></h5>
<p>For this solution, the objective is to maximise the portfolio return given a target risk value by the investor. This is very
similar to the <em>efficient_risk</em> solution. The optimiser will find the set of weights which efficiently try to maximise return
constrained by the provided target risk, hence the name “efficient return”.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp; \underset{\mathbf{w}}{\text{max}} &amp; &amp; \mu^Tw \\
    &amp; \text{s.t.} &amp; &amp; w^T\sum w = \sigma^{2}_t\\
    &amp;&amp;&amp; \sum_{j=1}^{n}w_{j} = 1 \\
    &amp;&amp;&amp; w_{j} \geq 0, j=1,..,N \\
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma^{2}_t\)</span> is the target portfolio risk set by the investor and the other symbols refer to their usual meanings.</p>
<p><strong>Solution String:</strong> <code class="docutils literal notranslate"><span class="pre">efficient_return</span></code></p>
</section>
<section id="maximum-return-minimum-volatility">
<h5>Maximum Return - Minimum Volatility<a class="headerlink" href="#maximum-return-minimum-volatility" title="Permalink to this heading">¶</a></h5>
<p>This is often referred to as <em>quadratic risk utility.</em> The objective function consists of both the portfolio return and the risk.
Thus, minimising the objective relates to minimising the risk and correspondingly maximising the return. Here, <span class="math notranslate nohighlight">\(\lambda\)</span> is
the risk-aversion parameter which models the amount of risk the user is willing to take. A higher value means the investor will
have high defense against risk at the expense of lower returns and keeping a lower value will place higher emphasis on maximising
returns, neglecting the risk associated with it.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp; \underset{\mathbf{w}}{\text{min}} &amp; &amp;  \lambda * w^T\sum w - \mu^Tw\\
    &amp; \text{s.t.} &amp; &amp; \sum_{j=1}^{n}w_{j} = 1 \\
    &amp;&amp;&amp; w_{j} \geq 0, j=1,..,N \\
\end{align*}\end{split}\]</div>
<p><strong>Solution String:</strong> <code class="docutils literal notranslate"><span class="pre">max_return_min_volatility</span></code></p>
</section>
<section id="maximum-diversification">
<h5>Maximum Diversification<a class="headerlink" href="#maximum-diversification" title="Permalink to this heading">¶</a></h5>
<p>Maximum diversification portfolio tries to diversify the holdings across as many assets as possible. In the 2008 paper, <a class="reference external" href="https://blog.thinknewfound.com/2018/12/maximizing-diversification/#easy-footnote-bottom-1-6608">Toward Maximum Diversification</a>, the diversification ratio, <span class="math notranslate nohighlight">\(D\)</span>, of a portfolio, is defined as:</p>
<div class="math notranslate nohighlight">
\[D = \frac{w^{T}\sigma}{\sqrt{w^{T}\sum w}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma\)</span> is the vector of volatilities and <span class="math notranslate nohighlight">\(\sum\)</span> is the covariance matrix. The term in the denominator is the
volatility of the portfolio and the term in the numerator is the weighted average volatility of the assets. More diversification
within a portfolio decreases the denominator and leads to a higher diversification ratio. The corresponding objective function and
the constraints are:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp; \underset{\mathbf{w}}{\text{max}} &amp; &amp;  D\\
    &amp; \text{s.t.} &amp; &amp; \sum_{j=1}^{n}w_{j} = 1 \\
    &amp;&amp;&amp; w_{j} \geq 0, j=1,..,N \\
\end{align*}\end{split}\]</div>
<p><strong>Solution String:</strong> <code class="docutils literal notranslate"><span class="pre">max_diversification</span></code></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><h4> Underlying Literature </h4>
You can read more about maximum diversification portfolio in the following blog post on the website <em>Flirting with Models:</em> <a class="reference external" href="https://blog.thinknewfound.com/2018/12/maximizing-diversification/">Maximizing Diversification</a>.</p>
</div>
</section>
<section id="maximum-decorrelation">
<h5>Maximum Decorrelation<a class="headerlink" href="#maximum-decorrelation" title="Permalink to this heading">¶</a></h5>
<p>For this solution, the objective is to minimise the correlation between the assets of a portfolio</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp; \underset{\mathbf{w}}{\text{min}} &amp; &amp;  w^TA w\\
    &amp; \text{s.t.} &amp; &amp; \sum_{j=1}^{n}w_{j} = 1 \\
    &amp;&amp;&amp; w_{j} \geq 0, j=1,..,N \\
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(A\)</span> is the correlation matrix of assets. The Maximum Decorrelation portfolio is closely related to Minimum Variance and Maximum Diversification, but applies to the case where an investor believes all assets have similar returns and volatility, but heterogeneous correlations. It is a Minimum Variance optimization that is performed on the correlation matrix rather than the covariance matrix, <span class="math notranslate nohighlight">\(\sum\)</span>.</p>
<p><strong>Solution String:</strong> <code class="docutils literal notranslate"><span class="pre">max_decorrelation</span></code></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><h4> Underlying Literature </h4>
You can read more on maximum decorrelation portfolio in the following blog post: <a class="reference external" href="https://systematicedge.wordpress.com/2013/05/12/max-decorrelation-portfolio/">Max Decorrelation Portfolio</a>.</p>
</div>
</section>
</section>
<section id="creating-a-custom-portfolio-allocation">
<h4>Creating a Custom Portfolio Allocation<a class="headerlink" href="#creating-a-custom-portfolio-allocation" title="Permalink to this heading">¶</a></h4>
<p>For most of the users, the above solutions will be enough for their use-cases. However, we also provide a way for users to create
their custom portfolio problem. <strong>This includes complete flexibility to specify the input, optimization variables, objective function and the corresponding constraints</strong>. Let us go through the step-by-step process of formulating your own allocation problem:</p>
<section id="non-cvxpy-variables">
<h5>Non-CVXPY Variables<a class="headerlink" href="#non-cvxpy-variables" title="Permalink to this heading">¶</a></h5>
<p>The first step is to specify input variables not related to cvxpy (i.e. not defined as cvxpy variable objects, <code class="xref py py-mod docutils literal notranslate"><span class="pre">cvxpy.Variable</span></code>). This can include anything ranging from raw asset prices data to historical returns to integer or string variables. All data types are supported - <code class="docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">str</span></code>, <code class="docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">matrices/lists</span></code>, <code class="docutils literal notranslate"><span class="pre">Python</span> <span class="pre">lists</span></code>, <code class="docutils literal notranslate"><span class="pre">Pandas</span> <span class="pre">dataframe</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;stock_prices.csv&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s2">&quot;Date&quot;</span><span class="p">)</span>
<span class="n">non_cvxpy_variables</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;asset_prices&#39;</span><span class="p">:</span> <span class="n">data</span><span class="p">,</span>
        <span class="s1">&#39;num_assets&#39;</span><span class="p">:</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="s1">&#39;covariance&#39;</span><span class="p">:</span> <span class="n">data</span><span class="o">.</span><span class="n">cov</span><span class="p">(),</span>
        <span class="s1">&#39;asset_names&#39;</span><span class="p">:</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span>
        <span class="s1">&#39;expected_returns&#39;</span><span class="p">:</span> <span class="n">ReturnsEstimation</span><span class="p">()</span><span class="o">.</span><span class="n">calculate_mean_historical_returns</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In the above code example, we initialise a dataframe of historical stock prices and then define the dictionary containing all user
required input variables. <strong>The key of the dictionary is the variable name and the value is the Pythonic value you want to assign that variable.</strong></p>
</section>
<section id="cvxpy-variables">
<h5>CVXPY Variables<a class="headerlink" href="#cvxpy-variables" title="Permalink to this heading">¶</a></h5>
<p>The second step is to specify the cvxpy specific variables which are declared in the syntax required by cvxpy. You can include as
many new variables as you need by initialising a simple Python list with each declaration being a string. <strong>Each of these variables should be a</strong> <code class="xref py py-mod docutils literal notranslate"><span class="pre">cvxpy.Variable</span></code> <strong>object.</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cvxpy_variables</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s1">&#39;risk = cp.quad_form(weights, covariance)&#39;</span><span class="p">,</span>
        <span class="s1">&#39;portfolio_return = cp.matmul(weights, expected_returns)&#39;</span>
<span class="p">]</span>
</pre></div>
</div>
<p>Here, we are declaring two new cvxpy variables - <code class="xref py py-mod docutils literal notranslate"><span class="pre">risk</span></code> and <code class="xref py py-mod docutils literal notranslate"><span class="pre">portfolio_return</span></code>. Note that we are using non-cvxpy
variables - <code class="xref py py-mod docutils literal notranslate"><span class="pre">covariance</span></code> and <code class="xref py py-mod docutils literal notranslate"><span class="pre">expected_returns</span></code> - declared in the previous step to initialise the new ones.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><h4> Variable for Portfolio Weights </h4></p>
<p>Internally, the code declares a Python variable - <code class="xref py py-mod docutils literal notranslate"><span class="pre">weights</span></code> - for the final portfolio weights. We request you to use
this same variable name whenever you want to include it in one of your custom variable declarations. Refer to the above code
snippet for an example.</p>
<p><h4> Calling CVXPY </h4></p>
<p>Internally, cvxpy is imported as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cvxpy</span> <span class="k">as</span> <span class="nn">cp</span>
</pre></div>
</div>
<p>For creating any cvxpy specific variables, you need to reference the library as <code class="xref py py-mod docutils literal notranslate"><span class="pre">cp</span></code> otherwise the code will fail to
run and give you an error.</p>
</div>
</section>
<section id="custom-objective-function">
<h5>Custom Objective Function<a class="headerlink" href="#custom-objective-function" title="Permalink to this heading">¶</a></h5>
<p>The third step is to specify the objective function for our portfolio optimization problem. You need to simply pass a string form
of the Python code for the objective function.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">custom_obj</span> <span class="o">=</span> <span class="s1">&#39;cp.Minimize(risk)&#39;</span>
</pre></div>
</div>
</section>
<section id="optimisation-constraints">
<h5>Optimisation Constraints<a class="headerlink" href="#optimisation-constraints" title="Permalink to this heading">¶</a></h5>
<p>This is an optional step which requires you to specify the constraints for your optimization problem. Similar to how we specified
cvxpy variables, the constraints need to be specified as a Python list with each constraint being a string representation.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">constraints</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;cp.sum(weights) == 1&#39;</span><span class="p">,</span> <span class="s1">&#39;weights &gt;= 0&#39;</span><span class="p">,</span> <span class="s1">&#39;weights &lt;= 1&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Please note that cvxpy does not support strict inequality constraints - <code class="docutils literal notranslate"><span class="pre">&gt;</span></code> or <code class="docutils literal notranslate"><span class="pre">&lt;</span></code> - and will fail to solve your
problem if you do specify one.</p>
</div>
<p>Bringing it all together, the code looks like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mvo</span> <span class="o">=</span> <span class="n">MeanVarianceOptimisation</span><span class="p">()</span>
<span class="n">custom_obj</span> <span class="o">=</span> <span class="s1">&#39;cp.Minimize(risk)&#39;</span>
<span class="n">constraints</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;cp.sum(weights) == 1&#39;</span><span class="p">,</span> <span class="s1">&#39;weights &gt;= 0&#39;</span><span class="p">,</span> <span class="s1">&#39;weights &lt;= 1&#39;</span><span class="p">]</span>
<span class="n">non_cvxpy_variables</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;num_assets&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="s1">&#39;covariance&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">cov</span><span class="p">(),</span>
    <span class="s1">&#39;expected_returns&#39;</span><span class="p">:</span> <span class="n">ReturnsEstimation</span><span class="p">()</span><span class="o">.</span><span class="n">calculate_mean_historical_returns</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
                                                                              <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">)</span>
<span class="p">}</span>
<span class="n">cvxpy_variables</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;risk = cp.quad_form(weights, covariance)&#39;</span><span class="p">,</span>
    <span class="s1">&#39;portfolio_return = cp.matmul(weights, expected_returns)&#39;</span>
<span class="p">]</span>
<span class="n">mvo</span><span class="o">.</span><span class="n">allocate_custom_objective</span><span class="p">(</span><span class="n">non_cvxpy_variables</span><span class="o">=</span><span class="n">non_cvxpy_variables</span><span class="p">,</span>
                              <span class="n">cvxpy_variables</span><span class="o">=</span><span class="n">cvxpy_variables</span><span class="p">,</span>
                              <span class="n">objective_function</span><span class="o">=</span><span class="n">custom_obj</span><span class="p">,</span>
                              <span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mvo</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><h4> Some Important Miscellaneous Points </h4></p>
<ul>
<li><p>The custom allocation feature still uses cvxpy as the quadratic optimiser. Hence, only convex objective functions are accepted since cvxpy currently does not support non-convex functions. We plan on adding support for non-linear and non-convex objective solutions soon!</p></li>
<li><p>The order of declaring variables also matters here. All non-cvxpy and cvxpy variables are initialised in a linear order, i.e. traversing dictionary from top to bottom and the list from left to right, hence you need to specify them in the order you want it to. For e.g. the following code is wrong and will give an error,</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cvxpy_variables</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;x = cp.quad_form(weights, y)&#39;</span><span class="p">,</span>
    <span class="s1">&#39;y = cp.Variable(1)&#39;</span>
<span class="p">]</span>
</pre></div>
</div>
</div></blockquote>
<p>The formula for <code class="xref py py-mod docutils literal notranslate"><span class="pre">x</span></code> uses <code class="xref py py-mod docutils literal notranslate"><span class="pre">y</span></code>, but due to the order of the list, <code class="xref py py-mod docutils literal notranslate"><span class="pre">y</span></code> will be initialised after
<code class="xref py py-mod docutils literal notranslate"><span class="pre">x</span></code> and give an error.</p>
</li>
</ul>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Although we have written extensive unittests, the custom allocation code is still in an experimental stage and you may
encounter errors which we may have failed to incorporate. We request you to raise an issue <a class="reference external" href="https://github.com/hudson-and-thames/mlfinlab/issues">here</a> and we will promptly push a fix for it.</p>
</div>
</section>
</section>
<section id="implementation">
<h4>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><h4> Using Custom Input </h4>
We provide great flexibility to the users in terms of the input data - they can either pass their own pre-calculated input
matrices/dataframes or leave it to us to calculate them. A quick reference on common input parameters which you will encounter
throughout the portfolio optimization module:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">asset_prices</span></code>: Dataframe/matrix of historical raw asset prices <strong>indexed by date</strong>.</p></li>
<li><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">asset_returns</span></code>: Dataframe/matrix of historical asset returns. This will be a <span class="math notranslate nohighlight">\(TxN\)</span> matrix where <span class="math notranslate nohighlight">\(T\)</span> is the time-series and <span class="math notranslate nohighlight">\(N\)</span> refers to the number of assets in the portfolio.</p></li>
<li><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">expected_asset_returns</span></code>: List of expected returns per asset i.e. the mean of historical asset returns. This refers to the parameter <span class="math notranslate nohighlight">\(\mu\)</span> used in portfolio optimization literature. For a portfolio of 5 assets, <code class="docutils literal notranslate"><span class="pre">expected_asset_returns</span> <span class="pre">=</span> <span class="pre">[0.45,</span> <span class="pre">0.56,</span> <span class="pre">0.89,</span> <span class="pre">1.34,</span> <span class="pre">2.4]</span></code>.</p></li>
<li><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">covariance_matrix</span></code>: The covariance matrix of asset returns.</p></li>
</ul>
</div></blockquote>
</div>
</section>
<section id="plotting">
<h4>Plotting<a class="headerlink" href="#plotting" title="Permalink to this heading">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">plot_efficient_frontier()</span></code> : Plots the efficient frontier. You can specify the minimum and maximum return till which you want
the frontier to be displayed.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mvo</span> <span class="o">=</span> <span class="n">MeanVarianceOptimisation</span><span class="p">()</span>
<span class="n">expected_returns</span> <span class="o">=</span> <span class="n">ReturnsEstimation</span><span class="p">()</span><span class="o">.</span><span class="n">calculate_mean_historical_returns</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
                                                                         <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">)</span>
<span class="n">covariance</span> <span class="o">=</span> <span class="n">ReturnsEstimation</span><span class="p">()</span><span class="o">.</span><span class="n">calculate_returns</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">cov</span><span class="p">()</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">mvo</span><span class="o">.</span><span class="n">plot_efficient_frontier</span><span class="p">(</span><span class="n">covariance</span><span class="o">=</span><span class="n">covariance</span><span class="p">,</span>
                                   <span class="n">max_return</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                                   <span class="n">expected_asset_returns</span><span class="o">=</span><span class="n">expected_returns</span><span class="p">)</span>
</pre></div>
</div>
<img alt="_images/efficient_frontier.png" src="_images/efficient_frontier.png" />
</section>
<section id="research-notebooks">
<h4>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h4>
<p>The following research notebooks provide a more detailed exploration of the algorithm.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Machine%20Learning%20Asset%20Allocation/Chapter16.ipynb">Chapter 16 Exercise Notebook</a></p></li>
</ul>
</section>
</section>
<span id="document-portfolio_optimisation/critical_line_algorithm"></span><span class="target" id="portfolio-optimisation-critical-line-algorithm"></span><div class="admonition note">
<p class="admonition-title">Note</p>
<p>The portfolio optimisation module contains different algorithms that are used for asset allocation and optimising strategies.
Each algorithm is encapsulated in its own class and has a public method called <code class="docutils literal notranslate"><span class="pre">allocate()</span></code> which calculates the weight
allocations on the specific user data. This way, each implementation can be called in the same way and this makes it simple
for users to use them.</p>
</div>
<section id="the-critical-line-algorithm-cla">
<h3>The Critical Line Algorithm (CLA)<a class="headerlink" href="#the-critical-line-algorithm-cla" title="Permalink to this heading">¶</a></h3>
<p>This is a robust alternative to the quadratic optimisation used to find mean-variance optimal portfolios. The major difference
between classic mean-variance optimisation and the critical line algorithm (CLA) are the optimisation constraints involved. A
typical mean-variance optimisation problem looks something like this:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp; \underset{\mathbf{w}}{\text{min}} &amp; &amp; w^T\sum w \\
    &amp; \text{s.t.} &amp; &amp; \sum_{i=1}^{n}w_{i} = 1 \\
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(w\)</span> refers to the set of weights for the portfolio assets, <span class="math notranslate nohighlight">\(\sum\)</span> is the covariance matrix of the assets
and <span class="math notranslate nohighlight">\(\mu\)</span> is the expected asset returns. CLA also solves the same problem but with some added constraints - each weight
of an asset in the portfolio can have different lower and upper bounds. The optimisation objective still remains the same but with
the new constraints on the weights, the problem looks like this:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp; \underset{\mathbf{w}}{\text{min}} &amp; &amp; w^T\sum w \\
    &amp; \text{s.t.} &amp; &amp; \sum_{i=1}^{n}w_{i} = 1 \\
    &amp;&amp;&amp; w_{i} &lt;= u_{i} \\
    &amp;&amp;&amp; w_{i} &gt;= l_{i} \\
\end{align*}\end{split}\]</div>
<p>Each weight in the allocation has an upper and a lower bound, which increases the number of constraints to be solved.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><h4> Underlying Literature </h4></p>
<p>The following sources elaborate extensively on the topic:</p>
<ul class="simple">
<li><p><strong>An Open-Source Implementation of the Critical-Line Algorithm for Portfolio Optimization</strong> <em>by</em> David H. Bailey <em>and</em> Marcos Lopez de Prado <a class="reference external" href="https://papers.ssrn.com/sol3/abstract_id=2197616">available here</a>.</p></li>
</ul>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><h4> Important Points about CLA </h4></p>
<ul class="simple">
<li><p>It is the only algorithm specifically designed for inequality-constrained portfolio optimization problems, which guarantees that the exact solution is found after a given number of iterations</p></li>
<li><p>It does not only compute a single portfolio, but also derives the entire efficient frontier solution.</p></li>
</ul>
</div>
<section id="supported-solutions">
<h4>Supported Solutions<a class="headerlink" href="#supported-solutions" title="Permalink to this heading">¶</a></h4>
<p>MlFinLab’s <code class="xref py py-mod docutils literal notranslate"><span class="pre">CriticalLineAlgorithm</span></code> class provides the following solutions to be used out-of-the-box.</p>
<section id="cla-turning-points">
<h5>CLA Turning Points<a class="headerlink" href="#cla-turning-points" title="Permalink to this heading">¶</a></h5>
<p>As described above, in a CLA problem there are some weights which are bounded by upper and lower bounds. These set of weights are
called <em>bounded</em> weights and the other weights not constrained by any bounds are called <em>free</em> weights.</p>
<p>A solution vector <span class="math notranslate nohighlight">\(w^{*}\)</span> is a <em>turning point</em> its vicinity there is another solution vector with different free weights. This is important because in those regions of the solution space away from turning points the inequality constraints are effectively irrelevant with respect to the free assets.</p>
<p>This solution finds all the sets of weights which satisfy the definition of a <em>turning point</em>. This means that there can be multiple set of optimal weights (or turning points) for a problem.</p>
<p><strong>Solution String:</strong> <code class="docutils literal notranslate"><span class="pre">cla_turning_points</span></code></p>
</section>
<section id="maximum-sharpe-ratio">
<h5>Maximum Sharpe Ratio<a class="headerlink" href="#maximum-sharpe-ratio" title="Permalink to this heading">¶</a></h5>
<p>The optimal weights will be chosen as a convex combination of all the turning points found by <code class="docutils literal notranslate"><span class="pre">cla_turning_points</span></code>. This will
yield a portfolio with the maximum Sharpe Ratio. The convex combination is found using the Golden section method.</p>
<p><strong>Solution String:</strong> <code class="docutils literal notranslate"><span class="pre">max_sharpe</span></code></p>
</section>
<section id="minimum-variance">
<h5>Minimum Variance<a class="headerlink" href="#minimum-variance" title="Permalink to this heading">¶</a></h5>
<p>The optimal weights will be chosen as a convex combination of all the turning points found by <code class="docutils literal notranslate"><span class="pre">cla_turning_points</span></code>. This will yield a portfolio with minimum variance,</p>
<div class="math notranslate nohighlight">
\[\sigma^{2} = w^T \sum w\]</div>
<p>where <span class="math notranslate nohighlight">\(w\)</span> is the vector of weights, <span class="math notranslate nohighlight">\(\sum\)</span> is the covariance matrix of elements in a portfolio.</p>
<p><strong>Solution String:</strong> <code class="docutils literal notranslate"><span class="pre">min_volatility</span></code></p>
</section>
<section id="efficient-frontier">
<h5>Efficient Frontier<a class="headerlink" href="#efficient-frontier" title="Permalink to this heading">¶</a></h5>
<p>Note that the turning points found by <code class="docutils literal notranslate"><span class="pre">cla_turning_points</span></code> constitute a small subset of all the points on the efficient frontier. This efficient frontier solution yields a list of all optimal weights lying on the efficient frontier and satisfying the problem. All these weights/points are found through the convex combination of CLA turning points.</p>
<p><strong>Solution String:</strong> <code class="docutils literal notranslate"><span class="pre">efficient_frontier</span></code></p>
</section>
</section>
<section id="implementation">
<h4>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><h4> Using Custom Input </h4>
We provide great flexibility to the users in terms of the input data - they can either pass their own pre-calculated input
matrices/dataframes or leave it to us to calculate them. A quick reference on common input parameters which you will encounter
throughout the portfolio optimization module:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">asset_prices</span></code>: Dataframe/matrix of historical raw asset prices <strong>indexed by date</strong>.</p></li>
<li><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">asset_returns</span></code>: Dataframe/matrix of historical asset returns. This will be a <span class="math notranslate nohighlight">\(TxN\)</span> matrix where <span class="math notranslate nohighlight">\(T\)</span> is the time-series and <span class="math notranslate nohighlight">\(N\)</span> refers to the number of assets in the portfolio.</p></li>
<li><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">expected_asset_returns</span></code>: List of expected returns per asset i.e. the mean of historical asset returns. This refers to the parameter <span class="math notranslate nohighlight">\(\mu\)</span> used in portfolio optimization literature. For a portfolio of 5 assets, <code class="docutils literal notranslate"><span class="pre">expected_asset_returns</span> <span class="pre">=</span> <span class="pre">[0.45,</span> <span class="pre">0.56,</span> <span class="pre">0.89,</span> <span class="pre">1.34,</span> <span class="pre">2.4]</span></code>.</p></li>
<li><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">covariance_matrix</span></code>: The covariance matrix of asset returns.</p></li>
</ul>
</div></blockquote>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><h4> Specifying Weight Bounds </h4>
Users can specify weight bounds in two ways:</p>
<ul>
<li><p>Use the same set of lower and upper bound values for all the assets simultaneously. In this case just pass a tuple of bounds.</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cla</span> <span class="o">=</span> <span class="n">CriticalLineAlgorithm</span><span class="p">(</span><span class="n">weight_bounds</span><span class="o">=</span><span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>If you want to specify individual bounds, you need to pass a tuple of lists - the first list containing lower bounds and the second list containing upper bounds for all assets respectively. Something like this:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cla</span> <span class="o">=</span> <span class="n">CriticalLineAlgorithm</span><span class="p">(</span><span class="n">weight_bounds</span><span class="o">=</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]))</span>
</pre></div>
</div>
</div></blockquote>
<p>Note that when using this way of passing bounds, you need to specify bounds for all the assets. For free assets i.e. those       with no specific bounds, just specify 0 as lower bound value and 1 as upper bound value.</p>
</li>
</ul>
</div>
</section>
<section id="example-code">
<h4>Example Code<a class="headerlink" href="#example-code" title="Permalink to this heading">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.portfolio_optimization.cla</span> <span class="kn">import</span> <span class="n">CriticalLineAlgorithm</span>

<span class="c1"># Read in data</span>
<span class="n">stock_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span>

<span class="c1"># Compute different solutions using CLA</span>
<span class="n">cla</span> <span class="o">=</span> <span class="n">CriticalLineAlgorithm</span><span class="p">()</span>

<span class="c1"># Turning : each row as a solution (turning_points)</span>
<span class="n">cla</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span> <span class="n">solution</span><span class="o">=</span><span class="s1">&#39;cla_turning_points&#39;</span><span class="p">)</span>
<span class="n">cla_weights</span> <span class="o">=</span> <span class="n">cla</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Maximum Sharpe Solution</span>
<span class="n">cla</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">solution</span><span class="o">=</span><span class="s1">&#39;max_sharpe&#39;</span><span class="p">)</span>
<span class="n">cla_weights</span> <span class="o">=</span> <span class="n">cla</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">max_sharpe_value</span> <span class="o">=</span> <span class="n">cla</span><span class="o">.</span><span class="n">max_sharpe</span> <span class="c1"># Accessing the max sharpe value</span>

<span class="c1"># Minimum Variance Solution</span>
<span class="n">cla</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">solution</span><span class="o">=</span><span class="s1">&#39;min_volatility&#39;</span><span class="p">)</span>
<span class="n">cla_weights</span> <span class="o">=</span> <span class="n">cla</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">min_variance_value</span> <span class="o">=</span> <span class="n">cla</span><span class="o">.</span><span class="n">min_var</span> <span class="c1"># Accessing the min-variance value</span>

<span class="c1"># Efficient Frontier Solution</span>
<span class="n">cla</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">solution</span><span class="o">=</span><span class="s1">&#39;efficient_frontier&#39;</span><span class="p">)</span>
<span class="n">cla_weights</span> <span class="o">=</span> <span class="n">cla</span><span class="o">.</span><span class="n">weights</span>
<span class="n">means</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">cla</span><span class="o">.</span><span class="n">efficient_frontier_means</span><span class="p">,</span> <span class="n">cla</span><span class="o">.</span><span class="n">efficient_frontier_sigma</span>
</pre></div>
</div>
</section>
<section id="research-notebooks">
<h4>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h4>
<p>The following research notebooks provide a more detailed exploration of the algorithm.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Machine%20Learning%20Asset%20Allocation/Chapter16.ipynb">Chapter 16 Exercise Notebook</a></p></li>
</ul>
</section>
</section>
<span id="document-portfolio_optimisation/hierarchical_risk_parity"></span><span class="target" id="portfolio-optimisation-hierarchical-risk-parity"></span><div class="admonition note">
<p class="admonition-title">Note</p>
<p>The portfolio optimisation module contains different algorithms that are used for asset allocation and optimising strategies.
Each algorithm is encapsulated in its own class and has a public method called <code class="docutils literal notranslate"><span class="pre">allocate()</span></code> which calculates the weight
allocations on the specific user data. This way, each implementation can be called in the same way and this makes it simple
for users to use them.</p>
</div>
<section id="hierarchical-risk-parity-hrp">
<h3>Hierarchical Risk Parity (HRP)<a class="headerlink" href="#hierarchical-risk-parity-hrp" title="Permalink to this heading">¶</a></h3>
<p>The Hierarchical Risk Parity algorithm is a novel portfolio optimisation method combining machine learning and traditional
portfolio optimisation. Although, it is a simple algorithm, it has been found to be very stable as compared to its older
counterparts (the traditional mean variance optimisation methods).</p>
<section id="overview-of-the-algorithm">
<h4>Overview of the Algorithm<a class="headerlink" href="#overview-of-the-algorithm" title="Permalink to this heading">¶</a></h4>
<section id="hierarchical-tree-clustering">
<h5>Hierarchical Tree Clustering<a class="headerlink" href="#hierarchical-tree-clustering" title="Permalink to this heading">¶</a></h5>
<p>This step breaks down the assets in our portfolio into different hierarchical clusters using the famous Hierarchical Tree
Clustering algorithm. The assets in the portfolio are segregated into clusters which mimic the real-life interactions between
the assets in a portfolio - some stocks are related to each other more than others and hence can be grouped within the same
cluster. At the end of the step, we are left with the follow tree structure (also called a dendrogram).</p>
<img alt="_images/dendrogram.png" src="_images/dendrogram.png" />
<p><br></p>
</section>
<section id="matrix-seriation">
<h5>Matrix Seriation<a class="headerlink" href="#matrix-seriation" title="Permalink to this heading">¶</a></h5>
<p>Matrix seriation is a very old statistical technique which is used to rearrange the data to show the inherent clusters
clearly. Using the order of hierarchical clusters from the previous step, we rearrange the rows and columns of the covariance
matrix of stocks so that similar investments are placed together and dissimilar investments are placed far apart</p>
<img alt="_images/seriation.png" src="_images/seriation.png" />
<p><br></p>
</section>
<section id="recursive-bisection">
<h5>Recursive Bisection<a class="headerlink" href="#recursive-bisection" title="Permalink to this heading">¶</a></h5>
<p>This is the final and the most important step of this algorithm where the actual weights are assigned to the assets in a
top-down recursive manner. Based on the hierarchical tree dendrogram formed in the first step, the weights trickle down the
tree and get assigned to the portfolio assets.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><h4> Underlying Literature </h4></p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2708678">Hierarchical Risk Parity Paper</a></p></li>
<li><p>For a detailed explanation of how hierarchical risk parity works, we have written an excellent <a class="reference external" href="https://hudsonthames.org/an-introduction-to-the-hierarchical-risk-parity-algorithm/">blog post</a> about it.</p></li>
</ul>
</div></blockquote>
</div>
</section>
</section>
<section id="implementation">
<h4>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><h4> Using Custom Input </h4>
We provide great flexibility to the users in terms of the input data - they can either pass their own pre-calculated input
matrices/dataframes or leave it to us to calculate them. A quick reference on common input parameters which you will encounter
throughout the portfolio optimisation module:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">asset_prices</span></code>: Dataframe/matrix of historical raw asset prices <strong>indexed by date</strong>.</p></li>
<li><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">asset_returns</span></code>: Dataframe/matrix of historical asset returns. This will be a <span class="math notranslate nohighlight">\(TxN\)</span> matrix where <span class="math notranslate nohighlight">\(T\)</span> is the time-series and <span class="math notranslate nohighlight">\(N\)</span> refers to the number of assets in the portfolio.</p></li>
<li><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">expected_asset_returns</span></code>: List of expected returns per asset i.e. the mean of historical asset returns. This refers to the parameter <span class="math notranslate nohighlight">\(\mu\)</span> used in portfolio optimisation literature. For a portfolio of 5 assets, <code class="docutils literal notranslate"><span class="pre">expected_asset_returns</span> <span class="pre">=</span> <span class="pre">[0.45,</span> <span class="pre">0.56,</span> <span class="pre">0.89,</span> <span class="pre">1.34,</span> <span class="pre">2.4]</span></code>.</p></li>
<li><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">covariance_matrix</span></code>: The covariance matrix of asset returns.</p></li>
</ul>
</div></blockquote>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><h4> Using Custom Distance Matrix </h4>
The hierarchical clustering step in the algorithm uses a distance matrix to calculate the clusters and form the hierarchical
tree. By default, we use the distance matrix mentioned in the original paper,</p>
<div class="math notranslate nohighlight">
\[D(i, j) = \sqrt{\frac{1}{2} * (1 - \rho(i, j))}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\rho\)</span> refers to the correlation matrix of assets. Users can specify their own custom matrix to be used instead of
the default one by passing an <span class="math notranslate nohighlight">\(NxN\)</span> symmetric pandas dataframe or a numpy matrix using the <code class="xref py py-mod docutils literal notranslate"><span class="pre">distance_matrix</span></code>
parameter. <strong>Note that the</strong> <code class="xref py py-mod docutils literal notranslate"><span class="pre">covariance_matrix</span></code> <strong>is still requested for computing the clustered variances.</strong></p>
<p><h4> Constructing a Long/Short Portfolio </h4>
By default, the weights are allocated assuming a long portfolio i.e. all the weights are positive. However, users can also opt
to have specific assets in their portfolio shorted, by specifying a custom list through the <code class="xref py py-mod docutils literal notranslate"><span class="pre">side_weights</span></code> parameter.
For e.g. in a portfolio of 5 assets, if you want the 3rd and the 5th asset to be shorted, then simply pass a list containing
1s and -1s for long and short positions respectively. In the above case, <code class="docutils literal notranslate"><span class="pre">side_weights</span> <span class="pre">=</span> <span class="pre">[1,1,-1,1,-1]</span></code>, where you
have -1 at the 3rd and 5th index. By default, a list of 1s will be initialised if no custom input is specified.</p>
<p><h4> Different Linkage Methods </h4>
HRP, by default, uses the single-linkage clustering algorithm. (See the tip under the HERC algorithm for more details.)</p>
</div>
</section>
<section id="example-code">
<h4>Example Code<a class="headerlink" href="#example-code" title="Permalink to this heading">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.portfolio_optimization.hrp</span> <span class="kn">import</span> <span class="n">HierarchicalRiskParity</span>

<span class="c1"># Read in data</span>
<span class="n">stock_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span>

<span class="c1"># Compute HRP weights</span>
<span class="n">hrp</span> <span class="o">=</span> <span class="n">HierarchicalRiskParity</span><span class="p">()</span>
<span class="n">hrp</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">)</span>
<span class="n">hrp_weights</span> <span class="o">=</span> <span class="n">hrp</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Building a dollar neutral Long/Short portfolio by shorting the first 4 stocks and being long the others</span>
<span class="n">hrp</span> <span class="o">=</span> <span class="n">HierarchicalRiskParity</span><span class="p">()</span>
<span class="n">side_weights</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">stock_prices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">side_weights</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">stock_prices</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="mi">4</span><span class="p">]]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">hrp</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">asset_names</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">side_weights</span><span class="o">=</span><span class="n">side_weights</span><span class="p">)</span>
<span class="n">hrp</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">side_weights</span><span class="o">=</span><span class="n">side_weights</span><span class="p">)</span>
<span class="n">hrp_weights</span> <span class="o">=</span> <span class="n">hrp</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="plotting">
<h4>Plotting<a class="headerlink" href="#plotting" title="Permalink to this heading">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">plot_clusters()</span></code> : Plots the hierarchical clusters formed during the clustering step in HRP. This is visualised in the form of dendrograms - a very common way of visualising the hierarchical tree clusters.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instantiate HRP Class</span>
<span class="n">hrp</span> <span class="o">=</span> <span class="n">HierarchicalRiskParity</span><span class="p">()</span>
<span class="n">hrp</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">)</span>

<span class="c1"># Plot Dendrogram</span>
<span class="n">hrp</span><span class="o">.</span><span class="n">plot_clusters</span><span class="p">(</span><span class="n">assets</span><span class="o">=</span><span class="n">stock_prices</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>
</div>
<img alt="_images/dendrogram.png" src="_images/dendrogram.png" />
</section>
<section id="research-notebooks">
<h4>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h4>
<p>The following research notebooks provide a more detailed exploration of the algorithm.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Advances%20in%20Financial%20Machine%20Learning/Machine%20Learning%20Asset%20Allocation/Chapter16.ipynb">Chapter 16 Exercise Notebook</a></p></li>
</ul>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Portfolio%20Optimisation%20Tutorials/Hierarchical%20Risk%20Parity%20(HRP)/HRP%20use%20examples.ipynb">How to use mlfinlab’s HierarchicalRiskParity class</a></p></li>
</ul>
</section>
</section>
<span id="document-portfolio_optimisation/hierarchical_equal_risk_contribution"></span><span class="target" id="portfolio-optimisation-hierarchical-equal-risk-contribution"></span><div class="admonition note">
<p class="admonition-title">Note</p>
<p>The portfolio optimisation module contains different algorithms that are used for asset allocation and optimising strategies.
Each algorithm is encapsulated in its own class and has a public method called <code class="docutils literal notranslate"><span class="pre">allocate()</span></code> which calculates the weight
allocations on the specific user data. This way, each implementation can be called in the same way and this makes it simple
for users to use them.</p>
</div>
<section id="hierarchical-equal-risk-contribution-herc">
<h3>Hierarchical Equal Risk Contribution (HERC)<a class="headerlink" href="#hierarchical-equal-risk-contribution-herc" title="Permalink to this heading">¶</a></h3>
<p>The Hierarchical Equal Risk Contribution (HERC) method takes inspiration from the Hierarchical Risk Parity (HRP)
algorithm and the Hierarchical Clustering based Asset Allocation (HCAA) and uses machine learning to allocate weights efficiently. While both the HERC and HRP algorithms use hierarchical tree clustering to allocate their weights, there are some subtle differences between the two. Lets look at a quick overview of how the HERC algorithm works:</p>
<section id="overview-of-the-algorithm">
<h4>Overview of the Algorithm<a class="headerlink" href="#overview-of-the-algorithm" title="Permalink to this heading">¶</a></h4>
<section id="hierarchical-tree-clustering">
<h5>Hierarchical Tree Clustering<a class="headerlink" href="#hierarchical-tree-clustering" title="Permalink to this heading">¶</a></h5>
<p>This step breaks down the assets in our portfolio into different hierarchical clusters using the famous Hierarchical Tree
Clustering algorithm (Agglomerative Clustering). The assets in the portfolio are segregated into clusters which mimic the
real-life interactions between the assets in a portfolio - some stocks are related to each other more than others and hence
can be grouped within the same cluster. At the end of the step, we are left with the follow tree structure (also called a
dendrogram).</p>
<img alt="_images/dendrogram.png" src="_images/dendrogram.png" />
<p><br></p>
</section>
<section id="calculate-optimal-number-of-clusters">
<h5>Calculate Optimal Number of Clusters<a class="headerlink" href="#calculate-optimal-number-of-clusters" title="Permalink to this heading">¶</a></h5>
<p>This step is where HERC deviates from the traditional HRP algorithm. The hierarchical risk parity method uses single linkage
and grows the tree to maximum depth. However, the number of clusters identified by growing the tree maximally may not be the
optimal one and can lead to sub-optimal results. <strong>This is why before allocating the weights, HERC calculates the optimal number of clusters and cuts the hierarchical tree formed in Step-1 to the required height and clusters</strong>. Currently, the Gap Index is used for
calculating the required number of clusters.</p>
<img alt="_images/gap.png" src="_images/gap.png" />
<p><br></p>
</section>
<section id="top-down-recursive-bisection">
<h5>Top-Down Recursive Bisection<a class="headerlink" href="#top-down-recursive-bisection" title="Permalink to this heading">¶</a></h5>
<p>This is the step where weights for the clusters are calculated. If you are familiar with how the hierarchical risk parity
algorithm works, then you know this is similar to how HRP also allocates its weights. However, there is a fundamental
difference between the recursive bisections of the two algorithms.</p>
<img alt="_images/bisection.png" src="_images/bisection.png" />
<p>As seen in the above image, at each step, the weights in HRP trickle down the tree by breaking it down the middle based on the
number of assets. Although, this uses the hierarchical tree identified in Step-1, it does not make use of the exact structure
of the dendrogram while calculating the cluster contributions. This is a fundamental disadvantage of HRP which is improved
upon by HERC by dividing the tree, at each step, based on the structure induced by the dendrogram.</p>
<p>At each level of the tree, an Equal Risk Contribution allocation is used i.e. the weights are:</p>
<div class="math notranslate nohighlight">
\[\alpha_1 = \frac{RC_1}{RC_1 + RC_2}; \alpha_2 = 1 - \alpha_1\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha_1\)</span>, <span class="math notranslate nohighlight">\(\alpha_2\)</span> are the weights of left and right clusters respectively and <span class="math notranslate nohighlight">\(RC_1\)</span>, <span class="math notranslate nohighlight">\(RC_2\)</span>
are the risk contributions of left and right clusters.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><h4> Risk Contribution of Clusters </h4>
While variance is a very simple and popular representation of risk used in the investing world, it is not the optimal one
and can underestimate the true risk of a portfolio which is why there are many other important risk metrics used by
investment managers that can correctly reflect the true risk of a portfolio/asset. With respect to this, the original HRP
algorithm can be tweaked to allocate its weights based on different risk representations of the clusters and generate
better weights. The HERC method in mlfinlab provides the following risk metrics:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">variance</span></code> : Variance of the clusters is used as a risk metric.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">standard_deviation</span></code> : Standard deviation of the clusters is used as a risk metric.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">equal_weighting</span></code> : All clusters are weighed equally in terms of risk.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">expected_shortfall</span></code> : Expected shortfall (CVaR) of the clusters is used as a risk metric.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">conditional_drawdown_at_risk</span></code> : Conditional drawdown at risk (CDaR) of the clusters is used as a risk metric.</p></li>
</ol>
</div>
</section>
<section id="naive-risk-parity">
<h5>Naive Risk Parity<a class="headerlink" href="#naive-risk-parity" title="Permalink to this heading">¶</a></h5>
<p>Having calculated the cluster weights in the previous step, this step calculates the final asset weights. Within the same
cluster, an initial set of weights - <span class="math notranslate nohighlight">\(w_{NRP}\)</span> - is calculated using the naive risk parity allocation. In this approach, assets are allocated weights in proportion to the inverse of their respective risk; higher risk assets will receive lower portfolio weights, and lower risk assets will receive higher weights. Here the risk can be quantified in different ways - variance, CVaR, CDaR, max daily loss etc…</p>
<p>The final weights are given by the following equation:</p>
<div class="math notranslate nohighlight">
\[w^{i}_{final} = w^{i}_{NRP} * C^{i}, \: i \in Clusters\]</div>
<p>where, <span class="math notranslate nohighlight">\(w^{i}_{NRP}\)</span> refers to naive risk parity weights of assets in the <span class="math notranslate nohighlight">\(i^{th}\)</span> cluster and <span class="math notranslate nohighlight">\(C^{i}\)</span> is the
weight of the  <span class="math notranslate nohighlight">\(i^{th}\)</span> cluster calculated in Step-3.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><h4> Underlying Literature </h4>
This implementation is based on the following two papers written by Thomas Raffinot:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3237540">Hierarchical Clustering based Asset Allocation</a></p></li>
<li><p><a class="reference external" href="https://ssrn.com/abstract=2840729">Hierarchical Equal Risk Contribution</a></p></li>
</ul>
</div></blockquote>
<p>You can read more about the Gap Index method in this paper:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://statweb.stanford.edu/~gwalther/gap">Gap Index Paper</a></p></li>
</ul>
</div></blockquote>
</div>
</section>
</section>
<section id="implementation">
<h4>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h4>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The calculation of optimal number of clusters using the Gap Index makes the algorithm run a little slower and you will notice
a significant speed difference for larger datasets. If you know the number of clusters for your data beforehand, it would be
better for you to pass that value directly to the method using the <code class="xref py py-mod docutils literal notranslate"><span class="pre">optimal_num_clusters</span></code> parameter. This will bypass
the Gap Index method and speed up the algorithm.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><h4> Using Custom Input </h4>
We provide great flexibility to the users in terms of the input data - they can either pass their own pre-calculated input
matrices/dataframes or leave it to us to calculate them. A quick reference on common input parameters which you will encounter
throughout the portfolio optimisation module:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">asset_prices</span></code>: Dataframe/matrix of historical raw asset prices <strong>indexed by date</strong>.</p></li>
<li><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">asset_returns</span></code>: Dataframe/matrix of historical asset returns. This will be a <span class="math notranslate nohighlight">\(TxN\)</span> matrix where <span class="math notranslate nohighlight">\(T\)</span> is the time-series and <span class="math notranslate nohighlight">\(N\)</span> refers to the number of assets in the portfolio.</p></li>
<li><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">expected_asset_returns</span></code>: List of expected returns per asset i.e. the mean of historical asset returns. This refers to the parameter <span class="math notranslate nohighlight">\(\mu\)</span> used in portfolio optimisation literature. For a portfolio of 5 assets, <code class="docutils literal notranslate"><span class="pre">expected_asset_returns</span> <span class="pre">=</span> <span class="pre">[0.45,</span> <span class="pre">0.56,</span> <span class="pre">0.89,</span> <span class="pre">1.34,</span> <span class="pre">2.4]</span></code>.</p></li>
<li><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">covariance_matrix</span></code>: The covariance matrix of asset returns.</p></li>
</ul>
</div></blockquote>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><h4> Different Linkage Methods </h4>
The following linkage methods are supported by the HERC class in mlfinlab. (The following is taken directly from and we highly
recommend you read):</p>
<p><a class="reference external" href="https://d-nb.info/1108447864/34">Papenbrock, J., 2011. Asset Clusters and Asset Networks in Financial Risk Management and Portfolio Optimization (Doctoral
dissertation, Karlsruher Institut für Technologie (KIT)).</a></p>
<blockquote>
<div><p><strong>1. Single-Linkage</strong></p>
<p>The idea behind single-linkage is to form groups of elements, which have the smallest distance to each other (nearest
neighbouring clustering). This oftentimes leads to large groups/chaining.</p>
<p>The single-link algorithm oftentimes forms clusters that are chained together and leaves large clusters. It can probably
be best understood as a way to give a “more robust” estimation of the distance matrix and furthermore preserves the original
structure as much as possible. Elements departing early from the tree can be interpreted as “different” from the overall dataset.
In terms of application, the single-link clustering algorithm is very useful to gain insights in the correlation structure
between assets and separates assets that were very different from the rest. If this separation is preferred and high weights
should be put on “outliers” the single link certainly is a good choice.</p>
<p><strong>2. Complete-Linkage</strong></p>
<p>The complete-linkage algorithm tries to avoid those large groups by considering the largest distances between elements.
It is thus called the farthest neighbour clustering.</p>
<p>The complete-link algorithm has a different idea: elements should be grouped together in a way that they are not too
different from each other when merged in a cluster. It thus has a much stronger definition of “similar pair of clusters”.
The complete-link algorithm therefore seems suitable for investors interested in grouping stocks that are similar in one cluster.</p>
<p><strong>3. Average-Linkage</strong></p>
<p>The average-linkage algorithm is a compromise between the single-linkage and complete-linkage algorithm.</p>
<p><strong>4. Ward-Linkage</strong></p>
<p>Whereas single-linkage, complete-linkage and average-linkage can be classified as graph-based clustering algorithms,
Ward’s method has a prototype-based view in which the clusters are represented by a centroid. For this reason, the
proximity between clusters is usually defined as the distance between cluster centroids. The Ward method uses the increase
in the sum of the squares error (SSE) to determine the clusters.</p>
</div></blockquote>
</div>
</section>
<section id="plotting">
<h4>Plotting<a class="headerlink" href="#plotting" title="Permalink to this heading">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">plot_clusters()</span></code> : Plots the hierarchical clusters formed during the clustering step. This is visualised in the form of dendrograms - a very common way of visualising the hierarchical tree clusters.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instantiate HERC Class</span>
<span class="n">herc</span> <span class="o">=</span> <span class="n">HierarchicalEqualRiskContribution</span><span class="p">()</span>
<span class="n">herc</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">risk_measure</span><span class="o">=</span><span class="s1">&#39;equal_weighting&#39;</span><span class="p">)</span>

<span class="c1"># Plot Dendrogram</span>
<span class="n">herc</span><span class="o">.</span><span class="n">plot_clusters</span><span class="p">(</span><span class="n">assets</span><span class="o">=</span><span class="n">stock_prices</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>
</div>
<img alt="_images/dendrogram_herc.png" src="_images/dendrogram_herc.png" />
<p>In the above image, the colors of assets corresponds to the cluster to which that asset belongs. Assets in the same cluster have the same color. Note that this coloring scheme may change based on the optimal number of clusters identified by the algorithm (or specified by the user).</p>
</section>
<section id="research-notebooks">
<h4>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h4>
<p>The following research notebooks provide a more detailed exploration of the algorithm.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Portfolio%20Optimisation%20Tutorials/Hierarchical%20Equal%20Risk%20Contribution%20(HERC)/HERC%20Tutorial%20Notebook.ipynb">How to use mlfinlab’s HierarchicalEqualRiskContribution class</a></p></li>
</ul>
</section>
</section>
<span id="document-portfolio_optimisation/nested_clustered_optimisation"></span><span class="target" id="portfolio-optimisation-nested-clustered-optimisation"></span><section id="nested-clustered-optimization-nco">
<h3>Nested Clustered Optimization (NCO)<a class="headerlink" href="#nested-clustered-optimization-nco" title="Permalink to this heading">¶</a></h3>
<p>The NCO class includes functions related to:</p>
<ul class="simple">
<li><p>Weight allocation using the Nested Clustered Optimization (NCO) algorithm.</p></li>
<li><p>Weight allocation using the Convex Optimization Solution (CVO).</p></li>
<li><p>Multiple simulations for the NCO and CVO algorithms using Monte Carlo Optimization Selection (MCOS) algorithm.</p></li>
<li><p>Sample data generation to use in the above functions.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><h4> Underlying Literature </h4></p>
<p>The following sources elaborate extensively on the topic:</p>
<ul class="simple">
<li><p><strong>A Robust Estimator of the Efficient Frontier</strong> <em>by</em> Marcos Lopez de Prado <a class="reference external" href="https://papers.ssrn.com/sol3/abstract_id=3469961">available here</a>. <em>Describes the NCO, CVO, and MCOS algorithms.</em></p></li>
</ul>
</div>
<section id="about-the-algorithm">
<h4>About the Algorithm<a class="headerlink" href="#about-the-algorithm" title="Permalink to this heading">¶</a></h4>
<p>The Nested Clustered Optimization algorithm estimates optimal weight allocation to either maximize the Sharpe ratio
or minimize the variance of a portfolio.</p>
<p>The steps of the NCO algorithm are:</p>
<ol class="arabic simple">
<li><p>Get the covariance matrix of the outcomes as an input (and the vector of means if the target is to maximize the Sharpe ratio).</p></li>
<li><p>Transform the covariance matrix to the correlation matrix and calculate the distance matrix based on it.</p></li>
<li><p>Cluster the covariance matrix into subsets of highly-correlated variables.</p></li>
<li><p>Compute the optimal weights allocation (Convex Optimization Solution) for every cluster.</p></li>
<li><p>Reduce the original covariance matrix to a reduced one - where each cluster is represented by a single variable.</p></li>
<li><p>Compute the optimal weights allocation (Convex Optimization Solution) for the reduced covariance matrix.</p></li>
<li><p>Compute the final allocations as a dot-product of the allocations between the clusters and inside the clusters.</p></li>
</ol>
</section>
<section id="convex-optimization-solution-cvo">
<h4>Convex Optimization Solution (CVO)<a class="headerlink" href="#convex-optimization-solution-cvo" title="Permalink to this heading">¶</a></h4>
<p>The Convex Optimization Solution is the result of convex optimization when solving a problem of calculating the optimal weight allocation
using the true covariance matrix and the true vector of means for a portfolio. The goal can be either the maximum Sharpe ratio or
minimum variance of a portfolio.</p>
<p>If the problem of portfolio optimization is:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}min_{w}\frac{1}{2}w'Vw\\s.t.: w'a = 1\end{aligned}\end{align} \]</div>
<p>Where <span class="math notranslate nohighlight">\(V\)</span> is the covariance matrix of elements in a portfolio, <span class="math notranslate nohighlight">\(w\)</span> is the vector of weights that minimizes the
variance or maximizes the Sharpe ratio, <span class="math notranslate nohighlight">\(a\)</span> is an optimal solution that defines the goal of optimization.</p>
<p>Then the Convex Optimization Solution to the problem is:</p>
<div class="math notranslate nohighlight">
\[w^* = \frac{V^{-1}a}{a'V^{-1}a}\]</div>
</section>
<section id="monte-carlo-optimization-selection-mcos">
<h4>Monte Carlo Optimization Selection (MCOS)<a class="headerlink" href="#monte-carlo-optimization-selection-mcos" title="Permalink to this heading">¶</a></h4>
<p>The Monte Carlo Optimization Selection algorithm calculates the NCO allocations and a simple optimal allocation for multiple
simulated pairs of mean vector and the covariance matrix to determine the most robust method for weight allocations for a given
pair of means vector and a covariance vector.</p>
<p>The steps of the MCOS algorithm are:</p>
<ol class="arabic simple">
<li><p>Get the covariance matrix and the means vector of the outcomes as an input (along with the simulation parameters to use).</p></li>
<li><p>Drawing the empirical covariance matrix and the empirical means vector based on the true ones.</p></li>
<li><p>If the kde_bwidth parameter is given, the empirical covariance matrix is de-noised.</p></li>
<li><p>Based on the min_var_portf parameter, either the minimum variance or the maximum Sharpe ratio is targeted in weights allocation.</p></li>
<li><p>CVO is applied to the empirical data to obtain the weights allocation.</p></li>
<li><p>NCO is applied to the empirical data to obtain the weights allocation.</p></li>
<li><p>Based on the original covariance matrix and the means vector a true optimal allocation is calculated.</p></li>
<li><p>For each weights estimation in a method, a standard deviation between the true weights and the obtained weights is calculated.</p></li>
<li><p>The error associated with each method is calculated as the mean of the standard deviation across all estimations for the method.</p></li>
</ol>
</section>
<section id="sample-data-generating">
<h4>Sample Data Generating<a class="headerlink" href="#sample-data-generating" title="Permalink to this heading">¶</a></h4>
<p>This method allows creating a random vector of means and a random covariance matrix that has the characteristics of securities. The elements are divided into clusters. The elements in clusters have a given level of correlation. The correlation between the clusters is set at another level. This structure is created in order to test the NCO and MCOS algorithms.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>These algorithms are described in more detail in the work <strong>A Robust Estimator of the Efficient Frontier</strong> <em>by</em> Marcos Lopez de Prado <a class="reference external" href="https://papers.ssrn.com/abstract_id=3469961">available here</a>.</p>
<p>Examples of using these functions are available in the <a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/NCO/NCO.ipynb">NCO Notebook</a>.</p>
</div>
</section>
<section id="implementation">
<h4>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h4>
</section>
<section id="example-code">
<h4>Example Code<a class="headerlink" href="#example-code" title="Permalink to this heading">¶</a></h4>
<p>Below is an example of how to use the package functions to calculate risk metrics for a portfolio.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.portfolio_optimization</span> <span class="kn">import</span> <span class="n">NCO</span>

<span class="c1"># Import dataframe of returns for assets in a portfolio</span>
<span class="n">assets_returns</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">DATA_PATH</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Calculate empirical covariance of assets</span>
<span class="n">assets_cov</span> <span class="o">=</span> <span class="n">assets_returns</span><span class="o">.</span><span class="n">cov</span><span class="p">()</span>

<span class="c1"># Calculate empirical means of assets</span>
<span class="n">assets_mean</span> <span class="o">=</span> <span class="n">assets_returns</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Class that contains needed functions</span>
<span class="n">nco</span> <span class="o">=</span> <span class="n">NCO</span><span class="p">()</span>

<span class="c1"># Find optimal weights using the NCO algorithm</span>
<span class="n">w_nco</span> <span class="o">=</span> <span class="n">nco</span><span class="o">.</span><span class="n">allocate_nco</span><span class="p">(</span><span class="n">assets_cov</span><span class="p">,</span> <span class="n">assets_mean</span><span class="p">)</span>

<span class="c1"># Find optimal weights using the CVO algorithm</span>
<span class="n">w_cvo</span> <span class="o">=</span> <span class="n">nco</span><span class="o">.</span><span class="n">allocate_cvo</span><span class="p">(</span><span class="n">assets_cov</span><span class="p">,</span> <span class="n">assets_mean</span><span class="p">)</span>

<span class="c1"># Compare the NCO solutions to the CVO ones using MCOS</span>
<span class="c1"># Parameters are: 10 simulations, 100 observations in a simulation</span>
<span class="c1"># goal of minimum variance, no LW shrinkage</span>
<span class="n">w_cvo</span><span class="p">,</span> <span class="n">w_nco</span> <span class="o">=</span> <span class="n">nco</span><span class="o">.</span><span class="n">allocate_mcos</span><span class="p">(</span><span class="n">assets_mean</span><span class="p">,</span> <span class="n">assets_cov</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

<span class="c1"># Find the errors in estimations of NCO and CVO in simulations</span>
<span class="n">err_cvo</span><span class="p">,</span> <span class="n">err_nco</span> <span class="o">=</span> <span class="n">nco</span><span class="o">.</span><span class="n">estim_errors_mcos</span><span class="p">(</span><span class="n">w_cvo</span><span class="p">,</span> <span class="n">w_nco</span><span class="p">,</span> <span class="n">assets_mean</span><span class="p">,</span> <span class="n">assets_cov</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="research-notebooks">
<h4>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h4>
<p>The following research notebooks provide a more detailed exploration of the algorithm.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Portfolio%20Optimisation%20Tutorials/Nested%20Clustered%20Optimisation%20(NCO)/NCO.ipynb">Nested Clustered Optimisation Notebook</a></p></li>
</ul>
</section>
</section>
<span id="document-portfolio_optimisation/theory_implied_correlation"></span><span class="target" id="portfolio-optimisation-theory-implied-correlation"></span><section id="theory-implied-correlation-tic">
<h3>Theory-Implied Correlation (TIC)<a class="headerlink" href="#theory-implied-correlation-tic" title="Permalink to this heading">¶</a></h3>
<p>This TIC class includes an algorithm to calculate the Theory-Implied Correlation and a method to calculate the correlation matrix distance proposed by Herdin and Bonek. This distance may be used to measure to which extent the TIC matrix has blended theory-implied views (tree structure of the elements) with empirical evidence (correlation matrix).</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><h4> Underlying Literature </h4></p>
<p>The following sources elaborate extensively on the topic:</p>
<ul class="simple">
<li><p><strong>Estimation of Theory-Implied Correlation Matrices</strong> <em>by</em> Marcos Lopez de Prado <a class="reference external" href="https://papers.ssrn.com/abstract_id=3484152">available here</a>. <em>Describes the TIC algorithm</em></p></li>
<li><p><strong>AMIMO Correlation Matrix based Metric for Characterizing Non-Stationarity</strong> <em>by</em> Markus Herdin <em>and</em> Ernst Bonek <a class="reference external" href="https://publik.tuwien.ac.at/files/pub-et_8791.pdf">available here</a>. <em>Describes the Correlation Matrix Distance metric</em></p></li>
</ul>
</div>
<section id="about-the-algorithm">
<h4>About the Algorithm<a class="headerlink" href="#about-the-algorithm" title="Permalink to this heading">¶</a></h4>
<p>The TIC algorithm is aiming to estimate a forward-looking correlation matrix based on economic theory. The method is using a theoretical classification of assets (hierarchical structure) and fits the empirical correlation matrix to the theoretical structure.</p>
<p>From the work <strong>Estimation of Theory-Implied Correlation Matrices</strong>:
“A problem of empirical correlation matrices is that they are purely observation driven, and do not impose a structural view of the investment universe, supported by economic theory.”</p>
<p>Using the TIC approach allows us to include forward-looking views to the world, instead of only backward-looking views from the empirical correlations matrix.</p>
<p>The economic theory in the algorithm is represented in terms of a tree graph. “The tree can include any number of levels needed, each branch should have one or more leaves, some branches may include more levels than others”.</p>
<p>An example of how the theoretical structure can be used is the <a class="reference external" href="https://www.msci.com/gics">MSCI’s Global Industry Classification Standard (GICS)</a> for investments. Using this structure, each stock can be classified using four levels of depth.</p>
<p>To use a tree as the input to the algorithm, it should have the “bottom-up order of the columns, where the leftmost column is corresponding to terminal leaves and the rightmost columns corresponding to the tree’s root”. An  example of a tree graph according to the GICS:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Ticker</p></th>
<th class="head"><p>Sub-Industry</p></th>
<th class="head"><p>Industry</p></th>
<th class="head"><p>Industry Group</p></th>
<th class="head"><p>Sector</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>A UN Equity</p></td>
<td><p>35203010</p></td>
<td><p>352030</p></td>
<td><p>3520</p></td>
<td><p>35</p></td>
</tr>
<tr class="row-odd"><td><p>AAL UW Equity</p></td>
<td><p>20302010</p></td>
<td><p>203020</p></td>
<td><p>2030</p></td>
<td><p>20</p></td>
</tr>
<tr class="row-even"><td><p>AAP UN Equity</p></td>
<td><p>25504050</p></td>
<td><p>255040</p></td>
<td><p>2550</p></td>
<td><p>25</p></td>
</tr>
<tr class="row-odd"><td><p>AAPL UW Equity</p></td>
<td><p>45202030</p></td>
<td><p>452020</p></td>
<td><p>4520</p></td>
<td><p>45</p></td>
</tr>
</tbody>
</table>
<p>The empirical correlation matrix used in the TIC algorithm is estimated on historical observations. “It should be symmetric and have a main diagonal of 1s. But it doesn’t need to be invertible, positive-definite or non-singular”.</p>
</section>
<section id="steps-of-the-algorithm">
<h4>Steps of the Algorithm<a class="headerlink" href="#steps-of-the-algorithm" title="Permalink to this heading">¶</a></h4>
<p>The TIC algorithm consists of three steps:</p>
<ol class="arabic simple">
<li><p>In the first step, the theoretical tree graph structure of the assets is fit on the evidence presented by the empirical correlation matrix.</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>If there is no top level of the tree (tree root), this level is added so that all variables are included in one general cluster.</p></li>
<li><p>The empirical correlation matrix is transformed into a matrix of distances using the above formula:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[d_{i,j} = \sqrt{\frac{1}{2}(1 - \rho_{i,j})}\]</div>
<ul class="simple">
<li><p>For each level of the tree, the elements are grouped by elements from the higher level. The algorithm iterates from the lowest to the highest level of the tree.</p></li>
<li><p>A linkage object is created for these grouped elements based on their distance matrix using the <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html">SciPy linkage function</a>. Each link in the linkage object is an array representing a cluster of two elements and has the following data as elements:</p></li>
</ul>
<blockquote>
<div><ul class="simple">
<li><p>ID of the first element in a cluster</p></li>
<li><p>ID of the second element in a cluster</p></li>
<li><p>Distance between the elements</p></li>
<li><p>Number of atoms (simple elements from the portfolio and not clusters) inside</p></li>
</ul>
</div></blockquote>
<ul class="simple">
<li><p>A linkage object is transformed to reflect the previously created clusters.</p></li>
<li><p>A transformed local linkage object is added to the global linkage object</p></li>
<li><p>The distance matrix is adjusted to the newly created clusters - elements that are now in the new clusters are replaced by the clusters in the distance matrix. The distance from the new clusters to the rest of the elements in the distance matrix is calculated as a weighted average of distances of two elements in a cluster to the other elements. The weight is the number of atoms in an element. So, the formula is:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[DistanceCluster = \frac{Distance_1 * NumAtoms_1 + Distance_2 * NumAtoms_2}{NumAtoms_1 + NumAtoms_2}\]</div>
<ul class="simple">
<li><p>The linkage object, representing a dendrogram of all elements in a portfolio is the result of the first step of the algorithm. It sequentially clusters two elements together, while measuring how closely together the two elements are, until all elements are subsumed within the same cluster.</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>In the second step, a correlation matrix is derived from the linkage object.</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>One by one, the clusters (each represented by a link in the linkage object) are decomposed to lists of atoms contained in each of the two elements of the cluster.</p></li>
<li><p>The elements on the main diagonal of the resulting correlation matrix are set to 1s. The off-diagonal correlations between the variables are computed as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\rho_{i,j} = 1 - 2 * d_{i,j}^{2}\]</div>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p>In the third step, the correlation matrix is de-noised.</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>The eigenvalues and eigenvectors of the correlation matrix are calculated.</p></li>
<li><p>Marcenko-Pastur distribution is fit to the eigenvalues of the correlation matrix and the maximum theoretical eigenvalue is calculated.</p></li>
<li><p>This maximum theoretical eigenvalue is set as a threshold and all the eigenvalues above the threshold are shrinked.</p></li>
<li><p>The de-noised correlation matrix is calculated back from the eigenvectors and the new eigenvalues.</p></li>
</ul>
</div></blockquote>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p>The algorithm for de-noising the correlation and the covariance matrix is implemented in the RiskEstimators class of the mlfinlab package. It is described in more detail <a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/RiskEstimators/RiskEstimators.ipynb">here</a>.</p></li>
<li><p>This algorithm is described in more detail in the paper <strong>Estimation of Theory-Implied Correlation Matrices</strong> <em>by</em> Marcos Lopez de Prado <a class="reference external" href="https://papers.ssrn.com/abstract_id=3484152">available here</a>.</p></li>
</ul>
</div>
</section>
<section id="correlation-matrix-distance">
<h4>Correlation Matrix Distance<a class="headerlink" href="#correlation-matrix-distance" title="Permalink to this heading">¶</a></h4>
<p>The similarity of the Empirical correlation matrix and the Theory-implied correlation matrix can be measured using the correlation matrix distance introduced by Herdin and Bonek.</p>
<p>The distance is calculated as:</p>
<div class="math notranslate nohighlight">
\[d[\sum_{1},\sum_{2}] = 1 - \frac{tr(\sum_{1}\sum_{2})}{||\sum_{1}||_f||\sum_{2}||_f}\]</div>
<p>Where <span class="math notranslate nohighlight">\(\sum_{1},\sum_{2}\)</span> are the two correlation matrices and the <span class="math notranslate nohighlight">\(||.||_f\)</span> is the Frobenius norm.</p>
<p>From the work <strong>Estimation of Theory-Implied Correlation Matrices</strong>:
“The distance <span class="math notranslate nohighlight">\(\sum_{1},\sum_{2}\)</span> measures the orthogonality between the considered correlation matrices. It becomes zero if the correlation matrices are equal up to a scaling factor, and one if they differ to a maximum extent”.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The Correlation Matrix Distance metric is described in more detail in the work <strong>AMIMO Correlation Matrix based Metric for Characterizing Non-Stationarity</strong> <em>by</em> Markus Herdin <em>and</em> Ernst Bonek <a class="reference external" href="https://publik.tuwien.ac.at/files/pub-et_8791.pdf">available here</a>.</p>
</div>
</section>
<section id="implementation">
<h4>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h4>
</section>
<section id="example-code">
<h4>Example Code<a class="headerlink" href="#example-code" title="Permalink to this heading">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.portfolio_optimization.tic</span> <span class="kn">import</span> <span class="n">TIC</span>

<span class="c1"># Reading data</span>
<span class="n">tree_classification</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;TREE_FILE_PATH&#39;</span><span class="p">)</span>
<span class="n">stock_returns</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;DATA_FILE_PATH&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span>

<span class="c1"># Calculating the empirical correlation matrix</span>
<span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">stock_returns</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>

<span class="c1"># Calculating the relation of sample length T to the number of variables N</span>
<span class="c1"># It&#39;s used for de-noising the TIC matrix</span>
<span class="n">tn_relation</span> <span class="o">=</span> <span class="n">stock_returns</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">stock_returns</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># The class that contains the TIC algorithm</span>
<span class="n">tic</span> <span class="o">=</span> <span class="n">TIC</span><span class="p">()</span>

<span class="c1"># Calculating the Theory-Implied Correlation matrix</span>
<span class="n">tic_matrix</span> <span class="o">=</span> <span class="n">tic</span><span class="o">.</span><span class="n">tic_correlation</span><span class="p">(</span><span class="n">tree_classification</span><span class="p">,</span> <span class="n">corr_matrix</span><span class="p">,</span> <span class="n">tn_relation</span><span class="p">,</span> <span class="n">kde_bwidth</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Calculating the distance between the empirical and the theory-implied correlation matrices</span>
<span class="n">matrix_distance</span> <span class="o">=</span> <span class="n">tic</span><span class="o">.</span><span class="n">corr_dist</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span> <span class="n">tic_matrix</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="research-notebooks">
<h4>Research Notebooks<a class="headerlink" href="#research-notebooks" title="Permalink to this heading">¶</a></h4>
<p>The following research notebook can be used to better understand how the algorithms within this module can be used on real data.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Portfolio%20Optimisation%20Tutorials/Theory%20Implied%20Correlation%20(TIC)/TIC.ipynb">Theory-Implied Correlation Notebook</a></p></li>
</ul>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-online_portfolio_selection/introduction"></span><div class="admonition note" id="online-portfolio-selection-introduction">
<p class="admonition-title">Note</p>
<p>Special thanks to Li and Hoi’s <a class="reference external" href="https://github.com/OLPS/OLPS">OLPS</a> toolbox and
Marigold’s <a class="reference external" href="https://github.com/Marigold/universal-portfolios">Universal Portfolio</a> for the implementation
of the Online Portfolio Selection module.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The online portfolio selection module contains different algorithms that are used for asset allocation and optimizing strategies. Each
algorithm is encapsulated in its own class and has a public method called <code class="docutils literal notranslate"><span class="pre">allocate()</span></code> which calculates the weight allocations
on the specific user data. This way, each implementation can be called in the same way and makes it simple for users to use them.</p>
</div>
<hr class="docutils" />
<section id="introduction">
<h3>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h3>
<section id="online-portfolio-selection">
<h4>Online Portfolio Selection<a class="headerlink" href="#online-portfolio-selection" title="Permalink to this heading">¶</a></h4>
<p>Online Portfolio Selection can broadly be divided into two parts: Portfolio Selection and Online Learning.</p>
<p>Portfolio Selection is a sequential allocation among a set of assets to maximize the final return of investment.
Every day a portfolio manager is given a task to decide the allocation of capital, and we formulate the problem
so that the weights and the daily returns can be represented in a vector format. The product of the two will represent
the daily returns of the strategy.</p>
<p>Online Learning utilizes computationally efficient algorithms to handle large scale applications. You could have
the best strategy in the world that predicts the stock market movements 100% of the time. However, if your strategy
takes one day to run, you would not be able to capture the opportunity presented by the situation. It is imperative
that these update algorithms can be done in a set amount of time and preferably a quick one to that. This would
actually expand the application of this selection algorithm to not only a daily or weekly time frame to something
that can be applied to even intraday or mid-frequency settings as well.</p>
<p>It is also noteworthy to understand that traditional theories for portfolio selection, such as Markowitz’s
Portfolio Theory, optimize the balance between the portfolio’s risks and returns. However, online portfolio
selection is founded on the capital growth theory, which solely focuses on maximizing the returns of the current
portfolio. Because the capital growth theory primarily relies on the Kelly criterion, traditional metrics such as
Sharpe ratios and maximum drawdowns are more so less useful. The primary metric in this situation becomes the log
of final wealth, which in turn indicates the maximum of final wealth.</p>
<a class="reference internal image-reference" href="_images/diagram.png"><img alt="_images/diagram.png" src="_images/diagram.png" style="width: 99%;" /></a>
<p>Four different strategies are currently implemented in the Online Portfolio Selection module with the
above diagram available for a general overview of some of the strategies.</p>
<ol class="arabic simple">
<li><p>Benchmarks</p></li>
<li><p>Momentum</p></li>
<li><p>Mean Reversion</p></li>
<li><p>Pattern Matching</p></li>
</ol>
<p>All of the online portfolio selection strategies will be built on top of the base constructor class <code class="docutils literal notranslate"><span class="pre">OLPS</span></code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Online Portfolio Selection module does not support:</p>
<ol class="arabic simple">
<li><p>Transaction costs</p></li>
<li><p>Short selling</p></li>
<li><p>Data points with null or 0 value.</p></li>
</ol>
</div>
</section>
<section id="import">
<h4>Import<a class="headerlink" href="#import" title="Permalink to this heading">¶</a></h4>
<p>Strategies can be imported by using variations of the following lines.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import all strategies.</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Import all benchmark strategies.</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection.benchmarks</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Import all momentum strategies.</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection.momentum</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Import all mean reversion strategies.</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection.mean_reversion</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Import all pattern matching strategies.</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection.pattern_matching</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Import a specific buy and hold strategy.</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection</span> <span class="kn">import</span> <span class="n">BAH</span>

<span class="c1"># Import buy and hold and universal portfolio.</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection</span> <span class="kn">import</span> <span class="n">BAH</span><span class="p">,</span> <span class="n">UP</span>
</pre></div>
</div>
</section>
<section id="initialize">
<h4>Initialize<a class="headerlink" href="#initialize" title="Permalink to this heading">¶</a></h4>
<p>Strategies are first initialized to create an object. Certain strategies require parameters to initialize.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize Buy and Hold.</span>
<span class="n">bah</span> <span class="o">=</span> <span class="n">BAH</span><span class="p">()</span>

<span class="c1"># Initialize Passive Aggressive Mean Reversion.</span>
<span class="n">pamr</span> <span class="o">=</span> <span class="n">PAMR</span><span class="p">(</span><span class="n">optimization_method</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">agg</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Initialize Correlation Driven Nonparametric Learning - K</span>
<span class="n">cornk</span> <span class="o">=</span> <span class="n">CORNK</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="allocate">
<h4>Allocate<a class="headerlink" href="#allocate" title="Permalink to this heading">¶</a></h4>
<p>All strategies use <code class="docutils literal notranslate"><span class="pre">allocate()</span></code> to calculate the portfolio weights based on the given data. The user must supply the given data in a <code class="docutils literal notranslate"><span class="pre">pd.DataFrame</span></code> that is indexed by time.</p>
<p>Three additional options are available for the <code class="docutils literal notranslate"><span class="pre">allocate</span></code> method.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">weights</span></code> is an option for the user to supply the initial weights.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">resample_by</span></code> changes the reallocation period for the portfolio.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">verbose</span></code> prints out a progress bar to allow users to follow the status.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize Buy and Hold.</span>
<span class="n">bah</span> <span class="o">=</span> <span class="n">BAH</span><span class="p">()</span>

<span class="c1"># Allocate with no additional inputs.</span>
<span class="n">bah</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">price_data</span><span class="p">)</span>

<span class="c1"># Allocate with monthly portfolio rebalancing.</span>
<span class="n">bah</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">price_data</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;M&#39;</span><span class="p">)</span>

<span class="c1"># Allocate with user given weights.</span>
<span class="n">bah</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">price_data</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">some_weight</span><span class="p">)</span>

<span class="c1"># Allocate with printed progress bar.</span>
<span class="n">bah</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">price_data</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="_images/allocate.png"><img alt="_images/allocate.png" src="_images/allocate.png" style="width: 99%;" /></a>
</section>
<section id="result">
<h4>Result<a class="headerlink" href="#result" title="Permalink to this heading">¶</a></h4>
<p>Upon weights allocation the possible outputs are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.weights</span></code> (np.array) Final portfolio weights prediction.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.all_weights</span></code> (pd.DataFrame) Portfolio weights for the time period.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.asset_name</span></code> (list) Name of assets.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.number_of_assets</span></code> (int) Number of assets.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.time</span></code> (datetime) Time index of the given data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.length_of_time</span></code> (int) Number of time periods.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.relative_return</span></code> (np.array) Relative returns of the assets.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.portfolio_return</span></code> (pd.DataFrame) Cumulative portfolio returns over time.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.asset_prices</span></code> (pd.DataFrame)`` Historical asset prices (daily close).</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">self.all_weights</span></code> returns all the predicted weights for the entire given dataset.
To interpret the results, if you see that for <code class="docutils literal notranslate"><span class="pre">2015-5-16</span></code> the final weights are
<code class="docutils literal notranslate"><span class="pre">{A:</span> <span class="pre">0.2,</span> <span class="pre">B:0.3,</span> <span class="pre">C:0.5}</span></code>, the strategy has predicted those particular weights given
the data until <code class="docutils literal notranslate"><span class="pre">2015-05-15</span></code>. The daily returns (or any periodic returns) are then
calculated by the dot product of the weights and returns.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to put this in a production setting, it is important to see if the particular
algorithm will require the entire dataset or just a particular window. As an example,
<code class="docutils literal notranslate"><span class="pre">OLMAR-1</span></code> and <code class="docutils literal notranslate"><span class="pre">RMR</span></code> both have a given window value as a lookback period. Therefore,
as long as we have the given weights that we calculated from the previous run and
sufficient data to cover the window value, we can output the new weights by adjusting the
<code class="docutils literal notranslate"><span class="pre">weights</span></code> for <code class="docutils literal notranslate"><span class="pre">.allocate()</span></code> and the given <code class="docutils literal notranslate"><span class="pre">asset_prices</span></code>. However, this method will
not work for strategies that look back for the entire period. <code class="docutils literal notranslate"><span class="pre">OLMAR-2</span></code> is an exponential
moving average algorithm that incorporates data for all periods and <code class="docutils literal notranslate"><span class="pre">CORN</span></code> algorithms look
at all historical values as well.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize Buy and Hold.</span>
<span class="n">bah</span> <span class="o">=</span> <span class="n">BAH</span><span class="p">()</span>

<span class="c1"># Allocate with no additional inputs.</span>
<span class="n">bah</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">price_data</span><span class="p">)</span>

<span class="c1"># All weights for the portfolio.</span>
<span class="n">bah</span><span class="o">.</span><span class="n">all_weights</span>

<span class="c1"># Portfolio returns.</span>
<span class="n">bah</span><span class="o">.</span><span class="n">portfolio_return</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="_images/portfolio_return.png"><img alt="_images/portfolio_return.png" src="_images/portfolio_return.png" style="width: 33%;" /></a>
</section>
<section id="data">
<h4>Data<a class="headerlink" href="#data" title="Permalink to this heading">¶</a></h4>
<p>Data selection is one of the hardest problems in research. With numerous test sets and a vast amount of resources
available to the public, it is tempting to overfit and choose the data that best represent your hypothesis.
However, conclusions that are reached from these weak models are more prone to outliers and can have a narrow
scope for applications. Online portfolio selection also deals with the same issues as it is heavily dependent
on the data available.</p>
<p>Traditional papers for online portfolio selection have consistently used the same datasets and developed their
arguments to improve on the performance of the prior papers. Thomas Cover first used a NYSE dataset that
contained 36 stocks from 1962 to 1984. Allan Borodin collected three datasets: 88 stocks from the Toronto
Stock Exchange from 1994 to 1998, the largest 25 stocks by market capitalization on S&amp;P500 from 1998 to 2003,
and 30 stocks from DJIA from 2001 to 2003. Bin Li and Steven Hoi introduced the MSCI World Index from 2006
to 2010 to add an additional perspective to the problem.</p>
<p>All of these datasets have different characteristics as Cover’s NYSE dataset all increased in value whereas
most assets in DJIA lost value. The S&amp;P 500 data contains both a bull and bear market environment, and the
stocks from TSE originate from a less liquid market and a long bear run. However, these mediations do not
seem enough to justify the applications and practicality of the newest module.</p>
<p>To offset these older datasets in my research, I’ll expand the MSCI world index to look back from 1993
to 2020 and also include 44 largest US stocks by market capitalization from 2011 to 2020. Through a different
lens of selection, I hope to introduce the readers to a more practical and familiar set of stocks to
understand the module in a more intuitive way.</p>
<section id="nyse-1962-1984">
<h5>NYSE 1962-1984<a class="headerlink" href="#nyse-1962-1984" title="Permalink to this heading">¶</a></h5>
<p>This is the original NYSE data that Thomas Cover used for his papers. Although it covers a lot of sectors
and should have been useful when the paper was published, it is difficult to gauge if this dataset adds
much value now because of the time difference.</p>
<p>Strategies that worked a year ago could quickly lose their value as the paradigm shifts. This data is
collected almost 60 years ago, and markets nowadays have many complex movements that cannot be comprehended
with data from a long time ago. Results from this data should be approached with a grain of salt.</p>
<a class="reference internal image-reference" href="_images/nyse_price.png"><img alt="_images/nyse_price.png" src="_images/nyse_price.png" style="width: 99%;" /></a>
<p>Generally, most assets in this dataset increased by a significant amount. The notable outperforming companies
are American Brands and Commercial Metals, and the least performing stock, DuPont, still ended with 2.9 times
returns as no stocks in this list decreased in value.</p>
</section>
<section id="djia-2001-2003">
<h5>DJIA 2001-2003<a class="headerlink" href="#djia-2001-2003" title="Permalink to this heading">¶</a></h5>
<p>This is a more recent dataset that involves companies that are still well known to us. 2001 to 2003 covers
a bear market run that should be useful to see how our strategies are affected in times of general downturn.
Most of these assets lost in value.</p>
<a class="reference internal image-reference" href="_images/djia_price.png"><img alt="_images/djia_price.png" src="_images/djia_price.png" style="width: 99%;" /></a>
<p>DJIA from 2001 to 2003 provides strikingly different patterns compared to the previous NYSE data.
Only 5 companies increased in price as most declined at a steady rate.</p>
</section>
<section id="tse-1994-1998">
<h5>TSE 1994-1998<a class="headerlink" href="#tse-1994-1998" title="Permalink to this heading">¶</a></h5>
<p>The Toronto Stock Exchange data includes a collection that may be unfamiliar to most researchers. It is an
interesting universe with half of the stocks decreasing in value. With a combination of both overperforming
and underperforming stocks, selection strategies will need to identify the ups and downs to have profitable returns.</p>
<a class="reference internal image-reference" href="_images/tse_price.png"><img alt="_images/tse_price.png" src="_images/tse_price.png" style="width: 99%;" /></a>
<p>Half of the stocks decreasing in value. With a combination of both overperforming and underperforming stocks,
selection strategies need to identify the ups and downs to have profitable returns.</p>
</section>
<section id="sp500-1998-2003">
<h5>SP500 1998-2003<a class="headerlink" href="#sp500-1998-2003" title="Permalink to this heading">¶</a></h5>
<p>This dataset also includes the bear and bull run during turbulent times. It is longer than the DJIA data by
3 more years and includes many companies that are familiar to us. This will be a good comparison to our new
US Equity dataset which looks at more recent history for most of these companies.</p>
<a class="reference internal image-reference" href="_images/sp500_price.png"><img alt="_images/sp500_price.png" src="_images/sp500_price.png" style="width: 99%;" /></a>
<p>This dataset also includes the bear and bull run during turbulent times. It is longer than the DJIA data by 3 years
and includes many companies that are familiar to us. SP500 during this time goes through the bear market in 2000,
and in the long run, all but 5 companies increase in value.</p>
</section>
<section id="msci-1993-2020">
<h5>MSCI 1993-2020<a class="headerlink" href="#msci-1993-2020" title="Permalink to this heading">¶</a></h5>
<p>I used the MSCI Developed Markets Index from 1993/01/01, which includes 23 countries:</p>
<ul class="simple">
<li><p>Americas: USA, Canada</p></li>
<li><p>Europe &amp; Middle East: Austria, Belgium, Denmark, Finland, France, Germany, Ireland, Israel, Italy, Netherlands, Norway, Portugal, Spain, Sweden, Switzerland, United Kingdom</p></li>
<li><p>Pacific: Australia, Hong Kong, Japan, New Zealand, Singapore</p></li>
</ul>
<p>Different from traditional assets, the world indexes capture much more than just the price changes of individual
companies. With an overarching representation of the countries’ market states, these market indexes will present a
different idea for applications of OLPS strategies.</p>
<a class="reference internal image-reference" href="_images/msci_price.png"><img alt="_images/msci_price.png" src="_images/msci_price.png" style="width: 99%;" /></a>
<p>Finland is not the first country to come in mind with metrics like these, but the rise and fall of Finland
around the 2000s puts every other country aside. Most countries show movements that are strongly correlated with each other.</p>
</section>
<section id="us-equity-2011-2020">
<h5>US Equity 2011-2020<a class="headerlink" href="#us-equity-2011-2020" title="Permalink to this heading">¶</a></h5>
<p>For a more recent dataset, I collected the 44 largest US stocks based on market capitalization according to a
Financial Times <a class="reference external" href="http://media.ft.com/cms/253867ca-1a60-11e0-b003-00144feab49a.pdf">report</a>.</p>
<p>Although included in the original report, I did not include United Technologies and Kraft Foods due to M&amp;A
and also excluded Hewlett-Packard because of the company split in 2015.</p>
<p>This dataset will be particularly interesting because it also includes the recent market impact by the coronavirus
as well. With 10 years of continuous bull run after the financial crisis in 2008, we can examine which strategy
was the most robust to the rapidly changing market paradigm in the last month.</p>
<p>The companies included are:</p>
<p>Exxon Mobil, Apple, Microsoft, Berkshire Hathaway, General Electric, Walmart, Chevron, IBM, PG, ATT, Johnson and Johnson,
JP Morgan, Wells Fargo, Oracle, Coca-Cola, Google, Pfizer, Citi, Bank of America, Intel, Schlumberger, Cisco, Merck,
Philip Morris, PepsiCo, ConocoPhillips, Goldman Sachs, McDonald’s, Amazon, Qualcomm, Occidental Petroleum, Abbott Laboratories,
Walt Disney, 3M, Comcast, Caterpillar, General Motors, Home Depot, Ford, Freeport-McMoran Copper &amp; Gold, United Parcel Service,
Amgen, US Bancorp, American Express</p>
<a class="reference internal image-reference" href="_images/equity_price.png"><img alt="_images/equity_price.png" src="_images/equity_price.png" style="width: 99%;" /></a>
<p>Amazon has been the clear winner for the past 10 years with Microsoft and Home Depot being a close second.
One key note for this data will be to see how our strategies fared during the downturn caused by the coronavirus.</p>
</section>
</section>
</section>
<span id="document-online_portfolio_selection/benchmarks"></span><div class="admonition note" id="online-portfolio-selection-benchmarks">
<p class="admonition-title">Note</p>
<p>Strategies were implemented with modifications from:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1212.2129">Li, B., Hoi, S. C.H., 2012. OnLine Portfolio Selection: A Survey. ACM Comput. Surv. V, N, Article A (December 2012), 33 pages.</a></p></li>
</ol>
</div>
<section id="benchmarks">
<h3>Benchmarks<a class="headerlink" href="#benchmarks" title="Permalink to this heading">¶</a></h3>
<p>Before we dive into the more interesting and complex models of portfolio selection, we will begin our analysis with benchmarks.
As unappealing as benchmarks are, traditional strategies such as tracking the S&amp;P 500 have been hugely successful.</p>
<p>Typically these are implemented in hindsight, so future data is often incorporated within the selection algorithm. For real-life
applications, we do not have access to future data from the present, so strategies here should be taken with a grain of salt.</p>
<p>There are four benchmarks strategies implemented in the Online Portfolio Selection module.</p>
<hr class="docutils" />
<section id="buy-and-hold">
<h4>Buy and Hold<a class="headerlink" href="#buy-and-hold" title="Permalink to this heading">¶</a></h4>
<p>Buy and Hold is a strategy where an investor invests in an initial portfolio and never rebalances it. The portfolio weights, however, change
as time goes by because the underlying assets change in prices.</p>
<p>Returns for Buy and Hold can be calculated by multiplying the initial weight and the cumulative product of relative returns.</p>
<div class="math notranslate nohighlight">
\[S_t(BAH(b_1)) = b_1 \cdot \left(\overset{t}{\underset{n=1}{\bigodot}} x_n\right)\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S(t)\)</span> is the total portfolio returns at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(b_t\)</span> is the portfolio vector at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_t\)</span> is the price relative change at time <span class="math notranslate nohighlight">\(t\)</span>. It is calculated by <span class="math notranslate nohighlight">\(\frac{p_t}{p_{t-1}}\)</span>, where <span class="math notranslate nohighlight">\(p_t\)</span> is the price at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\bigodot\)</span> is the element-wise cumulative product. In this case, the cumulative product represents the overall change in prices.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If no weights are given for the <code class="docutils literal notranslate"><span class="pre">allocate</span></code> method for Buy and Hold, uniform weights will be used.</p>
</div>
<section id="implementation">
<h5>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h5>
</section>
<section id="example-code">
<h5>Example Code<a class="headerlink" href="#example-code" title="Permalink to this heading">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Read in data.</span>
<span class="n">stock_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span>

<span class="c1"># Compute Buy and Hold with uniform weights as no weights are given.</span>
<span class="n">bah</span> <span class="o">=</span> <span class="n">BAH</span><span class="p">()</span>
<span class="n">bah</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute Buy and Hold weights with user given weights.</span>
<span class="n">bah</span> <span class="o">=</span> <span class="n">BAH</span><span class="p">()</span>
<span class="n">bah</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">some_weight</span><span class="p">)</span>

<span class="c1"># Get the latest predicted weights.</span>
<span class="n">bah</span><span class="o">.</span><span class="n">weights</span>

<span class="c1"># Get all weights for the strategy.</span>
<span class="n">bah</span><span class="o">.</span><span class="n">all_weights</span>

<span class="c1"># Get portfolio returns.</span>
<span class="n">bah</span><span class="o">.</span><span class="n">portfolio_return</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="best-stock">
<h4>Best Stock<a class="headerlink" href="#best-stock" title="Permalink to this heading">¶</a></h4>
<p>Best Stock strategy chooses the best performing asset in hindsight.</p>
<p>The best performing asset is determined with an argmax equation stated below. The portfolio selection strategy searches for the asset
that increases the most in price for the given time period.</p>
<div class="math notranslate nohighlight">
\[b_0 = \underset{b \in \Delta_m}{\arg\max} \: b \cdot \left(\overset{n}{\underset{t=1}{\bigodot}}  x_t \right)\]</div>
<p>Once the initial portfolio has been determined, the final weights can be represented as buying and holding the initial weight.</p>
<div class="math notranslate nohighlight">
\[S_t(BEST) = \underset{b \in \Delta_m}{\max} b \cdot \left(\overset{t}{\underset{n=1}{\bigodot}}  x_n \right) = S_t(BAH(b_0))\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S(t)\)</span> is the total portfolio returns at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(b_t\)</span> is the portfolio vector at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_t\)</span> is the price relative change at time <span class="math notranslate nohighlight">\(t\)</span>. It is calculated by <span class="math notranslate nohighlight">\(\frac{p_t}{p_{t-1}}\)</span>, where <span class="math notranslate nohighlight">\(p_t\)</span> is the price at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\bigodot\)</span> is the element-wise cumulative product. In this case, the cumulative product represents the overall change in prices.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta_m\)</span> is the simplex domain. The sum of all elements is 1, and each element is in the range of [0, 1].</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Weights given for the Best Stock’s <code class="docutils literal notranslate"><span class="pre">allocate</span></code> method will not change initial weights because best stock
inherently decides the weights for all time period by choosing the best performing asset.</p>
</div>
<section id="id1">
<h5>Implementation<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h5>
</section>
<section id="id2">
<h5>Example Code<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Read in data.</span>
<span class="n">stock_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span>

<span class="c1"># Compute Best Stock weights with no weights given.</span>
<span class="n">beststock</span> <span class="o">=</span> <span class="n">BestStock</span><span class="p">()</span>
<span class="n">beststock</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Get the latest predicted weights.</span>
<span class="n">beststock</span><span class="o">.</span><span class="n">weights</span>

<span class="c1"># Get all weights for the strategy.</span>
<span class="n">beststock</span><span class="o">.</span><span class="n">all_weights</span>

<span class="c1"># Get portfolio returns.</span>
<span class="n">beststock</span><span class="o">.</span><span class="n">portfolio_return</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="constant-rebalanced-portfolio">
<h4>Constant Rebalanced Portfolio<a class="headerlink" href="#constant-rebalanced-portfolio" title="Permalink to this heading">¶</a></h4>
<p>Constant Rebalanced Portfolio rebalances to a certain portfolio weight every time period. This particular weight can be set by the user,
and if there are no inputs, it will automatically allocate equal weights to all assets. The total returns for a CRP can be calculated by
taking the cumulative product of the weight and relative returns matrix.</p>
<div class="math notranslate nohighlight">
\[S_t(CRP(b)) = \overset{t}{\underset{n=1}{\prod}} \:  b^{\top}x_n\]</div>
<p>Once the initial portfolio has been determined, the final weights can be represented as buying and holding the initial weight.</p>
<div class="math notranslate nohighlight">
\[S_t(BEST) = \underset{b \in \Delta_m}{\max} b \cdot \left(\overset{t}{\underset{n=1}{\bigodot}}  x_n \right) = S_t(BAH(b_0))\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S(t)\)</span> is the total portfolio returns at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(b_t\)</span> is the portfolio vector at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_t\)</span> is the price relative change at time <span class="math notranslate nohighlight">\(t\)</span>. It is calculated by <span class="math notranslate nohighlight">\(\frac{p_t}{p_{t-1}}\)</span>, where <span class="math notranslate nohighlight">\(p_t\)</span> is the price at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\bigodot\)</span> is the element-wise cumulative product. In this case, the cumulative product represents the overall change in prices.</p></li>
<li><p><span class="math notranslate nohighlight">\(\prod\)</span> is the product of all elements.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta_m\)</span> is the simplex domain. The sum of all elements is 1, and each element is in the range of [0, 1].</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p>If only initial parameters are given, CRP will use the parameters as initial weights.</p></li>
<li><p>If weights are only given through <code class="docutils literal notranslate"><span class="pre">allocate</span></code> method, CRP will use the given weights.</p></li>
<li><p>If both initial parameters and weights are given, CRP will override the <code class="docutils literal notranslate"><span class="pre">allocate</span></code> weights and use the initial parameters.</p></li>
<li><p>If neither parameters or weights are given, CRP will use uniform weights.</p></li>
</ul>
</div>
<section id="id3">
<h5>Implementation<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h5>
</section>
<section id="id4">
<h5>Example Code<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Read in data.</span>
<span class="n">stock_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span>

<span class="c1"># Compute Constant Rebalanced Portfolio with unniform weights as no parameters or weights are given.</span>
<span class="n">crp</span> <span class="o">=</span> <span class="n">CRP</span><span class="p">()</span>
<span class="n">crp</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute Constant Rebalanced Portfolio with given weights.</span>
<span class="n">crp</span> <span class="o">=</span> <span class="n">CRP</span><span class="p">()</span>
<span class="n">crp</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">some_weight</span><span class="p">)</span>

<span class="c1"># Compute Constant Rebalanced Portfolio with initialized parameters.</span>
<span class="n">crp</span> <span class="o">=</span> <span class="n">CRP</span><span class="p">(</span><span class="n">some_weight</span><span class="p">)</span>
<span class="n">crp</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">)</span>

<span class="c1"># Compute Constant Rebalanced Portfolio with parameters and given weights.</span>
<span class="c1"># In this case, CRP will override the given weights with the parameters.</span>
<span class="n">crp</span> <span class="o">=</span> <span class="n">CRP</span><span class="p">(</span><span class="n">used_weight</span><span class="p">)</span>
<span class="n">crp</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">ignored_weight</span><span class="p">)</span>

<span class="c1"># Get the latest predicted weights.</span>
<span class="n">crp</span><span class="o">.</span><span class="n">weights</span>

<span class="c1"># Get all weights for the strategy.</span>
<span class="n">crp</span><span class="o">.</span><span class="n">all_weights</span>

<span class="c1"># Get portfolio returns.</span>
<span class="n">crp</span><span class="o">.</span><span class="n">portfolio_return</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="best-constant-rebalanced-portfolio">
<h4>Best Constant Rebalanced Portfolio<a class="headerlink" href="#best-constant-rebalanced-portfolio" title="Permalink to this heading">¶</a></h4>
<p>Best Constant Rebalanced Portfolio is a strategy that is implemented in hindsight, which is similar to Best Stock. It uses the same weight
for all time periods. However, it determines those weights by having the complete market sequence of the past. The objective function for
BCRP aims to maximize portfolio returns with the equation below.</p>
<div class="math notranslate nohighlight">
\[b^{\bf{\star}} = \underset{b_t \in \Delta_m}{\arg\max} \: S_t(CRP(b)) = \underset{b \in \Delta_m}{\arg\max} \overset{t}{\underset{n=1}{\prod}} \:  b^{\top}x_n\]</div>
<p>Once the optimal weight has been determined, the final returns can be calculated by using the CRP returns equation.</p>
<div class="math notranslate nohighlight">
\[S_t(BCRP) = \underset{b \in \Delta_m}{\max} \: S_t(CRP(b)) = S_t(CRP(b^{\bf \star}))\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S(t)\)</span> is the total portfolio returns at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(b_t\)</span> is the portfolio vector at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_t\)</span> is the price relative change at time <span class="math notranslate nohighlight">\(t\)</span>. It is calculated by <span class="math notranslate nohighlight">\(\frac{p_t}{p_{t-1}}\)</span>, where <span class="math notranslate nohighlight">\(p_t\)</span> is the price at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\bigodot\)</span> is the element-wise cumulative product. In this case, the cumulative product represents the overall change in prices.</p></li>
<li><p><span class="math notranslate nohighlight">\(\prod\)</span> is the product of all elements.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta_m\)</span> is the simplex domain. The sum of all elements is 1, and each element is in the range of [0, 1].</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Weights given for the Best Constant Rebalanced Portfolio’s <code class="docutils literal notranslate"><span class="pre">allocate</span></code> method will not change initial
weights because BCRP inherently decides the weights for all time period by choosing the weights.</p>
</div>
<section id="id5">
<h5>Implementation<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h5>
</section>
<section id="id6">
<h5>Example Code<a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Read in data.</span>
<span class="n">stock_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span>

<span class="c1"># Compute Best Constant Rebalanced Portfolio weights with no weights given.</span>
<span class="n">bcrp</span> <span class="o">=</span> <span class="n">BCRP</span><span class="p">()</span>
<span class="n">bcrp</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Get the latest predicted weights.</span>
<span class="n">bcrp</span><span class="o">.</span><span class="n">weights</span>

<span class="c1"># Get all weights for the strategy.</span>
<span class="n">bcrp</span><span class="o">.</span><span class="n">all_weights</span>

<span class="c1"># Get portfolio returns.</span>
<span class="n">bcrp</span><span class="o">.</span><span class="n">portfolio_return</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="research-notebook">
<h4>Research Notebook<a class="headerlink" href="#research-notebook" title="Permalink to this heading">¶</a></h4>
<p>The following <a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Online%20Portfolio%20Selection/Introduction%20to%20Online%20Portfolio%20Selection.ipynb">benchmarks</a>
notebook provides a more detailed exploration of the strategies.</p>
</section>
</section>
<span id="document-online_portfolio_selection/momentum"></span><div class="admonition note" id="online-portfolio-selection-momentum">
<p class="admonition-title">Note</p>
<p>Strategies were implemented with modifications from:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1212.2129">Li, B., Hoi, S. C.H., 2012. OnLine Portfolio Selection: A Survey. ACM Comput. Surv. V, N, Article A (December 2012), 33 pages.</a></p></li>
</ol>
</div>
<section id="momentum">
<h3>Momentum<a class="headerlink" href="#momentum" title="Permalink to this heading">¶</a></h3>
<p>Momentum strategies have been a popular quantitative strategy in recent decades as the simple but powerful trend-following
allows investors to exponentially increase their returns. This module will implement two types of momentum strategy with one
following the best-performing assets in the last period and the other following the Best Constant Rebalanced Portfolio until the last period.</p>
<p>There are three momentum strategies implemented in the Online Portfolio Selection module.</p>
<hr class="docutils" />
<section id="exponential-gradient">
<h4>Exponential Gradient<a class="headerlink" href="#exponential-gradient" title="Permalink to this heading">¶</a></h4>
<p>Exponential Gradient is a momentum strategy that focuses on the best performing asset of the last time period.
The portfolio shifts its weights to the best performing asset of the last period with an adjustment of <span class="math notranslate nohighlight">\(\eta\)</span>, the learning rate.
A higher value of <span class="math notranslate nohighlight">\(\eta\)</span> indicates the aggressiveness of the strategy to match the best performing assets. A lower value of <span class="math notranslate nohighlight">\(\eta\)</span>
indicates the passiveness of the strategy to match the best performing assets.</p>
<div class="math notranslate nohighlight">
\[b_{t+1} = \underset{b \in \Delta_m}{\arg\max} \: \eta \log b \cdot x_t - R(b,b_t)\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(b_t\)</span> is the portfolio vector at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_t\)</span> is the price relative change at time <span class="math notranslate nohighlight">\(t\)</span>. It is calculated by <span class="math notranslate nohighlight">\(\frac{p_t}{p_{t-1}}\)</span>, where <span class="math notranslate nohighlight">\(p_t\)</span> is the price at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(R(b, b_t)\)</span> is the regularization term for exponential gradient. Different update rules will use different regularization terms.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta_m\)</span> is the simplex domain. The sum of all elements is 1, and each element is in the range of [0, 1].</p></li>
</ul>
<p>Exponential Gradients have an extremely efficient computational time that scales with the number of assets,
and broadly speaking, there are three update methods to iteratively update the selection of portfolio weights.</p>
<section id="multiplicative-update">
<h5>Multiplicative Update<a class="headerlink" href="#multiplicative-update" title="Permalink to this heading">¶</a></h5>
<p>David Helmbold first proposed a regularization term that adopts relative entropy in his <a class="reference external" href="https://www.cis.upenn.edu/~mkearns/finread/portfolio.pdf">paper</a>.</p>
<div class="math notranslate nohighlight">
\[R(b,b_t) = \overset{m}{\underset{i=1}{\sum}}b_i \log \frac{b_i}{b_{t,i}}\]</div>
<p>Using log’s first order taylor expansion of <span class="math notranslate nohighlight">\(b_i\)</span></p>
<div class="math notranslate nohighlight">
\[\log b \cdot x_{t, i} \approx \log(b_t \cdot x_{t, i}) + \frac{x_{t, i}}{b_t \cdot x_{t, i}}(b-b_t)\]</div>
<p>Multiplicative update algorithm can be stated as the following.</p>
<div class="math notranslate nohighlight">
\[b_{t+1} = b_t \cdot \exp \left( \eta \frac{x_t}{b_t \cdot x_t} \right) / Z\]</div>
<p>where <span class="math notranslate nohighlight">\(Z\)</span> is a normalization term to sum the weights to 1.</p>
</section>
<section id="gradient-projection">
<h5>Gradient Projection<a class="headerlink" href="#gradient-projection" title="Permalink to this heading">¶</a></h5>
<p>Instead of relative entropy, gradient projection adopts an <span class="math notranslate nohighlight">\(L_2\)</span>-regularization term for the optimization equation.</p>
<div class="math notranslate nohighlight">
\[R(b,b_t) = \frac{1}{2}\overset{m}{\underset{i=1}{\sum}}(b_i - b_{t,i})^2\]</div>
<p>Gradient projection can then be iteratively updated with the following equation.</p>
<div class="math notranslate nohighlight">
\[b_{t+1} = b_t + \eta \cdot \left( \frac{x_t}{b_t \cdot x_t} - \frac{1}{m} \sum_{j=1}^{m} \frac{x_t}{b_t \cdot x_t} \right)\]</div>
</section>
<section id="expectation-maximization">
<h5>Expectation Maximization<a class="headerlink" href="#expectation-maximization" title="Permalink to this heading">¶</a></h5>
<p>Lastly, Expectation Maximization uses a <span class="math notranslate nohighlight">\(\chi^2\)</span> regularization term.</p>
<div class="math notranslate nohighlight">
\[R(b, b_t)=\frac{1}{2}\overset{m}{\underset{i=1}{\sum}}\frac{(b_i - b_{t,i})^2}{b_{t,i}}\]</div>
<p>Then the corresponding update rule becomes</p>
<div class="math notranslate nohighlight">
\[b_{t+1} = b_t \cdot \left( \eta \cdot \left( \frac{x_t}{b_t \cdot x_t} - 1 \right) + 1 \right)\]</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The three update methods have similar returns for the same set of parameters.</p>
</div>
</section>
<section id="parameters">
<h5>Parameters<a class="headerlink" href="#parameters" title="Permalink to this heading">¶</a></h5>
<p>Using <a class="reference external" href="https://optuna.org/">optuna</a>, we experimented with different parameters to provide a general guideline
for the users. For NYSE, a low value of <span class="math notranslate nohighlight">\(\eta\)</span> was optimal, which indicates a lack of a clear momentum strategy.</p>
<a class="reference internal image-reference" href="_images/nyse_eg_eta_0_1.png"><img alt="_images/nyse_eg_eta_0_1.png" src="_images/nyse_eg_eta_0_1.png" style="width: 49%;" /></a>
<a class="reference internal image-reference" href="_images/nyse_eg_eta_1_100.png"><img alt="_images/nyse_eg_eta_1_100.png" src="_images/nyse_eg_eta_1_100.png" style="width: 49%;" /></a>
<p>However, for the MSCI dataset, we see a high value of optimal <span class="math notranslate nohighlight">\(\eta\)</span>, indicating a possible presence
of a momentum strategy.</p>
<a class="reference internal image-reference" href="_images/msci_eg_eta_0_1.png"><img alt="_images/msci_eg_eta_0_1.png" src="_images/msci_eg_eta_0_1.png" style="width: 49%;" /></a>
<a class="reference internal image-reference" href="_images/msci_eg_eta_1_100.png"><img alt="_images/msci_eg_eta_1_100.png" src="_images/msci_eg_eta_1_100.png" style="width: 49%;" /></a>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p>High <span class="math notranslate nohighlight">\(\eta\)</span> : Aggressively follow the best performing asset.</p></li>
<li><p>Low <span class="math notranslate nohighlight">\(\eta\)</span> : Passively follow the best performing asset.</p></li>
</ul>
</div>
</section>
<section id="implementation">
<h5>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h5>
</section>
<section id="example-code">
<h5>Example Code<a class="headerlink" href="#example-code" title="Permalink to this heading">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Read in data.</span>
<span class="n">stock_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span>

<span class="c1"># Compute Multiplicative Update with eta of 0.2 with no given weights.</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">EG</span><span class="p">(</span><span class="n">update_rule</span><span class="o">=</span><span class="s1">&#39;MU&#39;</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">mu</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute Gradient Projection with eta of 0.5 with given weights.</span>
<span class="n">gp</span> <span class="o">=</span> <span class="n">EG</span><span class="p">(</span><span class="n">update_rule</span><span class="o">=</span><span class="s1">&#39;GP&#39;</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">gp</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">some_weight</span><span class="p">)</span>

<span class="c1"># Compute Expectation Maximization with eta of 0.8 with given weights.</span>
<span class="n">em</span> <span class="o">=</span> <span class="n">EG</span><span class="p">(</span><span class="n">update_rule</span><span class="o">=</span><span class="s1">&#39;EM&#39;</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">em</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">some_weight</span><span class="p">)</span>

<span class="c1"># Get the latest predicted weights.</span>
<span class="n">em</span><span class="o">.</span><span class="n">weights</span>

<span class="c1"># Get all weights for the strategy.</span>
<span class="n">mu</span><span class="o">.</span><span class="n">all_weights</span>

<span class="c1"># Get portfolio returns.</span>
<span class="n">gp</span><span class="o">.</span><span class="n">portfolio_return</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="follow-the-leader">
<h4>Follow the Leader<a class="headerlink" href="#follow-the-leader" title="Permalink to this heading">¶</a></h4>
<p>The biggest drawback of using Exponential Gradient is the failure to look at the changes before the latest period.
Follow the Leader mediates this shortfall by directly tracking the Best Constant Rebalanced Portfolio; therefore, FTL
looks at the whole history of the data and calculates the portfolio weights that would have had the maximum returns.</p>
<div class="math notranslate nohighlight">
\[b_{t+1} = b^{\bf{\star}}_t = \underset{b \in \Delta_m}{\arg\max} \overset{t}{\underset{n=1}{\sum}} \: \log(b \cdot x_n)\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(b_t\)</span> is the portfolio vector at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_t\)</span> is the price relative change at time <span class="math notranslate nohighlight">\(t\)</span>. It is calculated by <span class="math notranslate nohighlight">\(\frac{p_t}{p_{t-1}}\)</span>, where <span class="math notranslate nohighlight">\(p_t\)</span> is the price at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta_m\)</span> is the simplex domain. The sum of all elements is 1, and each element is in the range of [0, 1].</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>It is recommended to change <code class="docutils literal notranslate"><span class="pre">verbose=True</span></code> to follow the progress. Because the update algorithm optimizes for
every data point, the time complexity quadratically scales with the number of points.</p>
</div>
<section id="id1">
<h5>Implementation<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h5>
</section>
<section id="id2">
<h5>Example Code<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Read in data.</span>
<span class="n">stock_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span>

<span class="c1"># Compute Follow the Leader with no given weights.</span>
<span class="n">ftl</span> <span class="o">=</span> <span class="n">FTL</span><span class="p">()</span>
<span class="n">ftl</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute Follow the Leader with given weights.</span>
<span class="n">ftl</span> <span class="o">=</span> <span class="n">FTL</span><span class="p">()</span>
<span class="n">ftl</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">some_weight</span><span class="p">)</span>

<span class="c1"># Get the latest predicted weights.</span>
<span class="n">ftl</span><span class="o">.</span><span class="n">weights</span>

<span class="c1"># Get all weights for the strategy.</span>
<span class="n">ftl</span><span class="o">.</span><span class="n">all_weights</span>

<span class="c1"># Get portfolio returns.</span>
<span class="n">ftl</span><span class="o">.</span><span class="n">portfolio_return</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="follow-the-regularized-leader">
<h4>Follow the Regularized Leader<a class="headerlink" href="#follow-the-regularized-leader" title="Permalink to this heading">¶</a></h4>
<p>Follow the Regularized Leader adds an additional regularization term to the objective function for Follow the Leader to prevent a drastic deviation in each period.</p>
<div class="math notranslate nohighlight">
\[b_{t+1} = \underset{b \in \Delta_m}{\arg\max} \overset{t}{\underset{n=1}{\sum}} \: \log(b \cdot x_n) - \frac{\beta}{2}R(b)\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(b_t\)</span> is the portfolio vector at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_t\)</span> is the price relative change at time <span class="math notranslate nohighlight">\(t\)</span>. It is calculated by <span class="math notranslate nohighlight">\(\frac{p_t}{p_{t-1}}\)</span>, where <span class="math notranslate nohighlight">\(p_t\)</span> is the price at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta\)</span> is a penalty variable for the regularization.</p></li>
<li><p><span class="math notranslate nohighlight">\(R(b)\)</span> is the regularization term for follow the regularized leader.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta_m\)</span> is the simplex domain. The sum of all elements is 1, and each element is in the range of [0, 1].</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>It is recommended to change <code class="docutils literal notranslate"><span class="pre">verbose=True</span></code> to follow the progress. Because the update algorithm optimizes for
every data point, the time complexity quadratically scales with the number of points.</p>
</div>
<section id="id3">
<h5>Parameters<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h5>
<p>Using <a class="reference external" href="https://optuna.org/">optuna</a>, we experimented with different parameters to provide a general guideline
for the users. For NYSE, a high regularization was an effective method to generate high returns
as <span class="math notranslate nohighlight">\(\beta\)</span> of 20 was optimal.</p>
<a class="reference internal image-reference" href="_images/nyse_ftrl_beta_0_1.png"><img alt="_images/nyse_ftrl_beta_0_1.png" src="_images/nyse_ftrl_beta_0_1.png" style="width: 49%;" /></a>
<a class="reference internal image-reference" href="_images/nyse_ftrl_beta_1_100.png"><img alt="_images/nyse_ftrl_beta_1_100.png" src="_images/nyse_ftrl_beta_1_100.png" style="width: 49%;" /></a>
<p>However, for the MSCI dataset, we see that regularization is an ineffective means to follow the momentum strategy.
The highest returns are results with <span class="math notranslate nohighlight">\(\beta\)</span> of 0.2. Lower values of beta tend to follow the Uniform Constant Rebalanced Portfolio results.</p>
<a class="reference internal image-reference" href="_images/msci_ftrl_beta_0_1.png"><img alt="_images/msci_ftrl_beta_0_1.png" src="_images/msci_ftrl_beta_0_1.png" style="width: 49%;" /></a>
<a class="reference internal image-reference" href="_images/msci_ftrl_beta_1_100.png"><img alt="_images/msci_ftrl_beta_1_100.png" src="_images/msci_ftrl_beta_1_100.png" style="width: 49%;" /></a>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p>High <span class="math notranslate nohighlight">\(\beta\)</span> : Aggressively follow the best performing asset.</p></li>
<li><p>Low <span class="math notranslate nohighlight">\(\beta\)</span> : Passively follow the best performing asset. Strategy becomes a CRP with the given user weights and CRP-Uniform if no weights are given.</p></li>
</ul>
</div>
</section>
<section id="id5">
<h5>Implementation<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h5>
</section>
<section id="id6">
<h5>Example Code<a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Read in data.</span>
<span class="n">stock_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span>

<span class="c1"># Compute Follow the Regularized Leader with no given weights and beta of 10.</span>
<span class="n">ftrl</span> <span class="o">=</span> <span class="n">FTRL</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ftrl</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute Follow the Leader with given user weights and beta of 0.4.</span>
<span class="n">ftrl</span> <span class="o">=</span> <span class="n">FTRL</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="n">ftrl</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">some_weight</span><span class="p">)</span>

<span class="c1"># Get the latest predicted weights.</span>
<span class="n">ftrl</span><span class="o">.</span><span class="n">weights</span>

<span class="c1"># Get all weights for the strategy.</span>
<span class="n">ftrl</span><span class="o">.</span><span class="n">all_weights</span>

<span class="c1"># Get portfolio returns.</span>
<span class="n">ftrl</span><span class="o">.</span><span class="n">portfolio_return</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="research-notebook">
<h4>Research Notebook<a class="headerlink" href="#research-notebook" title="Permalink to this heading">¶</a></h4>
<p>The following <a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Online%20Portfolio%20Selection/Online%20Portfolio%20Selection%20-%20Momentum.ipynb">momentum</a>
notebook provides a more detailed exploration of the strategies.</p>
</section>
</section>
<span id="document-online_portfolio_selection/mean_reversion"></span><div class="admonition note" id="online-portfolio-selection-mean-reversion">
<p class="admonition-title">Note</p>
<p>Strategies were implemented with modifications from:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://link.springer.com/content/pdf/10.1007/s10994-012-5281-z.pdf">Li, B., Zhao, P., Hoi, S.C. and Gopalkrishnan, V., 2012. PAMR: Passive aggressive mean reversion strategy for portfolio selection. Machine learning, 87(2), pp.221-258.</a></p></li>
<li><p><a class="reference external" href="https://dl.acm.org/doi/pdf/10.1145/2435209.2435213">Li, B., Hoi, S.C., Zhao, P. and Gopalkrishnan, V., 2013. Confidence weighted mean reversion strategy for online portfolio selection. ACM Transactions on Knowledge Discovery from Data (TKDD), 7(1), pp.1-38.</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1206.4626.pdf">Li, B. and Hoi, S.C., 2012. On-line portfolio selection with moving average reversion. arXiv preprint arXiv:1206.4626.</a></p></li>
<li><p><a class="reference external" href="https://core.ac.uk/download/pdf/35455615.pdf">Huang, D., Zhou, J., Li, B., Hoi, S.C. and Zhou, S., 2016. Robust median reversion strategy for online portfolio selection. IEEE Transactions on Knowledge and Data Engineering, 28(9), pp.2480-2493.</a></p></li>
</ol>
</div>
<section id="mean-reversion">
<h3>Mean Reversion<a class="headerlink" href="#mean-reversion" title="Permalink to this heading">¶</a></h3>
<p>Mean Reversion is an effective quantitative strategy based on the theory that prices will revert back to its historical mean.
A basic example of mean reversion follows the benchmark of Constant Rebalanced Portfolio. By setting a predetermined allocation of
weight to each asset, the portfolio shifts its weights from increasing to decreasing ones.</p>
<p>Through this documentation, the importance of hyperparameters is highlighted as the choices greatly affect the outcome of returns.
A lot of the hyperparameters for traditional research has been chosen by looking at the data in hindsight, and fundamental analysis
of each dataset and market structure is required to profitably implement this strategy in a real-time market scenario.</p>
<p>There are four mean reversion strategies implemented in the Online Portfolio Selection module.</p>
<hr class="docutils" />
<section id="passive-aggressive-mean-reversion">
<h4>Passive Aggressive Mean Reversion<a class="headerlink" href="#passive-aggressive-mean-reversion" title="Permalink to this heading">¶</a></h4>
<p>Passive Aggressive Mean Reversion alternates between a passive and aggressive approach to the current market conditions.
The strategy can effectively prevent a huge loss and maximize returns by setting a threshold for mean reversion.</p>
<p>PAMR takes in a variable <span class="math notranslate nohighlight">\(\epsilon\)</span>, a threshold for the market condition. If the portfolio returns for the period are
below <span class="math notranslate nohighlight">\(\epsilon\)</span>, then PAMR will passively keep the previous portfolio, whereas if the returns are above the threshold,
the portfolio will actively rebalance to the less performing assets.</p>
<p>In a way, <span class="math notranslate nohighlight">\(\epsilon\)</span> can be interpreted as the maximum loss for the portfolio. It is most likely that the asset that
decreased in prices for the period will bounce back, but there are cases where some companies plummet in value. PAMR
is an effective algorithm that will prevent huge losses in blindly following these assets.</p>
<p>PAMR defines a loss function:</p>
<div class="math notranslate nohighlight">
\[l_{\epsilon} (b; x_t)\]</div>
<p>If the returns for the period are below the threshold, <span class="math notranslate nohighlight">\(\epsilon\)</span>:</p>
<div class="math notranslate nohighlight">
\[l_{\epsilon} (b; x_t) = 0\]</div>
<p>For returns that are higher than <span class="math notranslate nohighlight">\(\epsilon\)</span>:</p>
<div class="math notranslate nohighlight">
\[l_{\epsilon} (b; x_t) = b \cdot x_t - \epsilon\]</div>
<p>Typically <span class="math notranslate nohighlight">\(\epsilon\)</span> is set at a value between 0 and 1 and closer to 1 as daily returns fluctuate around 1.</p>
<p>We will introduce three versions of Passive Aggressive Mean Reversion: PAMR, PAMR-1, and PAMR-2.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(b_t\)</span> is the portfolio vector at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_t\)</span> is the price relative change at time <span class="math notranslate nohighlight">\(t\)</span>. It is calculated by <span class="math notranslate nohighlight">\(\frac{p_t}{p_{t-1}}\)</span>, where <span class="math notranslate nohighlight">\(p_t\)</span> is the price at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> is the mean reversion threshold constant.</p></li>
</ul>
<section id="pamr">
<h5>PAMR<a class="headerlink" href="#pamr" title="Permalink to this heading">¶</a></h5>
<p>The first method is described as the following optimization problem:</p>
<div class="math notranslate nohighlight">
\[b_{t+1} = \underset{b \in \Delta_m}{\arg\min} \frac{1}{2} \|b-b_t \|^2 \: \text{s.t.} \: l_{\epsilon}(b;x_t)=0\]</div>
<p>With the original problem formulation and <span class="math notranslate nohighlight">\(\epsilon\)</span> parameters, PAMR is the most basic implementation.</p>
</section>
<section id="pamr-1">
<h5>PAMR-1<a class="headerlink" href="#pamr-1" title="Permalink to this heading">¶</a></h5>
<p>PAMR-1 introduces a slack variable to PAMR.</p>
<p><span class="math notranslate nohighlight">\(C\)</span> is a positive parameter that can be interpreted as the aggressiveness of the strategy.</p>
<div class="math notranslate nohighlight">
\[b_{t+1} = \underset{b \in \Delta_m}{\arg\min} \left\lbrace\frac{1}{2} \|b-b_t \|^2 + C\xi\right\rbrace \: \text{s.t.} \: l_{\epsilon}(b;x_t) \leq \xi \geq 0\]</div>
<p>A higher <span class="math notranslate nohighlight">\(C\)</span> value indicates the affinity to a more aggressive approach.</p>
</section>
<section id="pamr-2">
<h5>PAMR-2<a class="headerlink" href="#pamr-2" title="Permalink to this heading">¶</a></h5>
<p>PAMR-2 contains a quadratic term to the original slack variable from PAMR-1.</p>
<div class="math notranslate nohighlight">
\[b_{t+1} = \underset{b \in \Delta_m}{\arg\min} \left\lbrace\frac{1}{2} \|b-b_t \|^2 + C\xi^2 \right\rbrace \: \text{s.t.} \: l_{\epsilon}(b;x_t) \leq \xi\]</div>
<p>By increasing the slack variable at a quadratic rate, the method regularizes portfolio deviations.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(C\)</span> is the aggressiveness of the strategy.</p></li>
<li><p><span class="math notranslate nohighlight">\(\xi\)</span> is the slack variable used to calculate the optimization equation.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta_m\)</span> is the simplex domain. The sum of all elements is 1, and each element is in the range of [0, 1].</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>PAMR-1 and PAMR-2 tends to have higher returns compared to a normal PAMR.</p>
</div>
</section>
<section id="parameters">
<h5>Parameters<a class="headerlink" href="#parameters" title="Permalink to this heading">¶</a></h5>
<p>Using <a class="reference external" href="https://optuna.org/">optuna</a>, we experimented with different parameters to provide a general guideline
for the users. For NYSE, aggressiveness was not an important parameter as returns were primarily affected by
the <span class="math notranslate nohighlight">\(\epsilon\)</span> value. <span class="math notranslate nohighlight">\(\epsilon\)</span> of 0 resulted as the
highest returns.</p>
<a class="reference internal image-reference" href="_images/nyse_pamr.png"><img alt="_images/nyse_pamr.png" src="_images/nyse_pamr.png" style="width: 32%;" /></a>
<a class="reference internal image-reference" href="_images/nyse_pamr1.png"><img alt="_images/nyse_pamr1.png" src="_images/nyse_pamr1.png" style="width: 32%;" /></a>
<a class="reference internal image-reference" href="_images/nyse_pamr2.png"><img alt="_images/nyse_pamr2.png" src="_images/nyse_pamr2.png" style="width: 32%;" /></a>
<p>For the US Equity dataset, the optimal <span class="math notranslate nohighlight">\(\epsilon\)</span> is actually 1; however, the difference in returns,
for <span class="math notranslate nohighlight">\(\epsilon\)</span> of 0 and 1 is not too far apart. Aggressiveness continues to be a nominal factor as
a hyperparameter.</p>
<a class="reference internal image-reference" href="_images/equity_pamr.png"><img alt="_images/equity_pamr.png" src="_images/equity_pamr.png" style="width: 32%;" /></a>
<a class="reference internal image-reference" href="_images/equity_pamr1.png"><img alt="_images/equity_pamr1.png" src="_images/equity_pamr1.png" style="width: 32%;" /></a>
<a class="reference internal image-reference" href="_images/equity_pamr2.png"><img alt="_images/equity_pamr2.png" src="_images/equity_pamr2.png" style="width: 32%;" /></a>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p>High <span class="math notranslate nohighlight">\(\epsilon\)</span>: passive mean reversion.</p></li>
<li><p>Low <span class="math notranslate nohighlight">\(\epsilon\)</span>: aggressive mean reversion.</p></li>
<li><p>Aggressiveness (Agg) has minimal impact on returns, but a value between 10 and 100 typically worked well.</p></li>
</ul>
</div>
</section>
<section id="implementation">
<h5>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h5>
</section>
<section id="example-code">
<h5>Example Code<a class="headerlink" href="#example-code" title="Permalink to this heading">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Read in data.</span>
<span class="n">stock_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span>

<span class="c1"># Compute Passive Aggressive Mean Reversion with no given weights and epsilon of 0.3.</span>
<span class="n">pamr</span> <span class="o">=</span> <span class="n">PAMR</span><span class="p">(</span><span class="n">optimization_method</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">pamr</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute Passive Aggressive Mean Reversion - 1 with given user weights and epsilon of 0.4.</span>
<span class="n">pamr1</span> <span class="o">=</span> <span class="n">PAMR</span><span class="p">(</span><span class="n">optimization_method</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">agg</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">pamr1</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">some_weight</span><span class="p">)</span>

<span class="c1"># Compute Passive Aggressive Mean Reversion - 2 with given user weights and epsilon of 0.8.</span>
<span class="n">pamr2</span> <span class="o">=</span> <span class="n">PAMR</span><span class="p">(</span><span class="n">optimization_method</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">agg</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">pamr2</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">some_weight</span><span class="p">)</span>

<span class="c1"># Get the latest predicted weights.</span>
<span class="n">pamr</span><span class="o">.</span><span class="n">weights</span>

<span class="c1"># Get all weights for the strategy.</span>
<span class="n">pamr1</span><span class="o">.</span><span class="n">all_weights</span>

<span class="c1"># Get portfolio returns.</span>
<span class="n">pamr2</span><span class="o">.</span><span class="n">portfolio_return</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="confidence-weighted-mean-reversion">
<h4>Confidence Weighted Mean Reversion<a class="headerlink" href="#confidence-weighted-mean-reversion" title="Permalink to this heading">¶</a></h4>
<p>Extending from PAMR, Confidence Weighted Mean Reversion looks at the autocovariance across all assets.
Instead of focusing on a single asset’s deviation from the original price, CWMR takes in second-order
information about the portfolio vector as well to formulate a set of weights.</p>
<p>For CWMR, we introduce <span class="math notranslate nohighlight">\(\sum\)</span>, a measure of anti-confidence in the portfolio weights, where a smaller
value represents higher confidence for the corresponding portfolio weights.</p>
<div class="math notranslate nohighlight">
\[(\mu_{t+1}, \Sigma_{t+1}) = \underset{\mu \in \Delta_m, \Sigma}{\arg\min}D_{KL}(N(\mu,\Sigma) | N(\mu_t,\Sigma_t) )\]</div>
<p>If the returns for the period are below the threshold, <span class="math notranslate nohighlight">\(\epsilon\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{such that } Pr[b^{\top} \cdot x_t \leq \epsilon] \geq \theta \text{ and } \mu \in \Delta_m\]</div>
<p>Here the problem can be interpreted as maximizing the portfolio confidence by minimizing <span class="math notranslate nohighlight">\(\Sigma\)</span> given
a confidence interval <span class="math notranslate nohighlight">\(\theta\)</span> determined by the threshold, <span class="math notranslate nohighlight">\(\epsilon\)</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(b_t\)</span> is the portfolio vector at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_t\)</span> is the price relative change at time <span class="math notranslate nohighlight">\(t\)</span>. It is calculated by <span class="math notranslate nohighlight">\(\frac{p_t}{p_{t-1}}\)</span>, where <span class="math notranslate nohighlight">\(p_t\)</span> is the price at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> is the mean reversion threshold constant.</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta\)</span> is the confidence interval for the given distribution.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mu_t\)</span> is the mean of the projected weights distribution at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma_t\)</span> is the confidence matrix in the projected weights distribution at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(N(\mu_t, \Sigma_t)\)</span> is the normal distribution for the projected weights.</p></li>
<li><p><span class="math notranslate nohighlight">\(D_{KL}\)</span> is the Kullback-Leibler Divergence. More information of KL Divergence is available <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">here</a>.</p></li>
</ul>
<p>CWMR has two variations to solve this optimization problem with CWMR-SD and CWMR-Var.</p>
<section id="cwmr-sd">
<h5>CWMR-SD<a class="headerlink" href="#cwmr-sd" title="Permalink to this heading">¶</a></h5>
<p>CWMR uses the Kullback-Leibler divergence to further formulate the optimization problem as following:</p>
<div class="math notranslate nohighlight">
\[(\mu_{t+1}, \Sigma_{t+1}) = \arg \min \frac{1}{2} \left( \log(\frac{\det \Sigma_t}{\det \Sigma}) + Tr(\Sigma_t^{-1}\Sigma) + (\mu_t-\mu)^{\top}\Sigma_t^{-1}(\mu_t - \mu) \right)\]</div>
<div class="math notranslate nohighlight">
\[\text{such that } \epsilon - \mu^{\top}\cdot x_t \geq \phi x_t^{\top} \Sigma x_t\text{, } \mu^{\top} \cdot \textbf{1} = 1 \text{, and } \mu \geq 0\]</div>
</section>
<section id="cwmr-var">
<h5>CWMR-Var<a class="headerlink" href="#cwmr-var" title="Permalink to this heading">¶</a></h5>
<p>The standard deviation method further assumes the PSD property of <span class="math notranslate nohighlight">\(\Sigma\)</span> to refactor the equations as the following:</p>
<div class="math notranslate nohighlight">
\[(\mu_{t+1},\Gamma_{t+1}) = \arg \min \frac{1}{2}\left(\log(\frac{\det \Gamma_t^2}{\det \Gamma^2}) +Tr(\Gamma_t^{-2}\Gamma^2) + (\mu_t - \mu)^{\top}\Gamma_t^{-2}(\mu_t -\mu) \right)\]</div>
<div class="math notranslate nohighlight">
\[\text{such that }\epsilon - \mu^{\top}\cdot x_t \geq \phi || \Gamma x_t || \text{, }\Gamma \text{ is a PSD, }\mu^{\top} \cdot \textbf{1} = 1\text{, and }\mu \geq 0\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\phi\)</span> is the inverse of the cumulative distribution function for a given confidence interval.</p></li>
<li><p><span class="math notranslate nohighlight">\(Tr\)</span> is the sum of the diagonal elements in a matrix.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Gamma\)</span> is the square root of the matrix <span class="math notranslate nohighlight">\(\Sigma\)</span>.</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>For both CWMR-Var and CWMR-SD, the calculations involve taking the inverse of a sum of another inverse matrix.
The constant calculations of matrix inversion is extremely unstable and makes the model prone to any outliers and hyperparameters.</p>
</div>
</section>
<section id="id1">
<h5>Parameters<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h5>
<p>Using <a class="reference external" href="https://optuna.org/">optuna</a>, we experimented with different parameters to provide a general guideline
for the users. CWMR in general does not have an optimal parameter. The results are extremely dependent on the
hyperparameters as seen with the case for the NYSE and TSE dataset.</p>
<p>For NYSE, a <span class="math notranslate nohighlight">\(\epsilon\)</span> of 1 and confidence of 1 had the highest returns.</p>
<a class="reference internal image-reference" href="_images/nyse_cwmrsd.png"><img alt="_images/nyse_cwmrsd.png" src="_images/nyse_cwmrsd.png" style="width: 49%;" /></a>
<a class="reference internal image-reference" href="_images/nyse_cwmrvar.png"><img alt="_images/nyse_cwmrvar.png" src="_images/nyse_cwmrvar.png" style="width: 49%;" /></a>
<p>TSE has a much wider range, and it is difficult to pinpoint which parameters are actually the most useful.
At least for TSE, the SD method has higher returns with lower confidence value, whereas the VAR method
seems to indicate a congregation at 0.5.</p>
<a class="reference internal image-reference" href="_images/tse_cwmrsd.png"><img alt="_images/tse_cwmrsd.png" src="_images/tse_cwmrsd.png" style="width: 49%;" /></a>
<a class="reference internal image-reference" href="_images/tse_cwmrvar.png"><img alt="_images/tse_cwmrvar.png" src="_images/tse_cwmrvar.png" style="width: 49%;" /></a>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>It is difficult to gauge the optimal <span class="math notranslate nohighlight">\(\epsilon\)</span> and confidence for any dataset. We recommend
using other mean reversion strategies for implementations.</p>
</div>
</section>
<section id="id3">
<h5>Implementation<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h5>
</section>
<section id="id4">
<h5>Example Code<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Read in data.</span>
<span class="n">stock_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span>

<span class="c1"># Compute Confidence Weighted Mean Reversion - SD with no given weights, epsilon of 0.5, and confidence of 0.5.</span>
<span class="n">cwmr_sd</span> <span class="o">=</span> <span class="n">CWMR</span><span class="p">(</span><span class="n">confidence</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;sd&#39;</span><span class="p">)</span>
<span class="n">cwmr_sd</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute Confidence Weighted Mean Reversion - Var with given user weights, epsilon of 1, and confidence of 1.</span>
<span class="n">cwmr_var</span> <span class="o">=</span> <span class="n">CWMR</span><span class="p">(</span><span class="n">confidence</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;var&#39;</span><span class="p">)</span>
<span class="n">cwmr_var</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">some_weight</span><span class="p">)</span>

<span class="c1"># Get the latest predicted weights.</span>
<span class="n">cwmr_sd</span><span class="o">.</span><span class="n">weights</span>

<span class="c1"># Get all weights for the strategy.</span>
<span class="n">cwmr_var</span><span class="o">.</span><span class="n">all_weights</span>

<span class="c1"># Get portfolio returns.</span>
<span class="n">cwmr_sd</span><span class="o">.</span><span class="n">portfolio_return</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="online-moving-average-reversion">
<h4>Online Moving Average Reversion<a class="headerlink" href="#online-moving-average-reversion" title="Permalink to this heading">¶</a></h4>
<p>Traditional mean reversion techniques have an underlying assumption that the next price relative is inversely
proportional to the latest price relative; however, mean reversion trends are not limited to a single period.
Unlike traditional reversion methods that rely on windows of just one, OLMAR looks to revert to the moving average of price data.</p>
<p>OLMAR proposes two different moving average methods: Simple Moving Average and Exponential Moving Average.</p>
<p>From these moving average methods, the strategy predicts the next period’s price relative. Using this new prediction,
the portfolio iteratively updates its new weights.</p>
<div class="math notranslate nohighlight">
\[b_{t+1} = b_t + \lambda_{t+1}(\tilde{x}_{t+1}-\bar x_{t+1}\textbf{1})\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda\)</span> is the constant multiplier to the new weights, and it is determined by the deviation from</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span>, the reversion threshold. The portfolio will look to rebalance itself to the underperforming assets only if the portfolio returns are lower than the <span class="math notranslate nohighlight">\(\epsilon\)</span> value.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\lambda_{t+1} = max \left\lbrace 0, \frac{\epsilon-b_t \cdot \tilde{x}_{t+1}}{\|\tilde{x}_{t+1}-\bar x_{t+1} \textbf{1}\|^2}\right\rbrace\]</div>
<p>OLMAR has two variations to solve this optimization problem with OLMAR-1 and OLMAR-2.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(b_t\)</span> is the portfolio vector at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_t\)</span> is the price relative change at time <span class="math notranslate nohighlight">\(t\)</span>. It is calculated by <span class="math notranslate nohighlight">\(\frac{p_t}{p_{t-1}}\)</span>, where <span class="math notranslate nohighlight">\(p_t\)</span> is the price at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\tilde{x}\)</span> is the projected price relative.</p></li>
<li><p><span class="math notranslate nohighlight">\(\bar x\)</span> is the mean of the projected price relative.</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> is the mean reversion threshold constant.</p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda\)</span> is the lagrangian multiplier to change the new weights.</p></li>
</ul>
<section id="olmar-1">
<h5>OLMAR-1<a class="headerlink" href="#olmar-1" title="Permalink to this heading">¶</a></h5>
<p>OLMAR-1 utilizes simple moving average to predict prices.</p>
<div class="math notranslate nohighlight">
\[\tilde{x}_{t+1}(w) = \frac{SMA_t(w)}{p_t}\]</div>
<div class="math notranslate nohighlight">
\[\: \: \: \: \: \: \: \: \: \: \: \: \: \: = \frac{1}{w} \left(\frac{p_t}{p_t} + \frac{p_{t-1}}{p_t}+ \cdot \cdot \cdot + \frac{p_{t-w+1}}{p_t}\right)\]</div>
<div class="math notranslate nohighlight">
\[\: \: \: \: \: \: \: \: \: \: \: \: \: \: = \frac{1}{w} \left( 1+ \frac{1}{x_t}+ \cdot \cdot \cdot + \frac{1}{\odot^{w-2}_{i=0}x_{t-i}} \right)\]</div>
</section>
<section id="olmar-2">
<h5>OLMAR-2<a class="headerlink" href="#olmar-2" title="Permalink to this heading">¶</a></h5>
<p>OLMAR-2 uses exponential moving average to predict prices.</p>
<div class="math notranslate nohighlight">
\[\tilde{x}_{t+1}(\alpha) = \frac{EMA_t(\alpha)}{p_t}\]</div>
<div class="math notranslate nohighlight">
\[\: \: \: \: \: \: \: \: \: \: \: \: \:= \frac{\alpha p_t+(1-\alpha)EMA_{t-1}(\alpha)}{p_t}\]</div>
<div class="math notranslate nohighlight">
\[\: \: \: \: \: \: \: \: \: \: \: \: \:= \alpha \textbf{1} + (1 - \alpha) \frac{EMA_{t-1}(\alpha)}{p_{t-1}}\frac{p_{t-1}}{p_t}\]</div>
<div class="math notranslate nohighlight">
\[\: \: \: \: \: \: \: \: \: \: \: \: \:= \alpha \textbf{1} + (1 - \alpha) \frac{\tilde{x_t}}{x_t}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(w\)</span> is the window value for simple moving average.</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> is the smoothing factor for exponential moving average.</p></li>
<li><p><span class="math notranslate nohighlight">\(\bigodot\)</span> is the element-wise cumulative product. In this case, the cumulative product represents the overall change in prices.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>For both OLMAR-1 and OLMAR-2, the corresponding window and alpha values are the most important parameters.
Every market has a different mean reversion pattern, and it is important to identify the exact parameter to apply this
strategy in a real trading environment.</p>
</div>
</section>
<section id="id5">
<h5>Parameters<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h5>
<p>Using <a class="reference external" href="https://optuna.org/">optuna</a>, we experimented with different parameters to provide a general guideline
for the users. <span class="math notranslate nohighlight">\(\epsilon\)</span> has minimal impact on the returns as the primary driving paramter for these strategies
is corresponding window or alpha value.</p>
<p>For NYSE, optimal window for OLMAR-1 was 23, whereas optimal alpha was 0.4.</p>
<a class="reference internal image-reference" href="_images/nyse_olmar1.png"><img alt="_images/nyse_olmar1.png" src="_images/nyse_olmar1.png" style="width: 49%;" /></a>
<a class="reference internal image-reference" href="_images/nyse_olmar2.png"><img alt="_images/nyse_olmar2.png" src="_images/nyse_olmar2.png" style="width: 49%;" /></a>
<p>TSE’s window was similar to that of NYSE, but the optimal alpha was much higher, with a value of 0.9.</p>
<a class="reference internal image-reference" href="_images/tse_olmar1.png"><img alt="_images/tse_olmar1.png" src="_images/tse_olmar1.png" style="width: 49%;" /></a>
<a class="reference internal image-reference" href="_images/tse_olmar2.png"><img alt="_images/tse_olmar2.png" src="_images/tse_olmar2.png" style="width: 49%;" /></a>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p>OLMAR-2 tends to have much higher returns compared to OLMAR-1, but this is also dependent on each dataset.</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> has minimal effect on returns.</p></li>
</ul>
</div>
</section>
<section id="id7">
<h5>Implementation<a class="headerlink" href="#id7" title="Permalink to this heading">¶</a></h5>
</section>
<section id="id8">
<h5>Example Code<a class="headerlink" href="#id8" title="Permalink to this heading">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Read in data.</span>
<span class="n">stock_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span>

<span class="c1"># Compute Online Moving Average Reversion - 1 with no given weights, epsilon of 10, and window of 7.</span>
<span class="n">olmar1</span> <span class="o">=</span> <span class="n">OLMAR</span><span class="p">(</span><span class="n">reversion_method</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="n">olmar1</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute Online Moving Average Reversion - 2 with given user weights, epsilon of 100, and alpha of 0.6.</span>
<span class="n">olmar2</span> <span class="o">=</span> <span class="n">OLMAR</span><span class="p">(</span><span class="n">reversion_method</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">olmar2</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">some_weight</span><span class="p">)</span>

<span class="c1"># Get the latest predicted weights.</span>
<span class="n">olmar1</span><span class="o">.</span><span class="n">weights</span>

<span class="c1"># Get all weights for the strategy.</span>
<span class="n">olmar2</span><span class="o">.</span><span class="n">all_weights</span>

<span class="c1"># Get portfolio returns.</span>
<span class="n">olmar1</span><span class="o">.</span><span class="n">portfolio_return</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="robust-median-reversion">
<h4>Robust Median Reversion<a class="headerlink" href="#robust-median-reversion" title="Permalink to this heading">¶</a></h4>
<p>Robust Median Reversion extends the previous Online Moving Average Reversion by introducing L1 median of the specified windows.
Instead of reverting to a moving average, RMR reverts to the L1 median estimator, which proves to be a more effective method of
predicting the next period’s price because financial data is inherently noisy and contains many outliers.</p>
<p>L1-median is calculated with the following equation:</p>
<div class="math notranslate nohighlight">
\[\mu = \underset{\mu}{\arg \min}\overset{k-1}{\underset{i=0}{\sum}}||p_{t-i} - \mu ||\]</div>
<p>where <span class="math notranslate nohighlight">\(k\)</span> is the number of historical price windows, and <span class="math notranslate nohighlight">\(\mu\)</span> represents the predicted price.</p>
<p>The calculation of L1-median is computationally inefficient, so the algorithm will be using the Modified Weiszfeld Algorithm.</p>
<div class="math notranslate nohighlight">
\[\hat{x}_{t+1} = T(\mu) = (1 - \frac{\eta(\mu)}{\gamma(\mu)})^+ \: \tilde{T}(\mu) + \min(1,\frac{\eta(\mu)}{\gamma(\mu)})\mu\]</div>
<div class="math notranslate nohighlight">
\[\eta(\mu) = 1 \text{ if } \mu =\text{ any price and }0 \text{ otherwise.}\]</div>
<div class="math notranslate nohighlight">
\[\gamma(\mu)=\left|\left|\underset{p_{t-i} \neq \mu}{\sum}\frac{p_{t-i}-\mu}{||p_{t-i}-\mu||}\right|\right|\]</div>
<div class="math notranslate nohighlight">
\[\tilde{T}(\mu)=\left\lbrace \underset{p_{t-i}\neq \mu}{\sum}\frac{1}{||p_{t-i}-\mu||}\right\rbrace^{-1}\underset{p_{t-i}\neq \mu}{\sum}\frac{p_{t-i}}{||p_{t-i}-\mu||}\]</div>
<p>Then next portfolio weights will use the predicted price to produce the optimal portfolio weights.</p>
<div class="math notranslate nohighlight">
\[b_{t+1} = b_{t} - \min \left \lbrace 0, \frac{\hat{x}_{t+1} b_t-\epsilon}{||\hat{x}_{t+1}-\bar{x}_{t+1}\cdot\textbf{1}||^2}\right \rbrace \cdot (\hat{x}_{t+1}-\bar{x}_{t+1}\cdot\textbf{1})\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(b_t\)</span> is the portfolio vector at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_t\)</span> is the price relative change at time <span class="math notranslate nohighlight">\(t\)</span>. It is calculated by <span class="math notranslate nohighlight">\(\frac{p_t}{p_{t-1}}\)</span>, where <span class="math notranslate nohighlight">\(p_t\)</span> is the price at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mu_t\)</span> is the projected price.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{x}\)</span> is the projected price relative.</p></li>
<li><p><span class="math notranslate nohighlight">\(\bar{x}\)</span> is the mean of the projected price relative.</p></li>
</ul>
<section id="id9">
<h5>Parameters<a class="headerlink" href="#id9" title="Permalink to this heading">¶</a></h5>
<p>Using <a class="reference external" href="https://optuna.org/">optuna</a>, we experimented with different parameters to provide a general guideline
for the users. Similarly to OLMAR, the parameters primarily depend on the window value. N_iteration of 200 typically had
the highest results with a <span class="math notranslate nohighlight">\(\epsilon\)</span> range of 15 to 25. Ultimately, the window range that decides
the period of mean reversion was the most influential parameter to affect the portfolio’s results.</p>
<a class="reference internal image-reference" href="_images/nyse_rmr.png"><img alt="_images/nyse_rmr.png" src="_images/nyse_rmr.png" style="width: 32%;" /></a>
<a class="reference internal image-reference" href="_images/djia_rmr.png"><img alt="_images/djia_rmr.png" src="_images/djia_rmr.png" style="width: 32%;" /></a>
<a class="reference internal image-reference" href="_images/msci_rmr.png"><img alt="_images/msci_rmr.png" src="_images/msci_rmr.png" style="width: 32%;" /></a>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p>Window values tended to be more experimental and is the most important parameter for RMR.</p></li>
<li><p>N_iteration of 200 typically had higher results.</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> range of 15 to 25 worked for many datasets.</p></li>
<li><p>It is recommended to leave the <span class="math notranslate nohighlight">\(\tau\)</span> at 0.001 for computational issues.</p></li>
</ul>
</div>
</section>
<section id="id11">
<h5>Implementation<a class="headerlink" href="#id11" title="Permalink to this heading">¶</a></h5>
</section>
<section id="id12">
<h5>Example Code<a class="headerlink" href="#id12" title="Permalink to this heading">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Read in data.</span>
<span class="n">stock_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span>

<span class="c1"># Compute Robust Median Reversion with no given weights, epsilon of 15, n_iteration of 100, and window of 7.</span>
<span class="n">rmr</span> <span class="o">=</span> <span class="n">RMR</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">n_iteration</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="n">rmr</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute Robust Median Reversion with given user weights, epsilon of 25, n_iteration of 500, and window of 21.</span>
<span class="n">rmr1</span> <span class="o">=</span> <span class="n">RMR</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">n_iteration</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">21</span><span class="p">)</span>
<span class="n">rmr1</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">some_weight</span><span class="p">)</span>

<span class="c1"># Get the latest predicted weights.</span>
<span class="n">rmr</span><span class="o">.</span><span class="n">weights</span>

<span class="c1"># Get all weights for the strategy.</span>
<span class="n">rmr</span><span class="o">.</span><span class="n">all_weights</span>

<span class="c1"># Get portfolio returns.</span>
<span class="n">rmr1</span><span class="o">.</span><span class="n">portfolio_return</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="research-notebook">
<h4>Research Notebook<a class="headerlink" href="#research-notebook" title="Permalink to this heading">¶</a></h4>
<blockquote>
<div><p>The following <a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Online%20Portfolio%20Selection/Online%20Portfolio%20Selection%20-%20Mean%20Reversion.ipynb">mean reversion</a>
notebook provides a more detailed exploration of the strategies.</p>
</div></blockquote>
</section>
</section>
<span id="document-online_portfolio_selection/pattern_matching"></span><div class="admonition note" id="online-portfolio-selection-pattern-matching">
<p class="admonition-title">Note</p>
<p>Strategies were implemented with modifications from:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://dl.acm.org/doi/pdf/10.1145/1961189.1961193">Li, B., Hoi, S.C. and Gopalkrishnan, V., 2011. Corn: Correlation-driven nonparametric learning approach for portfolio selection. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3), pp.1-29.</a></p></li>
<li><p><a class="reference external" href="https://jfds.pm-research.com/content/1/2/78.short">Wang, Y. and Wang, D., 2019. Market Symmetry and Its Application to Pattern-Matching-Based Portfolio Selection. The Journal of Financial Data Science, 1(2), pp.78-93.</a></p></li>
</ol>
</div>
<section id="pattern-matching">
<h3>Pattern Matching<a class="headerlink" href="#pattern-matching" title="Permalink to this heading">¶</a></h3>
<p>Pattern matching locates similarly acting historical market windows and make future predictions based on the similarity.
Traditional quantitative strategies such as momentum and mean reversion focus on the directionality of the market trends.
The underlying assumption that the immediate past trends will continue is simple but does not always perform the best in real markets.
Pattern matching strategies combine the strengths of both by exploiting the statistical correlations of the current market window to the past.</p>
<p>There are three pattern matching strategies implemented in the Online Portfolio Selection module.</p>
<hr class="docutils" />
<section id="correlation-driven-nonparametric-learning">
<h4>Correlation Driven Nonparametric Learning<a class="headerlink" href="#correlation-driven-nonparametric-learning" title="Permalink to this heading">¶</a></h4>
<p>Correlation Driven Nonparametric Learning strategies look at historical market sequences to identify similarly correlated periods.
Existing pattern matching strategies attempt to exploit and identify the correlation between different market windows by using
the Euclidean distance to measure the similarity between two market windows. However, the traditional Euclidean distance between
windows does not effectively capture the linear relation. CORN utilizes the Pearson correlation coefficient instead of Euclidean
distances to capture the whole market direction.</p>
<p>Three different variations of the CORN strategies are implemented in the Online Portfolio Selection module.</p>
<hr class="docutils" />
<section id="correlation-driven-nonparametric-learning-corn">
<h5>Correlation Driven Nonparametric Learning (CORN)<a class="headerlink" href="#correlation-driven-nonparametric-learning-corn" title="Permalink to this heading">¶</a></h5>
<p>CORN formally defines a similar set to be one that satisfies the following equation:</p>
<div class="math notranslate nohighlight">
\[C_t(w,\rho) = \left\lbrace w &lt; i &lt; t+1 \bigg\vert \frac{cov(x^{i-1}_{i-w}, x^t_{t-w+1})}{std(x^{i-1}_{i-w})std(x^t_{t-w+1})} \geq \rho\right\rbrace\]</div>
<p><span class="math notranslate nohighlight">\(W\)</span> represents the number of windows to lookback, and <span class="math notranslate nohighlight">\(\rho\)</span> is the correlation coefficient threshold.</p>
<p>For the specific correlation calculation, each market window of w periods is concatenated to obtain a univariate correlation coefficient between the two windows.</p>
<p>Once all the similar historical periods are identified, the strategy returns weights that will maximize returns for the period.</p>
<div class="math notranslate nohighlight">
\[b_{t+1}(w,\rho) = \underset{b \in \Delta_m}{\arg \max} \underset{i \in C_t(w,\rho)}{\prod}(b \cdot x_i)\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(b_t\)</span> is the portfolio vector at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_t\)</span> is the price relative change at time <span class="math notranslate nohighlight">\(t\)</span>. It is calculated by <span class="math notranslate nohighlight">\(\frac{p_t}{p_{t-1}}\)</span>, where <span class="math notranslate nohighlight">\(p_t\)</span> is the price at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(w\)</span> is the number of windows to lookback.</p></li>
<li><p><span class="math notranslate nohighlight">\(cov\)</span> is the covariance term.</p></li>
<li><p><span class="math notranslate nohighlight">\(std\)</span> is the standard deviation term.</p></li>
<li><p><span class="math notranslate nohighlight">\(\rho\)</span> is the correlation threshold.</p></li>
<li><p><span class="math notranslate nohighlight">\(C_t\)</span> is the set of similar periods.</p></li>
</ul>
<section id="corn-parameters">
<h6>CORN Parameters<a class="headerlink" href="#corn-parameters" title="Permalink to this heading">¶</a></h6>
<p>Using <a class="reference external" href="https://optuna.org/">optuna</a>, we experimented with different parameters to provide a general guideline
for the users. For NYSE, <span class="math notranslate nohighlight">\(\rho\)</span> of 0.3 and window of 1 had the highest returns.
SP500 images indicate an optimal rho of 0 and window of 6. Most of the times the window values should be less than 7 with a strong
inclination to 0 with rho values being on the lower end from 0 to 0.4.</p>
<a class="reference internal image-reference" href="_images/nyse_corn.png"><img alt="_images/nyse_corn.png" src="_images/nyse_corn.png" style="width: 49%;" /></a>
<a class="reference internal image-reference" href="_images/sp500_corn.png"><img alt="_images/sp500_corn.png" src="_images/sp500_corn.png" style="width: 49%;" /></a>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\rho\)</span> between 0 and 0.2 is optimal</p></li>
<li><p>Optimal window ranges are different, but are typically in between 1 and 7 and most likely 1.</p></li>
</ul>
</div>
</section>
<section id="corn-implementation">
<h6>CORN Implementation<a class="headerlink" href="#corn-implementation" title="Permalink to this heading">¶</a></h6>
</section>
</section>
<hr class="docutils" />
<section id="correlation-driven-nonparametric-learning-uniform-corn-u">
<h5>Correlation Driven Nonparametric Learning - Uniform (CORN-U)<a class="headerlink" href="#correlation-driven-nonparametric-learning-uniform-corn-u" title="Permalink to this heading">¶</a></h5>
<p>Because the CORN strategies are dependent on the parameters, we propose a more generic one that takes an
ensemble approach to reduce variability. One possible CORN ensemble is the CORN-U method.</p>
<p>CORN-U generates a set of experts with different window sizes and the same <span class="math notranslate nohighlight">\(\rho\)</span> value. After all the expert’s
weights are calculated, weights are evenly distributed among all experts to represent the strategy as a universal portfolio.</p>
<p>After gathering results for all the experts, the total portfolio weight will be determined by:</p>
<div class="math notranslate nohighlight">
\[b_{t+1}=\frac{\sum_{w, \rho}q(w,\rho)S_t(w,\rho)b_{t+1}(w,\rho)}{\sum_{w, \rho}q(w,\rho)S_t(w,\rho)}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S(t)\)</span> is the total portfolio returns at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(q(w, \rho)\)</span> is the weight allocation constant. For CORN-U, this is a uniform distribution.</p></li>
</ul>
<section id="corn-u-parameters">
<h6>CORN-U Parameters<a class="headerlink" href="#corn-u-parameters" title="Permalink to this heading">¶</a></h6>
<p>Using <a class="reference external" href="https://optuna.org/">optuna</a>, we experimented with different parameters to provide a general guideline
for the users. The best parameters for CORN-U were similar to the parameters for parent class CORN. The most important
parameter that affects returns tend to be the <span class="math notranslate nohighlight">\(\rho\)</span> value and a range between 0 and 0.4 works for most datasets.
Window ranges are trickier as they tend to be either just 1 or a much larger value.</p>
<a class="reference internal image-reference" href="_images/nyse_cornu.png"><img alt="_images/nyse_cornu.png" src="_images/nyse_cornu.png" style="width: 49%;" /></a>
<a class="reference internal image-reference" href="_images/sp500_cornu.png"><img alt="_images/sp500_cornu.png" src="_images/sp500_cornu.png" style="width: 49%;" /></a>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\rho\)</span> between 0 and 0.2 is optimal</p></li>
<li><p>Optimal window ranges are different, but are typically in between 1 and 7 and most likely 1.</p></li>
</ul>
</div>
</section>
<section id="corn-u-implementation">
<h6>CORN-U Implementation<a class="headerlink" href="#corn-u-implementation" title="Permalink to this heading">¶</a></h6>
</section>
</section>
<hr class="docutils" />
<section id="correlation-driven-nonparametric-learning-k-corn-k">
<h5>Correlation Driven Nonparametric Learning - K (CORN-K)<a class="headerlink" href="#correlation-driven-nonparametric-learning-k-corn-k" title="Permalink to this heading">¶</a></h5>
<p>CORN-K further improves the CORN-U by generating more parameters of experts. There is more variability as
different ranges of window and <span class="math notranslate nohighlight">\(\rho\)</span> value are considered to create more options.</p>
<p>The most important part of the CORN-K, however, is the capital allocation method. Unlike CORN-U, which uniformly
distributes capital among all the experts, CORN-K selects the top-k best performing experts until the last period
and equally allocate capital among them. This prunes the experts that have less optimal returns and puts more weight on the performing ones.</p>
<p>CORN-K takes in 3 parameters: window, <span class="math notranslate nohighlight">\(rho\)</span>, and <span class="math notranslate nohighlight">\(k\)</span>.</p>
<section id="corn-k-parameters">
<h6>CORN-K Parameters<a class="headerlink" href="#corn-k-parameters" title="Permalink to this heading">¶</a></h6>
<p>Using <a class="reference external" href="https://optuna.org/">optuna</a>, we experimented with different parameters to provide a general guideline
for the users. The most important parameter for CORN-K is k, and most of the times this should always be set at 1 or 2 for the highest returns. A low
value of k effectively prunes the less performing experts. Rho of 1 is good for datasets that have optimal CORN rho value of 0,
but if the optimal CORN rho is slightly above that rho should be changed to a value higher then 3 to capture the range. Typically,
range of [3, 5] worked for preliminary datasets. Window values also depend on each dataset, but the best
guess would be on the lower range of 1 or higher value of 7.</p>
<a class="reference internal image-reference" href="_images/nyse_cornk.png"><img alt="_images/nyse_cornk.png" src="_images/nyse_cornk.png" style="width: 49%;" /></a>
<a class="reference internal image-reference" href="_images/sp500_cornk.png"><img alt="_images/sp500_cornk.png" src="_images/sp500_cornk.png" style="width: 49%;" /></a>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(k\)</span> should be either 1 or 2 in most cases.</p></li>
<li><p><span class="math notranslate nohighlight">\(\rho\)</span> between 1 and 7 is optimal</p></li>
<li><p>Optimal window ranges are different, but are typically in between 1 and 7.</p></li>
</ul>
</div>
</section>
<section id="corn-k-implementation">
<h6>CORN-K Implementation<a class="headerlink" href="#corn-k-implementation" title="Permalink to this heading">¶</a></h6>
</section>
</section>
<section id="example-code">
<h5>Example Code<a class="headerlink" href="#example-code" title="Permalink to this heading">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Read in data.</span>
<span class="n">stock_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span>

<span class="c1"># CORN</span>
<span class="c1"># Compute Correlation Driven Nonparametric Learning with no given weights, window of 1, and rho of 0.3.</span>
<span class="n">corn</span> <span class="o">=</span> <span class="n">CORN</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">corn</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute Correlation Driven Nonparametric Learning with user given weights, window of 3, and rho of 0.5.</span>
<span class="n">corn1</span> <span class="o">=</span> <span class="n">CORN</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">corn1</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">some_weight</span><span class="p">)</span>

<span class="c1"># CORN-U</span>
<span class="c1"># Compute Correlation Driven Nonparametric Learning - Uniform with no given weights, window range of 10, and rho of 0.3.</span>
<span class="n">cornu</span> <span class="o">=</span> <span class="n">CORNU</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">cornu</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute Correlation Driven Nonparametric Learning - Uniform with user given weights, window range of 5, and rho of 0.1.</span>
<span class="n">cornu1</span> <span class="o">=</span> <span class="n">CORNU</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">cornu1</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">some_weight</span><span class="p">)</span>

<span class="c1"># CORN-K</span>
<span class="c1"># Compute Correlation Driven Nonparametric Learning - K with no given weights, window range of 10, rho of 7, and k of 2.</span>
<span class="n">cornk</span> <span class="o">=</span> <span class="n">CORNK</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">cornk</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute Correlation Driven Nonparametric Learning - K with user given weights, window range of 5, rho of 3, and k of 1.</span>
<span class="n">cornk1</span> <span class="o">=</span> <span class="n">CORNK</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">cornk1</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">some_weight</span><span class="p">)</span>

<span class="c1"># Recalculate k for cornk1 to save computational time of generating all experts.</span>
<span class="n">cornk1</span><span class="o">.</span><span class="n">recalculate_k</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Get the latest predicted weights.</span>
<span class="n">corn</span><span class="o">.</span><span class="n">weights</span>

<span class="c1"># Get all weights for the strategy.</span>
<span class="n">cornk</span><span class="o">.</span><span class="n">all_weights</span>

<span class="c1"># Get portfolio returns.</span>
<span class="n">cornu</span><span class="o">.</span><span class="n">portfolio_return</span>

<span class="c1"># Get each object of the generated experts.</span>
<span class="n">cornk1</span><span class="o">.</span><span class="n">experts</span>

<span class="c1"># Get each experts parameters.</span>
<span class="n">cornu</span><span class="o">.</span><span class="n">expert_params</span>

<span class="c1"># Get all expert&#39;s portfolio returns over time.</span>
<span class="n">cornu1</span><span class="o">.</span><span class="n">expert_portfolio_returns</span>

<span class="c1"># Get capital allocation weights.</span>
<span class="n">cornk1</span><span class="o">.</span><span class="n">weights_on_experts</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="symmetric-correlation-driven-nonparametric-learning">
<h4>Symmetric Correlation Driven Nonparametric Learning<a class="headerlink" href="#symmetric-correlation-driven-nonparametric-learning" title="Permalink to this heading">¶</a></h4>
<p>Symmetric CORN utilizes the concept of Market symmetry which states that markets have mirrored price movements. Increasing price trends
represents a mirror of a decreasing trend. This gives us an intuitional understanding that if the price movements are strongly negatively
correlated, the optimal portfolio weights should minimize the returns or the losses from those periods as it is most likely that the optimal
portfolio weights would be the inverse.</p>
<p>Introduced recently in a Journal of Financial Data Science paper by Yang Wang and Dong Wang in 2019, SCORN identifies positively
correlated windows and negatively correlated windows.</p>
<p>The positiviely correlated windows are identified similar to the process for CORN.</p>
<div class="math notranslate nohighlight">
\[C(x_t;w,\rho) = \lbrace x_j \vert R(X^{j-1}_{j-w},X^{t-1}_{t-w})  &gt; \rho)\]</div>
<p>And the negatively correlated windows are identified as any period with a correlation value below the negative of the threshold.</p>
<div class="math notranslate nohighlight">
\[C'(x_t;w,\rho) = \lbrace x_j \vert R(X^{j-1}_{j-w},X^{t-1}_{t-w})  &lt; -\rho)\]</div>
<p>The strategy, therefore, maximizes the returns for periods that are considered similar and minimize the losses over periods that are considered the opposite.</p>
<div class="math notranslate nohighlight">
\[b^{\bf{\star}}_t(w,\rho) = \underset{b \in \Delta_m}{\arg\max} \underset{x \in C(x_t;w,\rho)}{\sum}\log b^{\top}x - \underset{x \in C'(x_t;w,\rho)}{\sum}\log b^{\top}x\]</div>
<p>Two different variations of the SCORN strategies are implemented in the Online Portfolio Selection module.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(b_t\)</span> is the portfolio vector at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_t\)</span> is the price relative change at time <span class="math notranslate nohighlight">\(t\)</span>. It is calculated by <span class="math notranslate nohighlight">\(\frac{p_t}{p_{t-1}}\)</span>, where <span class="math notranslate nohighlight">\(p_t\)</span> is the price at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\rho\)</span> is the correlation threshold.</p></li>
<li><p><span class="math notranslate nohighlight">\(R\)</span> is the correlation coefficient.</p></li>
<li><p><span class="math notranslate nohighlight">\(C_t\)</span> is the set of similar periods.</p></li>
<li><p><span class="math notranslate nohighlight">\(C'_t\)</span> is the set of similar periods.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta_m\)</span> is the simplex domain. The sum of all elements is 1, and each element is in the range of [0, 1].</p></li>
</ul>
<hr class="docutils" />
<section id="symmetric-correlation-driven-nonparametric-learning-scorn">
<h5>Symmetric Correlation Driven Nonparametric Learning (SCORN)<a class="headerlink" href="#symmetric-correlation-driven-nonparametric-learning-scorn" title="Permalink to this heading">¶</a></h5>
<section id="scorn-parameters">
<h6>SCORN Parameters<a class="headerlink" href="#scorn-parameters" title="Permalink to this heading">¶</a></h6>
<p>Using <a class="reference external" href="https://optuna.org/">optuna</a>, we experimented with different parameters to provide a general guideline
for the users. The optimal <span class="math notranslate nohighlight">\(\rho\)</span> for SCORN is between 0 and 0.2. Most cases <span class="math notranslate nohighlight">\(\rho\)</span> would be 0 to indicate a
binary classification regarding the similarity sets; however, there are some instances where a value of 0.2
is more optimal. The optimal window value more or less varies with a tendency for a shorter value of 1 or 2.
Although, there are cases where a window of 21 had the highest returns.</p>
<a class="reference internal image-reference" href="_images/nyse_scorn.png"><img alt="_images/nyse_scorn.png" src="_images/nyse_scorn.png" style="width: 49%;" /></a>
<a class="reference internal image-reference" href="_images/msci_scorn.png"><img alt="_images/msci_scorn.png" src="_images/msci_scorn.png" style="width: 49%;" /></a>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\rho\)</span> between 0 and 0.2 is optimal. In most cases, 0 had the highest returns.</p></li>
<li><p>Optimal window ranges are different and will vary dependant on the data.</p></li>
</ul>
</div>
</section>
<section id="scorn-implementation">
<h6>SCORN Implementation<a class="headerlink" href="#scorn-implementation" title="Permalink to this heading">¶</a></h6>
</section>
</section>
<hr class="docutils" />
<section id="symmetric-correlation-driven-nonparametric-learning-k-scorn-k">
<h5>Symmetric Correlation Driven Nonparametric Learning - K (SCORN-K)<a class="headerlink" href="#symmetric-correlation-driven-nonparametric-learning-k-scorn-k" title="Permalink to this heading">¶</a></h5>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>More detailed explanation about the top-k strategy is available with the documentation for CORN-K.</p>
</div>
<section id="scorn-k-parameters">
<h6>SCORN-K Parameters<a class="headerlink" href="#scorn-k-parameters" title="Permalink to this heading">¶</a></h6>
<p>Using <a class="reference external" href="https://optuna.org/">optuna</a>, we experimented with different parameters to provide a general guideline
for the users. In general, <span class="math notranslate nohighlight">\(\rho\)</span> of 1 is sufficient as most of the time the ideal <span class="math notranslate nohighlight">\(\rho\)</span> is 0. For cases with
datasets that have optimal SCORN of 0.2, <span class="math notranslate nohighlight">\(\rho\)</span> should be increased to 3. Window values are also dependent
on each data, but in most cases, value of 2 was sufficient.</p>
<a class="reference internal image-reference" href="_images/nyse_scornk.png"><img alt="_images/nyse_scornk.png" src="_images/nyse_scornk.png" style="width: 49%;" /></a>
<a class="reference internal image-reference" href="_images/msci_scornk.png"><img alt="_images/msci_scornk.png" src="_images/msci_scornk.png" style="width: 49%;" /></a>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(k\)</span> should be either 1 or 2 in most cases.</p></li>
<li><p><span class="math notranslate nohighlight">\(\rho\)</span> between 1 and 7 is optimal</p></li>
<li><p>Optimal window ranges are different, but are typically in between 1 and 7.</p></li>
</ul>
</div>
</section>
<section id="scorn-k-implementation">
<h6>SCORN-K Implementation<a class="headerlink" href="#scorn-k-implementation" title="Permalink to this heading">¶</a></h6>
</section>
</section>
<section id="id5">
<h5>Example Code<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Read in data.</span>
<span class="n">stock_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span>

<span class="c1"># SCORN</span>
<span class="c1"># Compute Symmetric Correlation Driven Nonparametric Learning with no given weights, window of 1, and rho of 0.3.</span>
<span class="n">scorn</span> <span class="o">=</span> <span class="n">SCORN</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">scorn</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute Symmetric Correlation Driven Nonparametric Learning with user given weights, window of 3, and rho of 0.5.</span>
<span class="n">scorn1</span> <span class="o">=</span> <span class="n">SCORN</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">scorn1</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">some_weight</span><span class="p">)</span>

<span class="c1"># SCORN-K</span>
<span class="c1"># Compute Symmetric Correlation Driven Nonparametric Learning - K with no given weights, window range of 10, rho of 7, and k of 2.</span>
<span class="n">scornk</span> <span class="o">=</span> <span class="n">SCORNK</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">scornk</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute Symmetric Correlation Driven Nonparametric Learning - K with user given weights, window range of 5, rho of 3, and k of 1.</span>
<span class="n">scornk1</span> <span class="o">=</span> <span class="n">SCORNK</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">scornk1</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">some_weight</span><span class="p">)</span>

<span class="c1"># Recalculate k for scornk1 to save computational time of generating all experts.</span>
<span class="n">scornk1</span><span class="o">.</span><span class="n">recalculate_k</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Get the latest predicted weights.</span>
<span class="n">scorn</span><span class="o">.</span><span class="n">weights</span>

<span class="c1"># Get all weights for the strategy.</span>
<span class="n">scornk</span><span class="o">.</span><span class="n">all_weights</span>

<span class="c1"># Get portfolio returns.</span>
<span class="n">scorn</span><span class="o">.</span><span class="n">portfolio_return</span>

<span class="c1"># Get each object of the generated experts.</span>
<span class="n">scornk1</span><span class="o">.</span><span class="n">experts</span>

<span class="c1"># Get each experts parameters.</span>
<span class="n">scornk</span><span class="o">.</span><span class="n">expert_params</span>

<span class="c1"># Get all expert&#39;s portfolio returns over time.</span>
<span class="n">scornk</span><span class="o">.</span><span class="n">expert_portfolio_returns</span>

<span class="c1"># Get capital allocation weights.</span>
<span class="n">scornk1</span><span class="o">.</span><span class="n">weights_on_experts</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="functional-correlation-driven-nonparametric-learning">
<h4>Functional Correlation Driven Nonparametric Learning<a class="headerlink" href="#functional-correlation-driven-nonparametric-learning" title="Permalink to this heading">¶</a></h4>
<p>FCORN further extends the SCORN by introducing a concept of an activation function. Applying the concept
to the previous CORN algorithms, the activation function for the SCORN can be considered as a piecewise function.
For any value between the positive and negative value of the threshold, we discount the importance of the period by placing a constant of 0.</p>
<p>Instead of completely neglecting windows with correlation with absolute value less than the threshold,
FCORN introduces a sigmoid function that places a set of different weights depending on the correlation
to the current market window. By replacing with such a variable, it is possible for us to place different
importance on the correlated periods. One that has higher correlation will have higher weights of importance
whereas ones that are less correlated will have less importance on it.</p>
<p>The activation function can be labeled as the following:</p>
<div class="math notranslate nohighlight">
\[b^{\bf{\star}}_t(w,\rho) = \underset{b \in \Delta_m}{\arg\max} \underset{j \in \lbrace1,...,t-1\rbrace}{\sum}v(j)\log b^{\top}x_i\]</div>
<p>If the correlation is nonnegative, we place a positive weight.</p>
<div class="math notranslate nohighlight">
\[\text{if} \: c \geq 0 \rightarrow v(j) =  \frac{1}{1 + \exp(-\lambda(c-\rho))}\]</div>
<p>If the correlation is negative, we place a negative weight that approaches 0 for correlation values closer to 0.</p>
<div class="math notranslate nohighlight">
\[\text{if} \: c &lt; 0 \rightarrow v(j) =  \frac{1}{1 + \exp(-\lambda(c+\rho))} - 1\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(b_t\)</span> is the portfolio vector at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_t\)</span> is the price relative change at time <span class="math notranslate nohighlight">\(t\)</span>. It is calculated by <span class="math notranslate nohighlight">\(\frac{p_t}{p_{t-1}}\)</span>, where <span class="math notranslate nohighlight">\(p_t\)</span> is the price at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(c\)</span> is the correlation coefficient.</p></li>
<li><p><span class="math notranslate nohighlight">\(\rho\)</span> is the correlation threshold.</p></li>
<li><p><span class="math notranslate nohighlight">\(v(j)\)</span> is the activation function for the given period weights.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta_m\)</span> is the simplex domain. The sum of all elements is 1, and each element is in the range of [0, 1].</p></li>
</ul>
<p>Two different variations of the FCORN strategies are implemented in the Online Portfolio Selection module.</p>
<hr class="docutils" />
<section id="functional-correlation-driven-nonparametric-learning-fcorn">
<h5>Functional Correlation Driven Nonparametric Learning (FCORN)<a class="headerlink" href="#functional-correlation-driven-nonparametric-learning-fcorn" title="Permalink to this heading">¶</a></h5>
<section id="fcorn-parameters">
<h6>FCORN Parameters<a class="headerlink" href="#fcorn-parameters" title="Permalink to this heading">¶</a></h6>
<p>Using <a class="reference external" href="https://optuna.org/">optuna</a>, we experimented with different parameters to provide a general guideline
for the users. The optimal <span class="math notranslate nohighlight">\(\rho\)</span> for FCORN is between 0.4 and 0.8 with best lambd value at 1.
In most cases, window should be in the smaller range with 1 or 2 as seen with the case for the NYSE dataset; however,
SP500 has the highest returns with window of 5.</p>
<a class="reference internal image-reference" href="_images/nyse_fcorn.png"><img alt="_images/nyse_fcorn.png" src="_images/nyse_fcorn.png" style="width: 49%;" /></a>
<a class="reference internal image-reference" href="_images/sp500_fcorn.png"><img alt="_images/sp500_fcorn.png" src="_images/sp500_fcorn.png" style="width: 49%;" /></a>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\rho\)</span> between 0.4 and 0.8 is optimal</p></li>
<li><p>Optimal window ranges are different, but are typically in between 1 and 7.</p></li>
<li><p>Lambd of 1 to 5 typically had high returns.</p></li>
</ul>
</div>
</section>
<section id="fcorn-implementation">
<h6>FCORN Implementation<a class="headerlink" href="#fcorn-implementation" title="Permalink to this heading">¶</a></h6>
</section>
</section>
<hr class="docutils" />
<section id="functional-correlation-driven-nonparametric-learning-k-fcorn-k">
<h5>Functional Correlation Driven Nonparametric Learning - K (FCORN-K)<a class="headerlink" href="#functional-correlation-driven-nonparametric-learning-k-fcorn-k" title="Permalink to this heading">¶</a></h5>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>More detailed explanation about the top-k strategy is available with the documentation for CORN-K.</p>
</div>
<section id="fcorn-k-parameters">
<h6>FCORN-K Parameters<a class="headerlink" href="#fcorn-k-parameters" title="Permalink to this heading">¶</a></h6>
<p>Using <a class="reference external" href="https://optuna.org/">optuna</a>, we experimented with different parameters to provide a general guideline
for the users. <span class="math notranslate nohighlight">\(\rho\)</span> should be at least 5 to capture the range between 0.4 and 0.8, with lambd of 1 sufficient
to get the highest returns. Window values vary with each dataset, but a value of 1 or 2 typically
had the highest returns.</p>
</section>
<section id="fcorn-k-implementation">
<h6>FCORN-K Implementation<a class="headerlink" href="#fcorn-k-implementation" title="Permalink to this heading">¶</a></h6>
</section>
</section>
<section id="id8">
<h5>Example Code<a class="headerlink" href="#id8" title="Permalink to this heading">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Read in data.</span>
<span class="n">stock_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span>

<span class="c1"># FCORN</span>
<span class="c1"># Compute Functional Correlation Driven Nonparametric Learning with no given weights, window of 1, rho of 0.3, and lambd of 10.</span>
<span class="n">fcorn</span> <span class="o">=</span> <span class="n">FCORN</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">fcorn</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute Functional Correlation Driven Nonparametric Learning with user given weights, window of 3, rho of 0.5, and lambd of 5.</span>
<span class="n">fcorn1</span> <span class="o">=</span> <span class="n">FCORN</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">fcorn1</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">some_weight</span><span class="p">)</span>

<span class="c1"># FCORN-K</span>
<span class="c1"># Compute Functional Correlation Driven Nonparametric Learning - K with no given weights, window of 10, rho of 7, lambd of 1, and k of 2.</span>
<span class="n">fcornk</span> <span class="o">=</span> <span class="n">FCORNK</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">fcornk</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute Functional Correlation Driven Nonparametric Learning - K with user given weights, window of 5, rho of 3, lambd of 2, and k of 1.</span>
<span class="n">fcornk1</span> <span class="o">=</span> <span class="n">FCORNK</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">fcornk1</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">some_weight</span><span class="p">)</span>

<span class="c1"># Recalculate k for fcornk1 to save computational time of generating all experts.</span>
<span class="n">fcornk1</span><span class="o">.</span><span class="n">recalculate_k</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Get the latest predicted weights.</span>
<span class="n">fcorn</span><span class="o">.</span><span class="n">weights</span>

<span class="c1"># Get all weights for the strategy.</span>
<span class="n">fcornk</span><span class="o">.</span><span class="n">all_weights</span>

<span class="c1"># Get portfolio returns.</span>
<span class="n">fcorn</span><span class="o">.</span><span class="n">portfolio_return</span>

<span class="c1"># Get each object of the generated experts.</span>
<span class="n">fcornk1</span><span class="o">.</span><span class="n">experts</span>

<span class="c1"># Get each experts parameters.</span>
<span class="n">fcornk</span><span class="o">.</span><span class="n">expert_params</span>

<span class="c1"># Get all expert&#39;s portfolio returns over time.</span>
<span class="n">fcornk</span><span class="o">.</span><span class="n">expert_portfolio_returns</span>

<span class="c1"># Get capital allocation weights.</span>
<span class="n">fcornk1</span><span class="o">.</span><span class="n">weights_on_experts</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="research-notebook">
<h4>Research Notebook<a class="headerlink" href="#research-notebook" title="Permalink to this heading">¶</a></h4>
<p>The following <a class="reference external" href="https://github.com/hudson-and-thames/research/blob/master/Online%20Portfolio%20Selection/Online%20Portfolio%20Selection%20-%20Pattern%20Matching.ipynb">pattern matching</a>
notebook provides a more detailed exploration of the strategies.</p>
</section>
</section>
<span id="document-online_portfolio_selection/universal_portfolio"></span><div class="admonition note" id="online-portfolio-selection-universal-portfolio">
<p class="admonition-title">Note</p>
<p>Strategies were implemented with modifications from:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="http://web.mit.edu/6.962/www/www_fall_2001/shaas/universal_portfolios.pdf">Cover, T.M., 1996. Universal Portfolios.</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1212.2129">Li, B., Hoi, S. C.H., 2012. OnLine Portfolio Selection: A Survey. ACM Comput. Surv. V, N, Article A (December 2012), 33 pages.</a></p></li>
</ol>
</div>
<section id="universal-portfolio">
<h3>Universal Portfolio<a class="headerlink" href="#universal-portfolio" title="Permalink to this heading">¶</a></h3>
<p>The default universal portfolio acts as a fund of funds. With the given number of inputs as experts,
the ensemble portfolio will generate a random set of weighted CRP’s to aggregated based on the given
algorithm. This class also acts as a parent class for the other strategies’ ensemble methods.</p>
<p>Three different capital allocation methods have been implemented.</p>
<section id="historical-performance">
<h4>Historical Performance<a class="headerlink" href="#historical-performance" title="Permalink to this heading">¶</a></h4>
<p>The default capital allocation method according to Cover’s paper was a historical-performance based one.
In this case, the concept can be easily represented as initially allocating a set of capital to all
experts and collecting the total sum at the end of the period.</p>
</section>
<section id="uniform">
<h4>Uniform<a class="headerlink" href="#uniform" title="Permalink to this heading">¶</a></h4>
<p>Uniform allocation refers to a similar principle as a CRP. Instead of leaving each individual capital
to each expert, after every period capital is rebalanced equally among all experts. The capital from
the better performing expert will move to the less performing ones.</p>
</section>
<section id="top-k">
<h4>Top-K<a class="headerlink" href="#top-k" title="Permalink to this heading">¶</a></h4>
<a class="reference internal image-reference" href="_images/top_k.png"><img alt="_images/top_k.png" src="_images/top_k.png" style="width: 99%;" /></a>
<p>Top-K was introduced in the Pattern Matching strategies, where it chooses portfolio weights based
on each experts’ performance. In this case, each expert is defined as an independent portfolio
with predetermined parameters. The next period’s total weights are based on the previous period’s k
best-performing experts. Through this method, we can reduce our overfitting and make our models more robust.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the universal portfolio to represent the studies by Thomas Cover, it would have to generate an infinite
number of portfolios to cover all possible parameters. This is more or less possible with a low number of assets
as for a two-asset market we can generate 100 weights from (0.01, 0.99) to (0.99 to 0.01). However, as the number
of assets increase, the number of points require to represent the same dimensionality exponentially increases as
well. Due to the <a class="reference external" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a>, it is
difficult for us to exactly approximate a portfolio.</p>
</div>
</section>
<section id="results">
<h4>Results<a class="headerlink" href="#results" title="Permalink to this heading">¶</a></h4>
<p>In addition to the outputs represented with the original <code class="docutils literal notranslate"><span class="pre">OLPS</span></code> class, the ensemble methods have more options.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.experts</span></code> (list) Array to store all experts</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.number_of_experts</span></code> (int) Set the number of experts.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.expert_params</span></code> (np.array) Each expert’s parameters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.expert_portfolio_returns</span></code> (np.array) All experts’ portfolio returns over time.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.expert_all_weights</span></code> (np.array) Each experts’ weights over time.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.expert_weights</span></code> (np.array) Each experts’ final portfolio weights</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.weights_on_experts</span></code> (np.array) Capital allocation on each experts.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.weighted</span></code> (np.array) Weights allocated to each experts.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.k</span></code> (int) Number of top-k experts.</p></li>
</ul>
</section>
<section id="example-code">
<h4>Example Code<a class="headerlink" href="#example-code" title="Permalink to this heading">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">mlfinlab.online_portfolio_selection</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Read in data.</span>
<span class="n">stock_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;FILE_PATH&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span>

<span class="c1"># Compute Universal Portfolio with 100 experts with historical performance weights.</span>
<span class="n">up</span> <span class="o">=</span> <span class="n">UP</span><span class="p">(</span><span class="n">number_of_experts</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">weighted</span><span class="o">=</span><span class="s1">&#39;hist_performance&#39;</span><span class="p">,)</span>
<span class="n">up</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute Universal Portfolio with 1000 experts with uniform weights.</span>
<span class="n">up</span> <span class="o">=</span> <span class="n">UP</span><span class="p">(</span><span class="n">number_of_experts</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">weighted</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">)</span>
<span class="n">up</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute Universal Portfolio with 10000 experts with top-k weights and k of 5.</span>
<span class="n">up</span> <span class="o">=</span> <span class="n">UP</span><span class="p">(</span><span class="n">number_of_experts</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">weighted</span><span class="o">=</span><span class="s1">&#39;top-k&#39;</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">up</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">asset_prices</span><span class="o">=</span><span class="n">stock_prices</span><span class="p">,</span> <span class="n">resample_by</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Recalculate k to 3 for top-k to experiment with different parameters.</span>
<span class="n">up</span><span class="o">.</span><span class="n">recalculate_k</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Get the latest predicted weights.</span>
<span class="n">up</span><span class="o">.</span><span class="n">weights</span>

<span class="c1"># Get all weights for the strategy.</span>
<span class="n">up</span><span class="o">.</span><span class="n">all_weights</span>

<span class="c1"># Get portfolio returns.</span>
<span class="n">up</span><span class="o">.</span><span class="n">portfolio_return</span>
</pre></div>
</div>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-additional_information/contributing"></span><section id="contributing">
<span id="additional-information-contributing"></span><h3>Contributing<a class="headerlink" href="#contributing" title="Permalink to this heading">¶</a></h3>
<section id="areas-of-contribution">
<h4>Areas of Contribution<a class="headerlink" href="#areas-of-contribution" title="Permalink to this heading">¶</a></h4>
<p>Currently we have a <a class="reference external" href="https://github.com/orgs/hudson-and-thames/projects">live project board</a> that follows the principles of Agile Project Management. This board is available
to the public and lets everyone know what the Researchers are currently working on.</p>
<p>At the time of writing, we are focusing our attentions primarily on those contributions by the current Researchers enrolled
in our <a class="reference external" href="https://hudsonthames.org/mentorship/">Mentorship Program</a>.</p>
<p>There is of course room for the public to make contributions. The most useful are those that help to improve user experience.
Good examples of this is writing <a class="reference external" href="https://github.com/hudson-and-thames/research">tutorial notebooks</a> which answer questions
from the back of a chapter, mlfinlab recipes, improving docstrings, and adding new sphinx documentation.</p>
</section>
<section id="raising-issues">
<h4>Raising Issues<a class="headerlink" href="#raising-issues" title="Permalink to this heading">¶</a></h4>
<p>We have created <a class="reference external" href="https://github.com/hudson-and-thames/mlfinlab/issues/new/choose">templates</a> to help aid in creating issues and PRs:</p>
<ul class="simple">
<li><p>Bug report</p></li>
<li><p>Feature request</p></li>
<li><p>Custom issue template</p></li>
<li><p>Pull Request Template</p></li>
</ul>
<p>Please do create issues for new feature requests and bug fixes.</p>
</section>
</section>
<span id="document-additional_information/license"></span><section id="license">
<span id="additional-information-license"></span><h3>License<a class="headerlink" href="#license" title="Permalink to this heading">¶</a></h3>
<p><strong>Copyright 2019, Hudson and Thames Quantitative Research</strong></p>
<p><strong>Copyright Protection Notice and Licensing Agreement</strong></p>
<p>This codebase is open-source only in the sense that the code is free to use
as-is, and the source code is publicly available, however, all other rights
are reserved under the Hudson and Thames Quantitative Research brand.</p>
<p>Our intention is to make some of the techniques developed publicly available
and to promote research in quantitative finance and machine learning.</p>
<section id="copyright-protection-notice">
<h4>1. Copyright Protection Notice:<a class="headerlink" href="#copyright-protection-notice" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>1.1 Kindly note that the code-based platform and/or any part thereof which</dt><dd><p>includes but is not limited to algorithms, coding and/or development:
- which is currently hosted on Github, is an open source platform for
purposes of training and research.</p>
</dd>
<dt>1.2 Kindly note further that the code-based platform or any part thereof is</dt><dd><p>available to the public and/or private domain for the sole purpose of
training and research by virtue of further development and coding and
that such further development and coding in its original and/or current
format (whichever is applicable), at all material or implied and tacit
times will remain and become the Intellectual Property of the Copyright
Holder, Hudson and Thames Quantitative Research.</p>
</dd>
<dt>1.3 The code-based platform allows for the use of the code whilst the</dt><dd><p>Copyright Holder herewith automatically and directly and/or indirectly
retains and owns all rights, interest and title generated and simultaneously
granted to the Copyright Holder in terms of the International Intellectual
Property Law, either by further development or coding.</p>
</dd>
<dt>1.4 The code-based platform allows for the use of the code whilst the Copyright</dt><dd><p>Holder herewith automatically and directly and/or indirectly retains and
owns all rights, interest and title generated and simultaneously granted
to the Copyright Holder in terms of the International Intellectual Property
Law, either by further development or coding.</p>
</dd>
<dt>1.5 Any further development and/or coding of any nature whatsoever that either</dt><dd><p>enhances and/or improves the original and/or current format (whichever is
applicable) will remain the sole property and ownership of the Copyright
Holder and accordingly no claim for proprietorship and/or ownership,
alternatively monetary compensation will exist either in the past, present
or future. In simpler terms:- No person, agent or AI will have any claim of
proprietorship or ownership against the Copyright Holder for any reason of
any nature whatsoever for any development and or coding that may or may not
improve and/or enhance the platform in its original and/or current format
(whichever is applicable) and accordingly herewith irrevocably and
unconditionally renounces and waives any such claims of any nature whatsoever
either in the past, present or future.</p>
</dd>
</dl>
</section>
<section id="terms-and-conditions-of-use-in-terms-of-the-code-based-platform">
<h4>2. Terms and Conditions of use in terms of the code-based platform:<a class="headerlink" href="#terms-and-conditions-of-use-in-terms-of-the-code-based-platform" title="Permalink to this heading">¶</a></h4>
<dl>
<dt>2.1 The terms and conditions of licensing agreement shall be governed and regulated</dt><dd><p>solely in terms of this document and that this agreement supersedes any other
agreement (either historically and/or currently) entered into with the Copyright
Holder.</p>
</dd>
<dt>2.2 Any application with respect to the code-based platform and/or any part thereof</dt><dd><p>which includes but is not limited to algorithms, coding and/or development i.e.
further development and/or coding are subject to the following implied, material,
agreed, contented and tacit terms and conditions:</p>
<dl class="simple">
<dt>2.2.1 The user may use the platform in its original and/or current format</dt><dd><p>(whichever is applicable) and any content derived from such usage is
permissible and free of charge or consideration of any kind;</p>
</dd>
<dt>2.2.2 It is the distinct and strict obligation of the user to add, disclose and</dt><dd><p>release to the Copyright Holder any and all improvements, enhancements
and/or modifications (whether in source or binary form) to the respective
repositories without any undue delay. The user agrees, consents and acknowledges
that he/she/it will under no circumstances of any nature whatsoever withhold
and/or cause to withhold any improvements, enhancements and/or modifications
and as such any of the aforementioned will automatically and by default become
the sole property of the Copyright Holder, Hudson and Thames Quantitative
Research Brand, whether or not such improvements, enhancements and/or
modifications have been uploaded to the repository or not;</p>
</dd>
<dt>2.2.3 The user understands, acknowledges and consents that he/she/it must at all</dt><dd><p>material and implied times obtain the written consent of the Copyright Holder
to reproduce, distribute, duplicate or create derivative works in whole or in
part thereof (whichever is applicable) of the code-based platform for any
reason whatsoever which includes but is not limited to monetary value, further
development or research;</p>
</dd>
<dt>2.2.4 It is an explicit term and condition of this agreement that any sale, reproduction,</dt><dd><p>marketing or distribution of the code-based platform is prohibited and that any
act or conduct to commit any of the aforementioned either in whole or in part
thereof constitutes an immediate and direct breach of this agreement. The user
agrees, admits and consents that he/she/it will be liable to pay to the Copyright
Holder any damages suffered by the Copyright Holder of any nature whatsoever
(proven or not proven) as a direct and/or indirect result of any such conduct
and/or act performed in whole or in part by the user;</p>
</dd>
<dt>2.2.5 The user agrees that he/she/it will under no circumstances for any reason whatsoever</dt><dd><p>use the name of the Copyright Holder nor any affiliated and/or associated names,
details and/or information of the Copyright Holder to promote, endorse or distribute
any product using code or algorithms from this platform either as a whole or in part
thereof; and</p>
</dd>
<dt>2.2.6 This platform or any part thereof is used at the sole risk of the user and accordingly</dt><dd><p>the user irrevocably and unconditionally indemnifies and holds the Copyright Holder,
its employees and contributors harmless for any direct and/or indirect damages of any
nature whatsoever which includes but is not limited to: - incidental, exemplary, or
consequential damages for the loss of use, data or profits, or business interruption,
however caused, including any theory of liability, whether in contract, strict liability
or Tort (including negligence or otherwise arising in any way out of the use of this software),
even if advised of the possibility of such damage.</p>
</dd>
</dl>
</dd>
</dl>
</section>
</section>
</div>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="#">mlfinlab</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-getting_started/installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-additional_information/contact">Join the Slack Channel</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-getting_started/barriers_to_entry">Barriers to Entry</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-getting_started/researcher">Become a Researcher</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-getting_started/datasets">Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-getting_started/research_tools">Research Tools</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Feature Engineering</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-implementations/data_structures">Data Structures</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-implementations/filters">Filters</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-implementations/frac_diff">Fractionally Differentiated Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-implementations/structural_breaks">Structural Breaks</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-implementations/microstructural_features">Microstructural Features</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Codependence</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-codependence/introduction">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-codependence/correlation_based_metrics">Correlation-Based Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-codependence/information_theory_metrics">Information Theory Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-codependence/codependence_marti">Codependence by Marti</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-codependence/codependence_matrix">Codependence Matrix</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Labeling</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-labeling/tb_meta_labeling">Triple-Barrier and Meta-Labelling</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-labeling/labeling_trend_scanning">Trend Scanning</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-labeling/labeling_tail_sets">Tail Sets</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-labeling/labeling_fixed_time_horizon">Fixed Horizon Method</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-labeling/labeling_matrix_flags">Labeling Matrix Flags</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-labeling/labeling_excess_median">Excess Over Median</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-labeling/labeling_raw_return">Raw Returns</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-labeling/labeling_vs_benchmark">Return Versus Benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-labeling/labeling_excess_mean">Excess Over Mean</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Modelling</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-implementations/sampling">Sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-implementations/sb_bagging">Sequentially Bootstrapped Bagging Classifier/Regressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-implementations/feature_importance">Feature Importance</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-implementations/cross_validation">Cross Validation</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-implementations/EF3M">Exact Fit using first 3 Moments (EF3M)</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-implementations/bet_sizing">Bet Sizing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Clustering</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-implementations/onc">Optimal Number of Clusters (ONC)</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-implementations/feature_clusters">Feature Clusters</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Backtest Overfitting</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-implementations/backtesting">Backtesting by Campbell and Yan</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-implementations/backtest_statistics">Backtest Statistics</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Portfolio Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-portfolio_optimisation/risk_metrics">Risk Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-portfolio_optimisation/returns_estimation">Returns Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-portfolio_optimisation/risk_estimators">Risk Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-portfolio_optimisation/mean_variance">Mean-Variance Optimisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-portfolio_optimisation/critical_line_algorithm">The Critical Line Algorithm (CLA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-portfolio_optimisation/hierarchical_risk_parity">Hierarchical Risk Parity (HRP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-portfolio_optimisation/hierarchical_equal_risk_contribution">Hierarchical Equal Risk Contribution (HERC)</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-portfolio_optimisation/nested_clustered_optimisation">Nested Clustered Optimization (NCO)</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-portfolio_optimisation/theory_implied_correlation">Theory-Implied Correlation (TIC)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Online Portfolio Selection</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-online_portfolio_selection/introduction">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-online_portfolio_selection/benchmarks">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-online_portfolio_selection/momentum">Momentum</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-online_portfolio_selection/mean_reversion">Mean Reversion</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-online_portfolio_selection/pattern_matching">Pattern Matching</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-online_portfolio_selection/universal_portfolio">Universal Portfolio</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Additional Information</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-additional_information/contributing">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-additional_information/license">License</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Hudson & Thames,.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 6.1.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
    </div>

    

    
  </body>
</html>