# Research Notebooks
Inferential analysis of data comprises of using a sample of data to describe the characteristics of a population (such as average height or mean return). For such an analysis and inference to be accurate, it is necessary that the underlying data generation process to remain constant. In the context of finance, the mean return and variance of those returns should not change over time.  If they change then it is hard to predict the expected return (or risk as defined by the volatility) of that stock in some time in the future.  A similar requirement exists in the case of supervised machine learning (SML).  In SML, unseen observations are mapped to a set of labeled examples (features) to know the label (decision or outcome) of the new observation. If the data (features, in the case of SML) is not “stationary” or constant then the machine learning algorithm would not be able to correctly infer the label of the new observation. Hence, stationarity becomes a necessary condition for inferential analysis and supervised machine learning. However, asset prices experience trends (or drifts) that make the time series non-stationary. But the underlying trend is also useful in prediction. This leads to a challenge – how can one make the time series stationary while retaining its predictive power (or memory).  

Hosking [1981] showed that “fractionally differenced processes exhibit long-term persistence and anti-persistence; the dependence between observations a long time span apart decays much more slowly with time span than is the case with commonly used time series models”.  Hence, fractional differentiation is used to make the time series stationary while retaining as much memory as possible.

The following notebook answer the questions at the back of chapter 5 and in the process explore the concept
of fractional differentiation (frac-diff) in detail.
